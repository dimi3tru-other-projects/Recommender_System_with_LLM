{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b461584d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b4696d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "import gc\n",
    "import weakref\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    torch.mps.set_per_process_memory_fraction(0.9) # Memory usage limit for MacOS\n",
    "    torch.mps.empty_cache() \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.manual_seed(911)\n",
    "random.seed(911)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8ebaea",
   "metadata": {},
   "source": [
    "**For cleaning memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb7a4dda-0964-49d1-8812-6c0bb98246b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if isinstance(obj, torch.Tensor) and obj.is_mps: # or obj.is_cuda\n",
    "            ref = weakref.ref(obj)\n",
    "            del obj\n",
    "            del ref\n",
    "    except ReferenceError:\n",
    "        pass\n",
    "\n",
    "gc.collect()\n",
    "torch.mps.empty_cache() # or torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5931cb14-3e62-4c69-b4cc-b034940382c2",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c78abc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path().resolve() # Path(__file__).resolve()\n",
    "project_dir = file_path.parent\n",
    "raw_data_path = project_dir / 'data' / 'raw'\n",
    "products_data_dir = project_dir / 'data' / 'processed' / 'products_data'\n",
    "interim_data_dir = project_dir / 'data' / 'interim'\n",
    "models_outputs_dir = project_dir / 'data' / 'processed' / 'models_outputs'\n",
    "models_dir = project_dir / 'models'\n",
    "processed_images_dir = products_data_dir / 'processed_images_224x224'\n",
    "\n",
    "raw_data_path.mkdir(parents=True, exist_ok=True)\n",
    "products_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "interim_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "models_outputs_dir.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "processed_images_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc263f5a",
   "metadata": {},
   "source": [
    "## Fast Data load (only if data was processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b330cef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articul_encrypred</th>\n",
       "      <th>product_created_at_day</th>\n",
       "      <th>sales_total</th>\n",
       "      <th>CLS_google_vit_huge_patch14_224_in21k</th>\n",
       "      <th>mean_patch_google_vit_huge_patch14_224_in21k</th>\n",
       "      <th>pooled_google_vit_huge_patch14_224_in21k</th>\n",
       "      <th>pooled_microsoft_resnet50</th>\n",
       "      <th>CLS_openai_clip_vit_large_patch14</th>\n",
       "      <th>mean_patch_openai_clip_vit_large_patch14</th>\n",
       "      <th>pooled_openai_clip_vit_large_patch14</th>\n",
       "      <th>embedding_e5_large_v2</th>\n",
       "      <th>embedding_bge_large_en_v15</th>\n",
       "      <th>embedding_nomic_embed_text_v15</th>\n",
       "      <th>articul_encrypred_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>228217</th>\n",
       "      <td>xspvtuqv</td>\n",
       "      <td>18529</td>\n",
       "      <td>9417.00</td>\n",
       "      <td>[0.1068585216999054, -0.06343062222003937, -0....</td>\n",
       "      <td>[0.017839273437857628, -0.010774536058306694, ...</td>\n",
       "      <td>[-0.24715401232242584, 0.41301143169403076, -0...</td>\n",
       "      <td>[0.3190707266330719, 0.028264325112104416, 0.0...</td>\n",
       "      <td>[0.5622039437294006, -0.4325735867023468, 0.44...</td>\n",
       "      <td>[0.8816746473312378, 0.691596269607544, 0.1884...</td>\n",
       "      <td>[0.2884502708911896, 0.38565462827682495, -0.0...</td>\n",
       "      <td>[0.03428741917014122, -0.06396043300628662, 0....</td>\n",
       "      <td>[0.007279624696820974, 0.006243441719561815, 0...</td>\n",
       "      <td>[-0.036529459059238434, 0.03486456722021103, -...</td>\n",
       "      <td>218574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228218</th>\n",
       "      <td>xtqwuuuy</td>\n",
       "      <td>18530</td>\n",
       "      <td>19380.79</td>\n",
       "      <td>[0.09790005534887314, -0.03745045466348529, 0....</td>\n",
       "      <td>[0.012738960678689182, -0.002624053042382002, ...</td>\n",
       "      <td>[-0.03077596426010132, 0.19863871857523918, -0...</td>\n",
       "      <td>[0.03357352642342448, 0.0015215009916573763, 0...</td>\n",
       "      <td>[0.5787270814180374, -0.054604075849056244, 0....</td>\n",
       "      <td>[0.7522355616092682, 0.7038511633872986, 0.364...</td>\n",
       "      <td>[-0.0036218371242284775, 0.18161564506590366, ...</td>\n",
       "      <td>[0.03048939537256956, -0.06178726628422737, 0....</td>\n",
       "      <td>[-0.0015129486564546824, 0.01574902841821313, ...</td>\n",
       "      <td>[0.005874196067452431, 0.08644555881619453, -0...</td>\n",
       "      <td>207628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       articul_encrypred  product_created_at_day  sales_total  \\\n",
       "228217          xspvtuqv                   18529      9417.00   \n",
       "228218          xtqwuuuy                   18530     19380.79   \n",
       "\n",
       "                    CLS_google_vit_huge_patch14_224_in21k  \\\n",
       "228217  [0.1068585216999054, -0.06343062222003937, -0....   \n",
       "228218  [0.09790005534887314, -0.03745045466348529, 0....   \n",
       "\n",
       "             mean_patch_google_vit_huge_patch14_224_in21k  \\\n",
       "228217  [0.017839273437857628, -0.010774536058306694, ...   \n",
       "228218  [0.012738960678689182, -0.002624053042382002, ...   \n",
       "\n",
       "                 pooled_google_vit_huge_patch14_224_in21k  \\\n",
       "228217  [-0.24715401232242584, 0.41301143169403076, -0...   \n",
       "228218  [-0.03077596426010132, 0.19863871857523918, -0...   \n",
       "\n",
       "                                pooled_microsoft_resnet50  \\\n",
       "228217  [0.3190707266330719, 0.028264325112104416, 0.0...   \n",
       "228218  [0.03357352642342448, 0.0015215009916573763, 0...   \n",
       "\n",
       "                        CLS_openai_clip_vit_large_patch14  \\\n",
       "228217  [0.5622039437294006, -0.4325735867023468, 0.44...   \n",
       "228218  [0.5787270814180374, -0.054604075849056244, 0....   \n",
       "\n",
       "                 mean_patch_openai_clip_vit_large_patch14  \\\n",
       "228217  [0.8816746473312378, 0.691596269607544, 0.1884...   \n",
       "228218  [0.7522355616092682, 0.7038511633872986, 0.364...   \n",
       "\n",
       "                     pooled_openai_clip_vit_large_patch14  \\\n",
       "228217  [0.2884502708911896, 0.38565462827682495, -0.0...   \n",
       "228218  [-0.0036218371242284775, 0.18161564506590366, ...   \n",
       "\n",
       "                                    embedding_e5_large_v2  \\\n",
       "228217  [0.03428741917014122, -0.06396043300628662, 0....   \n",
       "228218  [0.03048939537256956, -0.06178726628422737, 0....   \n",
       "\n",
       "                               embedding_bge_large_en_v15  \\\n",
       "228217  [0.007279624696820974, 0.006243441719561815, 0...   \n",
       "228218  [-0.0015129486564546824, 0.01574902841821313, ...   \n",
       "\n",
       "                           embedding_nomic_embed_text_v15  \\\n",
       "228217  [-0.036529459059238434, 0.03486456722021103, -...   \n",
       "228218  [0.005874196067452431, 0.08644555881619453, -0...   \n",
       "\n",
       "        articul_encrypred_id  \n",
       "228217                218574  \n",
       "228218                207628  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id_encrypred</th>\n",
       "      <th>articul_encrypred</th>\n",
       "      <th>order_date</th>\n",
       "      <th>store_encoded</th>\n",
       "      <th>articul_encrypred_id</th>\n",
       "      <th>sales_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3354708</th>\n",
       "      <td>wyyyyqqsyyvxvvtr</td>\n",
       "      <td>wyuutrv</td>\n",
       "      <td>2020-08-17</td>\n",
       "      <td>3</td>\n",
       "      <td>69858</td>\n",
       "      <td>453198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3354709</th>\n",
       "      <td>wyyyyqqsyyvxvvtr</td>\n",
       "      <td>wywqryu</td>\n",
       "      <td>2020-07-15</td>\n",
       "      <td>3</td>\n",
       "      <td>129264</td>\n",
       "      <td>155835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        anon_id_encrypred articul_encrypred  order_date  store_encoded  \\\n",
       "3354708  wyyyyqqsyyvxvvtr           wyuutrv  2020-08-17              3   \n",
       "3354709  wyyyyqqsyyvxvvtr           wywqryu  2020-07-15              3   \n",
       "\n",
       "         articul_encrypred_id  sales_total  \n",
       "3354708                 69858       453198  \n",
       "3354709                129264       155835  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df_products_articul = pd.read_parquet(products_data_dir / 'df_products_articul.parquet')\n",
    "df_sales_articul = pd.read_parquet(products_data_dir / 'df_sales_articul.parquet')\n",
    "\n",
    "display(df_products_articul.tail(2), df_sales_articul.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb74048",
   "metadata": {},
   "source": [
    "## Products Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc17ce36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articul_encrypred</th>\n",
       "      <th>color_base</th>\n",
       "      <th>brand</th>\n",
       "      <th>ktt1</th>\n",
       "      <th>ktt2</th>\n",
       "      <th>ktt3</th>\n",
       "      <th>ktt4</th>\n",
       "      <th>title</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_created_at</th>\n",
       "      <th>slug</th>\n",
       "      <th>photo_analytics</th>\n",
       "      <th>sales_total</th>\n",
       "      <th>image_path</th>\n",
       "      <th>CLS_google_vit_huge_patch14_224_in21k</th>\n",
       "      <th>mean_patch_google_vit_huge_patch14_224_in21k</th>\n",
       "      <th>pooled_google_vit_huge_patch14_224_in21k</th>\n",
       "      <th>pooled_microsoft_resnet50</th>\n",
       "      <th>CLS_openai_clip_vit_large_patch14</th>\n",
       "      <th>mean_patch_openai_clip_vit_large_patch14</th>\n",
       "      <th>pooled_openai_clip_vit_large_patch14</th>\n",
       "      <th>description</th>\n",
       "      <th>embedding_e5_large_v2</th>\n",
       "      <th>embedding_bge_large_en_v15</th>\n",
       "      <th>embedding_nomic_embed_text_v15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>306670</th>\n",
       "      <td>wqsvuwy</td>\n",
       "      <td>Разноцветный</td>\n",
       "      <td>Lancel</td>\n",
       "      <td>Товары для женщин</td>\n",
       "      <td>Аксессуары</td>\n",
       "      <td>Платки</td>\n",
       "      <td>Платок шелковый</td>\n",
       "      <td>Шелковый платок</td>\n",
       "      <td>13567982</td>\n",
       "      <td>2020-06-25</td>\n",
       "      <td>6027468-shelkovyi-platok-lancel-raznotcvetnyi-...</td>\n",
       "      <td>https://st-cdn.tsum.com/int/height/1526/i/f5/9...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>/Users/dimi3tru/Downloads/Downloads/my_python_...</td>\n",
       "      <td>[0.14456921815872192, -0.06249238923192024, -0...</td>\n",
       "      <td>[0.013126503676176071, -0.0019127298146486282,...</td>\n",
       "      <td>[0.22613008320331573, 0.1457078456878662, -0.1...</td>\n",
       "      <td>[0.030423777177929878, 0.0, 0.0, 0.08489743620...</td>\n",
       "      <td>[0.15574012696743011, -0.3459433317184448, 0.6...</td>\n",
       "      <td>[0.6875616312026978, 0.6793960332870483, 0.354...</td>\n",
       "      <td>[0.42158013582229614, 0.2945811450481415, 0.35...</td>\n",
       "      <td>This Lancel silk scarf features shades of blue...</td>\n",
       "      <td>[0.02612929418683052, -0.05050811171531677, 0....</td>\n",
       "      <td>[-0.020242616534233093, -0.01976708509027958, ...</td>\n",
       "      <td>[0.01306484080851078, 0.022184912115335464, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306671</th>\n",
       "      <td>ttquswt</td>\n",
       "      <td>Чёрный</td>\n",
       "      <td>Giorgio Armani</td>\n",
       "      <td>Товары для женщин</td>\n",
       "      <td>Бижутерия</td>\n",
       "      <td>Брошь</td>\n",
       "      <td>Брошь</td>\n",
       "      <td>Брошь</td>\n",
       "      <td>11201309</td>\n",
       "      <td>2016-07-26</td>\n",
       "      <td>5504265-brosh-giorgio-armani-chernyi</td>\n",
       "      <td>https://st-cdn.tsum.com/int/height/1526//i/76/...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>/Users/dimi3tru/Downloads/Downloads/my_python_...</td>\n",
       "      <td>[-0.12436471879482269, -0.030923746526241302, ...</td>\n",
       "      <td>[-0.01337357982993126, -0.0029991380870342255,...</td>\n",
       "      <td>[-0.09756504744291306, -0.15952929854393005, 0...</td>\n",
       "      <td>[0.03119073063135147, 0.0002419875527266413, 0...</td>\n",
       "      <td>[0.5343070030212402, 0.26047518849372864, 0.37...</td>\n",
       "      <td>[0.7018769979476929, 0.5926704406738281, 0.369...</td>\n",
       "      <td>[0.2984030842781067, 0.4144335687160492, -0.12...</td>\n",
       "      <td>This item is a black brooch from Giorgio Arman...</td>\n",
       "      <td>[0.013706686906516552, -0.061785973608493805, ...</td>\n",
       "      <td>[-0.029468011111021042, 0.002113671973347664, ...</td>\n",
       "      <td>[0.03772303834557533, 0.08191631734371185, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       articul_encrypred    color_base           brand               ktt1  \\\n",
       "306670           wqsvuwy  Разноцветный          Lancel  Товары для женщин   \n",
       "306671           ttquswt        Чёрный  Giorgio Armani  Товары для женщин   \n",
       "\n",
       "              ktt2    ktt3             ktt4            title  product_id  \\\n",
       "306670  Аксессуары  Платки  Платок шелковый  Шелковый платок    13567982   \n",
       "306671   Бижутерия   Брошь            Брошь            Брошь    11201309   \n",
       "\n",
       "       product_created_at                                               slug  \\\n",
       "306670         2020-06-25  6027468-shelkovyi-platok-lancel-raznotcvetnyi-...   \n",
       "306671         2016-07-26               5504265-brosh-giorgio-armani-chernyi   \n",
       "\n",
       "                                          photo_analytics  sales_total  \\\n",
       "306670  https://st-cdn.tsum.com/int/height/1526/i/f5/9...         0.01   \n",
       "306671  https://st-cdn.tsum.com/int/height/1526//i/76/...         0.01   \n",
       "\n",
       "                                               image_path  \\\n",
       "306670  /Users/dimi3tru/Downloads/Downloads/my_python_...   \n",
       "306671  /Users/dimi3tru/Downloads/Downloads/my_python_...   \n",
       "\n",
       "                    CLS_google_vit_huge_patch14_224_in21k  \\\n",
       "306670  [0.14456921815872192, -0.06249238923192024, -0...   \n",
       "306671  [-0.12436471879482269, -0.030923746526241302, ...   \n",
       "\n",
       "             mean_patch_google_vit_huge_patch14_224_in21k  \\\n",
       "306670  [0.013126503676176071, -0.0019127298146486282,...   \n",
       "306671  [-0.01337357982993126, -0.0029991380870342255,...   \n",
       "\n",
       "                 pooled_google_vit_huge_patch14_224_in21k  \\\n",
       "306670  [0.22613008320331573, 0.1457078456878662, -0.1...   \n",
       "306671  [-0.09756504744291306, -0.15952929854393005, 0...   \n",
       "\n",
       "                                pooled_microsoft_resnet50  \\\n",
       "306670  [0.030423777177929878, 0.0, 0.0, 0.08489743620...   \n",
       "306671  [0.03119073063135147, 0.0002419875527266413, 0...   \n",
       "\n",
       "                        CLS_openai_clip_vit_large_patch14  \\\n",
       "306670  [0.15574012696743011, -0.3459433317184448, 0.6...   \n",
       "306671  [0.5343070030212402, 0.26047518849372864, 0.37...   \n",
       "\n",
       "                 mean_patch_openai_clip_vit_large_patch14  \\\n",
       "306670  [0.6875616312026978, 0.6793960332870483, 0.354...   \n",
       "306671  [0.7018769979476929, 0.5926704406738281, 0.369...   \n",
       "\n",
       "                     pooled_openai_clip_vit_large_patch14  \\\n",
       "306670  [0.42158013582229614, 0.2945811450481415, 0.35...   \n",
       "306671  [0.2984030842781067, 0.4144335687160492, -0.12...   \n",
       "\n",
       "                                              description  \\\n",
       "306670  This Lancel silk scarf features shades of blue...   \n",
       "306671  This item is a black brooch from Giorgio Arman...   \n",
       "\n",
       "                                    embedding_e5_large_v2  \\\n",
       "306670  [0.02612929418683052, -0.05050811171531677, 0....   \n",
       "306671  [0.013706686906516552, -0.061785973608493805, ...   \n",
       "\n",
       "                               embedding_bge_large_en_v15  \\\n",
       "306670  [-0.020242616534233093, -0.01976708509027958, ...   \n",
       "306671  [-0.029468011111021042, 0.002113671973347664, ...   \n",
       "\n",
       "                           embedding_nomic_embed_text_v15  \n",
       "306670  [0.01306484080851078, 0.022184912115335464, -0...  \n",
       "306671  [0.03772303834557533, 0.08191631734371185, -0....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_products = pd.read_parquet(products_data_dir / 'products_data.parquet')\n",
    "df_products.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c93e89",
   "metadata": {},
   "source": [
    "There are some broken URLs — we remove these items from our dataset.\n",
    "\n",
    "![alt text](../junk/broken_urls.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c40ce97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows initially: 306672\n",
      "Number of rows with at least one missing value: 8\n",
      "Number of rows after cleaning: 306664\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of rows initially: {df_products.shape[0]}')\n",
    "print(f'Number of rows with at least one missing value: {df_products[df_products.isna().any(axis=1)].shape[0]}')\n",
    "\n",
    "df_products.dropna(inplace=True)\n",
    "print(f'Number of rows after cleaning: {df_products.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde12f24",
   "metadata": {},
   "source": [
    "**Переход с ключа `product_id` на ключ `articul_encrypred`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d71d9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articul_encrypred</th>\n",
       "      <th>product_created_at_day</th>\n",
       "      <th>sales_total</th>\n",
       "      <th>CLS_google_vit_huge_patch14_224_in21k</th>\n",
       "      <th>mean_patch_google_vit_huge_patch14_224_in21k</th>\n",
       "      <th>pooled_google_vit_huge_patch14_224_in21k</th>\n",
       "      <th>pooled_microsoft_resnet50</th>\n",
       "      <th>CLS_openai_clip_vit_large_patch14</th>\n",
       "      <th>mean_patch_openai_clip_vit_large_patch14</th>\n",
       "      <th>pooled_openai_clip_vit_large_patch14</th>\n",
       "      <th>embedding_e5_large_v2</th>\n",
       "      <th>embedding_bge_large_en_v15</th>\n",
       "      <th>embedding_nomic_embed_text_v15</th>\n",
       "      <th>articul_encrypred_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>228214</th>\n",
       "      <td>wyyyxuw</td>\n",
       "      <td>18430</td>\n",
       "      <td>372493.07</td>\n",
       "      <td>[0.054675329476594925, -0.031218096613883972, ...</td>\n",
       "      <td>[-0.015285112895071507, -0.002974292729049921,...</td>\n",
       "      <td>[-0.10003115236759186, -0.007143696304410696, ...</td>\n",
       "      <td>[0.022604845464229584, 0.008857643231749535, 0...</td>\n",
       "      <td>[-0.005107337608933449, 0.055002182722091675, ...</td>\n",
       "      <td>[0.6566806435585022, 0.8994556665420532, 0.425...</td>\n",
       "      <td>[0.40712353587150574, 0.6740058064460754, -0.0...</td>\n",
       "      <td>[0.026547163724899292, -0.05078291893005371, 0...</td>\n",
       "      <td>[0.010242261923849583, -0.008357411250472069, ...</td>\n",
       "      <td>[-0.01627330109477043, 0.03971276432275772, -0...</td>\n",
       "      <td>80702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228215</th>\n",
       "      <td>wyyyxux</td>\n",
       "      <td>18405</td>\n",
       "      <td>55930.00</td>\n",
       "      <td>[0.12628699839115143, -0.1826455146074295, -0....</td>\n",
       "      <td>[0.013777711428701878, -0.02734513208270073, -...</td>\n",
       "      <td>[-0.25407537817955017, -0.1469070166349411, -0...</td>\n",
       "      <td>[0.1269151270389557, 0.0, 6.139278411865234e-0...</td>\n",
       "      <td>[0.1935054361820221, 0.6354405879974365, 0.621...</td>\n",
       "      <td>[0.6567713618278503, 0.706050455570221, 0.4033...</td>\n",
       "      <td>[0.5280981063842773, 0.5120133757591248, 0.190...</td>\n",
       "      <td>[0.03120279870927334, -0.04832116514444351, 0....</td>\n",
       "      <td>[-0.02726542204618454, -0.004029609262943268, ...</td>\n",
       "      <td>[-0.024061240255832672, 0.05157441273331642, -...</td>\n",
       "      <td>176363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228216</th>\n",
       "      <td>wyyyxuy</td>\n",
       "      <td>18412</td>\n",
       "      <td>293330.55</td>\n",
       "      <td>[-0.03089914657175541, 0.0044742790050804615, ...</td>\n",
       "      <td>[0.010508312843739986, 0.01059587299823761, -0...</td>\n",
       "      <td>[-0.01950656808912754, -0.05603202059864998, -...</td>\n",
       "      <td>[0.004777229391038418, 0.0, 0.1000245586037635...</td>\n",
       "      <td>[0.4373447895050049, 0.04957278072834015, 0.67...</td>\n",
       "      <td>[0.8001006245613098, 0.9261599779129028, 0.368...</td>\n",
       "      <td>[0.34528201818466187, 0.702330470085144, 0.210...</td>\n",
       "      <td>[0.01696249097585678, -0.058472465723752975, 0...</td>\n",
       "      <td>[-0.0057455929927527905, -0.000962883175816386...</td>\n",
       "      <td>[0.03578423336148262, 0.022266210988163948, -0...</td>\n",
       "      <td>94224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228217</th>\n",
       "      <td>xspvtuqv</td>\n",
       "      <td>18529</td>\n",
       "      <td>9417.00</td>\n",
       "      <td>[0.1068585216999054, -0.06343062222003937, -0....</td>\n",
       "      <td>[0.017839273437857628, -0.010774536058306694, ...</td>\n",
       "      <td>[-0.24715401232242584, 0.41301143169403076, -0...</td>\n",
       "      <td>[0.3190707266330719, 0.028264325112104416, 0.0...</td>\n",
       "      <td>[0.5622039437294006, -0.4325735867023468, 0.44...</td>\n",
       "      <td>[0.8816746473312378, 0.691596269607544, 0.1884...</td>\n",
       "      <td>[0.2884502708911896, 0.38565462827682495, -0.0...</td>\n",
       "      <td>[0.03428741917014122, -0.06396043300628662, 0....</td>\n",
       "      <td>[0.007279624696820974, 0.006243441719561815, 0...</td>\n",
       "      <td>[-0.036529459059238434, 0.03486456722021103, -...</td>\n",
       "      <td>218574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228218</th>\n",
       "      <td>xtqwuuuy</td>\n",
       "      <td>18530</td>\n",
       "      <td>19380.79</td>\n",
       "      <td>[0.09790005534887314, -0.03745045466348529, 0....</td>\n",
       "      <td>[0.012738960678689182, -0.002624053042382002, ...</td>\n",
       "      <td>[-0.03077596426010132, 0.19863871857523918, -0...</td>\n",
       "      <td>[0.03357352642342448, 0.0015215009916573763, 0...</td>\n",
       "      <td>[0.5787270814180374, -0.054604075849056244, 0....</td>\n",
       "      <td>[0.7522355616092682, 0.7038511633872986, 0.364...</td>\n",
       "      <td>[-0.0036218371242284775, 0.18161564506590366, ...</td>\n",
       "      <td>[0.03048939537256956, -0.06178726628422737, 0....</td>\n",
       "      <td>[-0.0015129486564546824, 0.01574902841821313, ...</td>\n",
       "      <td>[0.005874196067452431, 0.08644555881619453, -0...</td>\n",
       "      <td>207628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       articul_encrypred  product_created_at_day  sales_total  \\\n",
       "228214           wyyyxuw                   18430    372493.07   \n",
       "228215           wyyyxux                   18405     55930.00   \n",
       "228216           wyyyxuy                   18412    293330.55   \n",
       "228217          xspvtuqv                   18529      9417.00   \n",
       "228218          xtqwuuuy                   18530     19380.79   \n",
       "\n",
       "                    CLS_google_vit_huge_patch14_224_in21k  \\\n",
       "228214  [0.054675329476594925, -0.031218096613883972, ...   \n",
       "228215  [0.12628699839115143, -0.1826455146074295, -0....   \n",
       "228216  [-0.03089914657175541, 0.0044742790050804615, ...   \n",
       "228217  [0.1068585216999054, -0.06343062222003937, -0....   \n",
       "228218  [0.09790005534887314, -0.03745045466348529, 0....   \n",
       "\n",
       "             mean_patch_google_vit_huge_patch14_224_in21k  \\\n",
       "228214  [-0.015285112895071507, -0.002974292729049921,...   \n",
       "228215  [0.013777711428701878, -0.02734513208270073, -...   \n",
       "228216  [0.010508312843739986, 0.01059587299823761, -0...   \n",
       "228217  [0.017839273437857628, -0.010774536058306694, ...   \n",
       "228218  [0.012738960678689182, -0.002624053042382002, ...   \n",
       "\n",
       "                 pooled_google_vit_huge_patch14_224_in21k  \\\n",
       "228214  [-0.10003115236759186, -0.007143696304410696, ...   \n",
       "228215  [-0.25407537817955017, -0.1469070166349411, -0...   \n",
       "228216  [-0.01950656808912754, -0.05603202059864998, -...   \n",
       "228217  [-0.24715401232242584, 0.41301143169403076, -0...   \n",
       "228218  [-0.03077596426010132, 0.19863871857523918, -0...   \n",
       "\n",
       "                                pooled_microsoft_resnet50  \\\n",
       "228214  [0.022604845464229584, 0.008857643231749535, 0...   \n",
       "228215  [0.1269151270389557, 0.0, 6.139278411865234e-0...   \n",
       "228216  [0.004777229391038418, 0.0, 0.1000245586037635...   \n",
       "228217  [0.3190707266330719, 0.028264325112104416, 0.0...   \n",
       "228218  [0.03357352642342448, 0.0015215009916573763, 0...   \n",
       "\n",
       "                        CLS_openai_clip_vit_large_patch14  \\\n",
       "228214  [-0.005107337608933449, 0.055002182722091675, ...   \n",
       "228215  [0.1935054361820221, 0.6354405879974365, 0.621...   \n",
       "228216  [0.4373447895050049, 0.04957278072834015, 0.67...   \n",
       "228217  [0.5622039437294006, -0.4325735867023468, 0.44...   \n",
       "228218  [0.5787270814180374, -0.054604075849056244, 0....   \n",
       "\n",
       "                 mean_patch_openai_clip_vit_large_patch14  \\\n",
       "228214  [0.6566806435585022, 0.8994556665420532, 0.425...   \n",
       "228215  [0.6567713618278503, 0.706050455570221, 0.4033...   \n",
       "228216  [0.8001006245613098, 0.9261599779129028, 0.368...   \n",
       "228217  [0.8816746473312378, 0.691596269607544, 0.1884...   \n",
       "228218  [0.7522355616092682, 0.7038511633872986, 0.364...   \n",
       "\n",
       "                     pooled_openai_clip_vit_large_patch14  \\\n",
       "228214  [0.40712353587150574, 0.6740058064460754, -0.0...   \n",
       "228215  [0.5280981063842773, 0.5120133757591248, 0.190...   \n",
       "228216  [0.34528201818466187, 0.702330470085144, 0.210...   \n",
       "228217  [0.2884502708911896, 0.38565462827682495, -0.0...   \n",
       "228218  [-0.0036218371242284775, 0.18161564506590366, ...   \n",
       "\n",
       "                                    embedding_e5_large_v2  \\\n",
       "228214  [0.026547163724899292, -0.05078291893005371, 0...   \n",
       "228215  [0.03120279870927334, -0.04832116514444351, 0....   \n",
       "228216  [0.01696249097585678, -0.058472465723752975, 0...   \n",
       "228217  [0.03428741917014122, -0.06396043300628662, 0....   \n",
       "228218  [0.03048939537256956, -0.06178726628422737, 0....   \n",
       "\n",
       "                               embedding_bge_large_en_v15  \\\n",
       "228214  [0.010242261923849583, -0.008357411250472069, ...   \n",
       "228215  [-0.02726542204618454, -0.004029609262943268, ...   \n",
       "228216  [-0.0057455929927527905, -0.000962883175816386...   \n",
       "228217  [0.007279624696820974, 0.006243441719561815, 0...   \n",
       "228218  [-0.0015129486564546824, 0.01574902841821313, ...   \n",
       "\n",
       "                           embedding_nomic_embed_text_v15  \\\n",
       "228214  [-0.01627330109477043, 0.03971276432275772, -0...   \n",
       "228215  [-0.024061240255832672, 0.05157441273331642, -...   \n",
       "228216  [0.03578423336148262, 0.022266210988163948, -0...   \n",
       "228217  [-0.036529459059238434, 0.03486456722021103, -...   \n",
       "228218  [0.005874196067452431, 0.08644555881619453, -0...   \n",
       "\n",
       "        articul_encrypred_id  \n",
       "228214                 80702  \n",
       "228215                176363  \n",
       "228216                 94224  \n",
       "228217                218574  \n",
       "228218                207628  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_products.copy()\n",
    "del df_products\n",
    "\n",
    "df['product_created_at_day'] = pd.to_datetime(df['product_created_at']).dt.floor('D')\n",
    "df['product_created_at_day'] = (df['product_created_at_day'] - pd.Timestamp('1970-01-01')) // pd.Timedelta(days=1)\n",
    "\n",
    "# Embedding columns\n",
    "embedding_cols = [col for col in df.columns if any(prefix in col for prefix in ['CLS_', 'mean_patch_', 'pooled_', 'embedding_'])]\n",
    "agg_dict = {'product_created_at_day': 'min', \n",
    "            'sales_total': 'sum'}\n",
    "# Mean by dimenshion for embeddings\n",
    "for col in embedding_cols:\n",
    "    agg_dict[col] = 'mean'\n",
    "df_products_articul = df.groupby('articul_encrypred').agg(agg_dict).reset_index()\n",
    "\n",
    "del df\n",
    "\n",
    "# Create ranked ID by descending sales volume\n",
    "articul_rank = df_products_articul[['articul_encrypred', 'sales_total']].drop_duplicates().sort_values('sales_total', ascending=False).reset_index(drop=True).assign(articul_encrypred_id=lambda x: x.index + 1)\n",
    "df_products_articul = df_products_articul.merge(articul_rank[['articul_encrypred', 'articul_encrypred_id']], on='articul_encrypred', how='left')\n",
    "\n",
    "df_products_articul.to_parquet(products_data_dir / 'df_products_articul.parquet', index=False)\n",
    "df_products_articul.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b125a",
   "metadata": {},
   "source": [
    "## Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab5214c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id_encrypred</th>\n",
       "      <th>articul_encrypred</th>\n",
       "      <th>order_date</th>\n",
       "      <th>store_encoded</th>\n",
       "      <th>articul_encrypred_id</th>\n",
       "      <th>sales_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3369678</th>\n",
       "      <td>wyyyyqqsyyvxvvtr</td>\n",
       "      <td>wyuutrv</td>\n",
       "      <td>2020-08-17</td>\n",
       "      <td>3</td>\n",
       "      <td>69858</td>\n",
       "      <td>453198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3369679</th>\n",
       "      <td>wyyyyqqsyyvxvvtr</td>\n",
       "      <td>wywqryu</td>\n",
       "      <td>2020-07-15</td>\n",
       "      <td>3</td>\n",
       "      <td>129264</td>\n",
       "      <td>155835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        anon_id_encrypred articul_encrypred  order_date  store_encoded  \\\n",
       "3369678  wyyyyqqsyyvxvvtr           wyuutrv  2020-08-17              3   \n",
       "3369679  wyyyyqqsyyvxvvtr           wywqryu  2020-07-15              3   \n",
       "\n",
       "         articul_encrypred_id  sales_total  \n",
       "3369678                 69858       453198  \n",
       "3369679                129264       155835  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales = pd.read_csv(raw_data_path / 'full_orders_v6.csv', sep=None, engine='python')[['anon_id_encrypred', 'articul_encrypred', 'order_date', 'store']]\n",
    "\n",
    "df_grouped = df_sales.groupby(['anon_id_encrypred', 'articul_encrypred', 'order_date']).agg(lambda x: x.mode().iloc[0]).reset_index() \n",
    "\n",
    "# Encode purchase channel\n",
    "store_encoding = {'E': 1, 'O': 2, 'T': 3, 'D': 4, 'B': 5} \n",
    "df_grouped['store_encoded'] = df_grouped['store'].map(store_encoding) \n",
    "\n",
    "# Add total sales by articul (sales total calculated in photo_embeddings.py)\n",
    "df_sales_articul = df_grouped.merge(df_products_articul[['articul_encrypred', 'articul_encrypred_id', 'sales_total']], on='articul_encrypred', how='left')\n",
    "df_sales_articul.drop(columns='store', inplace=True)\n",
    "\n",
    "# Drop articul_encrypred which are not represented in df_products_articul\n",
    "df_sales_articul.dropna(inplace=True)\n",
    "df_sales_articul['articul_encrypred_id'] = df_sales_articul['articul_encrypred_id'].astype(int)\n",
    "df_sales_articul['sales_total'] = df_sales_articul['sales_total'].astype(int)\n",
    "\n",
    "del df_grouped, df_sales\n",
    "\n",
    "df_sales_articul.to_parquet(products_data_dir / 'df_sales_articul.parquet', index=False)\n",
    "df_sales_articul.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64930a5f",
   "metadata": {},
   "source": [
    "Keep only the products that are present in the df_products dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25e145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique 'articul_encrypred' values — df_sales: 234460, df_products: 228219\n",
      "Unique 'product_id' values — df_sales: 314113, df_products: 306664\n",
      "\n",
      "Unique 'articul_encrypred' values (after filtering) — df_sales: 228219, df_products: 228219\n",
      "Unique 'product_id' values (after filtering) — df_sales: 306664, df_products: 306664\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Unique 'articul_encrypred' values — articul_encrypred: {len(articul_encrypred['articul_encrypred'].unique())}, df_products: {len(df_products['articul_encrypred'].unique())}\")\n",
    "# print(f\"Unique 'articul_encrypred' values — df_sales: {len(df_sales['articul_encrypred'].unique())}, df_products: {len(df_products['articul_encrypred'].unique())}\")\n",
    "\n",
    "# df_sales = df_sales.merge(\n",
    "#     df_products[['articul_encrypred']],\n",
    "#     on=['articul_encrypred'],\n",
    "#     how='inner'\n",
    "# )\n",
    "# print()\n",
    "# print(f\"Unique 'articul_encrypred' values (after filtering) — df_sales: {len(df_sales['articul_encrypred'].unique())}, df_products: {len(df_products['articul_encrypred'].unique())}\")\n",
    "# print(f\"Unique 'articul_encrypred' values (after filtering) — df_sales: {len(df_sales['articul_encrypred'].unique())}, df_products: {len(df_products['articul_encrypred'].unique())}\")\n",
    "\n",
    "# df_sales.to_parquet(interim_data_dir / 'df_sales.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba3df99b-df71-4157-8c56-0825361a43cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_purchase_days_per_user = 100  # Maximum number of purchase days allowed per user (a purchase = unique user-day pair)\n",
    "# min_purchases_per_user = 2       # Minimum number of purchases required per user (a purchase = user-item interaction)\n",
    "\n",
    "# # Exclude \"resellers\" — users with too many unique purchase days (e.g., buying 5 items on 1 day = 1 purchase)\n",
    "# purchase_days = df_sales.groupby('anon_id_encrypred')['order_date'].nunique().reset_index()\n",
    "# purchase_days.columns = ['anon_id_encrypred', 'unique_purchase_days']\n",
    "# resellers = purchase_days[purchase_days['unique_purchase_days'] > max_purchase_days_per_user]['anon_id_encrypred']\n",
    "\n",
    "# # Exclude users with too few purchases (e.g., buying 5 items on 1 day = 5 purchases)\n",
    "# user_purchase_counts = df_sales['anon_id_encrypred'].value_counts()\n",
    "\n",
    "# df_sales = (df_sales[~df_sales['anon_id_encrypred'].isin(resellers)]\n",
    "#                [df_sales['anon_id_encrypred'].isin(user_purchase_counts[user_purchase_counts >= min_purchases_per_user].index)]\n",
    "#            ).reset_index(drop=True)\n",
    "\n",
    "# df_sales.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c3fb1-d990-47f8-a8e3-aa0ff9260640",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76daa2ba-dbc4-43d1-b2f2-7217b2d6007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k_gpu(loader, k=10, device=device):\n",
    "    '''\n",
    "    Computes Precision@K for batched recommendation results using GPU.\n",
    "    '''\n",
    "    precision_sum, total_users = 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Precision@K'):\n",
    "            recs = batch['recs'][:, :k]     # [B, k]\n",
    "            true_pad = batch['true']        # [B, L]\n",
    "            true_len = batch['true_len']    # [B]\n",
    "\n",
    "            # hits_mask: [B, k] – True if the recommended item appears in the ground truth\n",
    "            # torch.isin performs elementwise comparison: [B, k, L], .any(-1) collapses last dim — \"was there at least one match\"\n",
    "            # hits_mask = torch.isin(\n",
    "            #     recs.unsqueeze(-1),       # [B, k, 1] – add dummy dimension\n",
    "            #     true_pad.unsqueeze(1)     # [B, 1, L_max] – reshape ground truth\n",
    "            # ).any(-1)\n",
    "\n",
    "            # !!! torch.isin with .any is not reliable on MPS (works fine on CUDA and even Mac CPU)\n",
    "            hits_mask = (recs.unsqueeze(-1) == true_pad.unsqueeze(1)).any(-1)\n",
    "\n",
    "            hits_cnt = hits_mask.sum(1).float()  # [B] – number of correct recommendations per user\n",
    "            denom = torch.minimum(true_len.clamp(min=1).float(), torch.tensor(k, device=device))  # min(k, true_len) – avoid penalizing users with few ground truth items\n",
    "            precision = hits_cnt / denom\n",
    "\n",
    "            precision_sum += precision.sum().item()\n",
    "            total_users += recs.size(0)\n",
    "\n",
    "    return precision_sum / total_users\n",
    "    \n",
    "\n",
    "def recall_at_k_gpu(loader, k=10, device=device):\n",
    "    '''\n",
    "    Computes Recall@K for batched recommendation results using GPU.\n",
    "    '''\n",
    "    recall_sum, total_users = 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Recall@K'):\n",
    "            recs      = batch['recs'][:, :k]      # [B, k]  top-k recommendations\n",
    "            true_pad  = batch['true']             # [B, L]  true items, padded with -1\n",
    "            true_len  = batch['true_len']         # [B]     lengths of true lists\n",
    "\n",
    "            # hits_mask: [B, k] – True if a recommendation matches any true item\n",
    "            # torch.isin compares [B, k, 1] with [B, 1, L], resulting in [B, k, L]; .any(-1) collapses over L to detect any match\n",
    "            # hits_mask = torch.isin(\n",
    "            #     recs.unsqueeze(-1),       # [B, k, 1] – expand recs\n",
    "            #     true_pad.unsqueeze(1)     # [B, 1, L] – expand ground truth\n",
    "            # ).any(-1)\n",
    "\n",
    "            # !!! torch.isin with .any does not work correctly on MPS (works on CUDA and Mac CPU)\n",
    "            hits_mask = (recs.unsqueeze(-1) == true_pad.unsqueeze(1)).any(-1)     \n",
    "\n",
    "            hits_cnt = hits_mask.sum(1).float()    # [B]  number of relevant recommendations\n",
    "\n",
    "            # recall = hits / |true|; skip users with no true items\n",
    "            valid_mask = true_len > 0              # [B]  boolean mask\n",
    "            recall = torch.zeros_like(hits_cnt)\n",
    "            recall[valid_mask] = hits_cnt[valid_mask] / true_len[valid_mask].float()\n",
    "\n",
    "            recall_sum  += recall.sum().item()\n",
    "            total_users += valid_mask.sum().item()\n",
    "\n",
    "    return recall_sum / total_users\n",
    "    \n",
    "\n",
    "def map_at_k_gpu(loader, k=10, device=device):\n",
    "    '''\n",
    "    Computes Mean Average Precision (MAP@K) for batched recommendation results using GPU.\n",
    "    '''\n",
    "    map_sum, total_users = 0.0, 0\n",
    "    positions = (torch.arange(k, device=device).float() + 1)  # [1 … k]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='MAP@K'):\n",
    "            recs      = batch['recs'][:, :k]      # [B, k]\n",
    "            true_pad  = batch['true']             # [B, L]\n",
    "            true_len  = batch['true_len']         # [B]\n",
    "\n",
    "            # hits_mask: [B, k] – True if a recommended item is in the ground truth\n",
    "            # torch.isin performs pairwise comparison and returns [B, k, L]; .any(-1) collapses last dim to check if any match exists\n",
    "            # hits_mask = torch.isin(\n",
    "            #     recs.unsqueeze(-1),       # [B, k, 1] – expand recs\n",
    "            #     true_pad.unsqueeze(1)     # [B, 1, L_max] – expand ground truth\n",
    "            # ).any(-1)\n",
    "\n",
    "            # !!! torch.isin with .any does not work correctly on MPS (works fine on CUDA and Mac CPU)\n",
    "            hits_mask = (recs.unsqueeze(-1) == true_pad.unsqueeze(1)).any(-1)     \n",
    "\n",
    "            # cum_hits: number of hits encountered up to position j\n",
    "            cum_hits  = torch.cumsum(hits_mask.float(), dim=1)   # [B, k]\n",
    "\n",
    "            # precisions@j are computed only at hit positions\n",
    "            precisions = (cum_hits * hits_mask.float()) / positions  # [B, k]\n",
    "\n",
    "            # AP = sum of precisions / min(|true|, k)\n",
    "            denom = torch.minimum(true_len.clamp(min=1).float(), torch.tensor(k, device=device)) # [B]\n",
    "            ap = precisions.sum(1) / denom # [B]\n",
    "\n",
    "            map_sum += ap.sum().item()\n",
    "            total_users += recs.size(0)\n",
    "\n",
    "    return map_sum / total_users\n",
    "\n",
    "\n",
    "def ndcg_at_k_gpu(loader, k=10, device=device):\n",
    "    '''\n",
    "    Computes Normalized Discounted Cumulative Gain (NDCG@K) for batched recommendation results using GPU.\n",
    "    '''\n",
    "    ndcg_sum, total_users = 0.0, 0\n",
    "    discount = 1.0 / torch.log2(torch.arange(k, device=device).float() + 2)  # [1/log2(i+2)]\n",
    "    ideal_cum = torch.cumsum(discount, dim=0)                                # prefix sum for IDCG\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='NDCG@K'):\n",
    "            recs = batch['recs'][:, :k]        # [B, k]\n",
    "            true_pad = batch['true']           # [B, L]\n",
    "            true_len = batch['true_len']       # [B]\n",
    "\n",
    "            # hits_mask: [B, k] – True if the recommended item appears in the ground truth\n",
    "            # torch.isin performs elementwise comparison [B, k, L]; .any(-1) checks if at least one match exists per position\n",
    "            # hits_mask = torch.isin(\n",
    "            #     recs.unsqueeze(-1),       # [B, k, 1]\n",
    "            #     true_pad.unsqueeze(1)     # [B, 1, L_max]\n",
    "            # ).any(-1)\n",
    "\n",
    "            # !!! torch.isin with .any does not work properly on MPS (works fine on CUDA and Mac CPU)\n",
    "            hits_mask = (recs.unsqueeze(-1) == true_pad.unsqueeze(1)).any(-1)     \n",
    "\n",
    "            # DCG: sum of discounted gains at hit positions\n",
    "            dcg = (hits_mask.float() * discount).sum(1)   # [B]\n",
    "\n",
    "            # IDCG: maximum possible DCG under ideal ranking\n",
    "            idcg_len = torch.minimum(true_len, torch.tensor(k, device=device))  # [B]\n",
    "            idcg = torch.zeros_like(dcg)\n",
    "            idcg[idcg_len > 0] = ideal_cum[idcg_len[idcg_len > 0] - 1]\n",
    "\n",
    "            # NDCG = DCG / IDCG (0 if IDCG == 0)\n",
    "            ndcg = torch.where(idcg > 0, dcg / idcg, torch.zeros_like(dcg))\n",
    "\n",
    "            ndcg_sum += ndcg.sum().item()\n",
    "            total_users += recs.size(0)\n",
    "\n",
    "    return ndcg_sum / total_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a47c837-d7ec-4526-ae3b-f39c8a630f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_results(model_name, precision_k, recall_k, map_k, ndcg_k, k, hyperparameters=None, round_level=4):\n",
    "    '''\n",
    "    Appends model evaluation results to the dataframe.\n",
    "    \n",
    "    model_name: str – name of the model\n",
    "    precision_k, recall_k, map_k, ndcg_k: torch.Tensor or float – evaluation metrics\n",
    "    k: int – value of K used in metrics\n",
    "    hyperparameters: dict – model hyperparameters (optional)\n",
    "    round_level: int – number of decimal places to round to (default is 4)\n",
    "    '''\n",
    "    global df_metrics\n",
    "    \n",
    "    # Convert tensors to floats and round them (if they are tensors)\n",
    "    precision_k = round(precision_k.item() if isinstance(precision_k, torch.Tensor) else precision_k, round_level)\n",
    "    recall_k = round(recall_k.item() if isinstance(recall_k, torch.Tensor) else recall_k, round_level)\n",
    "    map_k = round(map_k.item() if isinstance(map_k, torch.Tensor) else map_k, round_level)\n",
    "    ndcg_k = round(ndcg_k.item() if isinstance(ndcg_k, torch.Tensor) else ndcg_k, round_level)\n",
    "    \n",
    "    new_row = pd.DataFrame([{\n",
    "        'Model': model_name,\n",
    "        'k': k,\n",
    "        'Precision@k': precision_k,\n",
    "        'Recall@k': recall_k,\n",
    "        'MAP@k': map_k,\n",
    "        'NDCG@k': ndcg_k,\n",
    "        'Other_hyperparameters': hyperparameters\n",
    "    }])\n",
    "    \n",
    "    df_metrics = pd.concat([df_metrics, new_row], ignore_index=True)\n",
    "\n",
    "df_metrics = pd.DataFrame(columns=['Model', 'k', 'Precision@k', 'Recall@k', 'MAP@k', 'NDCG@k', 'Other_hyperparameters'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d3648-7b10-4194-86be-59363065dc73",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1259b4-022b-416e-8cb2-f955bfc6e142",
   "metadata": {},
   "source": [
    "## Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "929c50d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommendationDataset(Dataset):\n",
    "    def __init__(self, user_recommendations, user_to_true_items, k):\n",
    "        self.recommendations = [torch.LongTensor(recs[:k]) for recs in user_recommendations.values()]\n",
    "        self.true_items = [torch.LongTensor(list(user_to_true_items.get(u, []))) for u in user_recommendations.keys()]\n",
    "        self.true_len = [len(t) for t in self.true_items] \n",
    "        self.k = k\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.true_items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'recs': self.recommendations[idx], \n",
    "            'true': self.true_items[idx],\n",
    "            'true_len': self.true_len[idx]\n",
    "        }\n",
    "    \n",
    "\n",
    "def collate_fn(batch, device='mps'):\n",
    "    \"\"\"\n",
    "    Assembles the batch into full tensors so that all metrics\n",
    "    can be computed without Python loops (vectorized GPU).\n",
    "    Returns:\n",
    "        recs      – LongTensor [B, k]\n",
    "        true      – LongTensor [B, L_max] (padding = -100)\n",
    "        true_len  – LongTensor [B]        (length of the true list)\n",
    "    \"\"\"\n",
    "    recs = torch.stack([item['recs'] for item in batch])  # [B, k]\n",
    "    true_len = torch.tensor([item['true_len'] for item in batch], dtype=torch.long)\n",
    "    L_max = int(true_len.max()) if true_len.numel() else 0\n",
    "    true_pad = torch.full((len(batch), L_max), -100, dtype=torch.long)  # -100: sentinel\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        if true_len[i]:\n",
    "            true_pad[i, :true_len[i]] = item['true']\n",
    "\n",
    "    return {\n",
    "        'recs': recs.to(device),\n",
    "        'true': true_pad.to(device),\n",
    "        'true_len': true_len.to(device)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fa8898",
   "metadata": {},
   "source": [
    "**User-based temporal split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2512df89-a9ef-4132-b136-08aa155ee5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Сортируем данные по времени и бьём на трейн и тест\n",
    "# df_sales_articul['order_date'] = pd.to_datetime(df_sales_articul['order_date'])\n",
    "# df_sales_articul = df_sales_articul.sort_values(by=['anon_id_encrypred', 'order_date'])\n",
    "\n",
    "# train_data = []\n",
    "# test_data = []\n",
    "\n",
    "# for user, user_df in df_sales_articul.groupby('anon_id_encrypred'):\n",
    "#     split_idx = int(len(user_df) * 0.8)\n",
    "#     train_data.append(user_df.iloc[:split_idx])\n",
    "#     test_data.append(user_df.iloc[split_idx:])\n",
    "\n",
    "# train_df = pd.concat(train_data)\n",
    "# test_df = pd.concat(test_data)\n",
    "\n",
    "# df_sales_articul.to_csv(interim_data_dir / 'df_sales_articul.csv', index=False)\n",
    "# train_df.to_csv(interim_data_dir / 'train_data_by_users.csv', index=False)\n",
    "# test_df.to_csv(interim_data_dir / 'test_data_by_users.csv', index=False)\n",
    "\n",
    "# df_sales_articul = pd.read_csv(interim_data_dir / 'df_sales_articul.csv')\n",
    "# train_df = pd.read_csv(interim_data_dir / 'train_data_by_users.csv')\n",
    "# test_df = pd.read_csv(interim_data_dir / 'test_data_by_users.csv')\n",
    "\n",
    "# print(f\"Train shape: {train_df.shape}\")\n",
    "# print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# test_user_to_true_items = test_df.groupby('anon_id_encrypred')['articul_encrypred'].apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e5337b",
   "metadata": {},
   "source": [
    "**Global temporal split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "816c5351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min date: 2019-01-01 00:00:00\n",
      "Max date: 2021-02-18 00:00:00\n",
      "Threshold date (85.0%): 2020-10-24 00:00:00\n",
      "Train shape: (2361946, 6)\n",
      "Test shape: (445954, 6)\n",
      "\n",
      "Total number of users: 134362\n",
      "Number of users in the training set: 130282\n",
      "Number of users in the test set: 79468\n",
      "\n",
      "Total number of 'articul_encrypred_id': 222135\n",
      "Number of 'articul_encrypred_id' in the training set: 202924\n",
      "Number of 'articul_encrypred_id' in the test set: 99765\n"
     ]
    }
   ],
   "source": [
    "df_sales_articul['order_date'] = pd.to_datetime(df_sales_articul['order_date'])\n",
    "df_sales_articul = df_sales_articul.sort_values(by=['anon_id_encrypred', 'order_date'])\n",
    "\n",
    "# Leave only users with 5+ purchases (for decreasing the noise)\n",
    "user_counts = df_sales_articul['anon_id_encrypred'].value_counts()\n",
    "active_users = user_counts[user_counts >= 5].index\n",
    "df_sales_articul = df_sales_articul[df_sales_articul['anon_id_encrypred'].isin(active_users)]\n",
    "\n",
    "threshold_level = 0.85\n",
    "min_date = df_sales_articul['order_date'].min()\n",
    "max_date = df_sales_articul['order_date'].max()\n",
    "\n",
    "print(f\"Min date: {min_date}\")\n",
    "print(f\"Max date: {max_date}\")\n",
    "\n",
    "total_days = (max_date - min_date).days\n",
    "threshold_days = int(total_days * threshold_level)\n",
    "threshold_date = min_date + pd.Timedelta(days=threshold_days)\n",
    "\n",
    "print(f\"Threshold date ({round(threshold_level * 100, 0)}%): {threshold_date}\")\n",
    "\n",
    "train_df = df_sales_articul[df_sales_articul['order_date'] < threshold_date]\n",
    "test_df = df_sales_articul[df_sales_articul['order_date'] >= threshold_date]\n",
    "\n",
    "# df_sales_articul.to_csv(interim_data_dir / 'df_sales_articul.csv', index=False)\n",
    "# train_df.to_csv(interim_data_dir / 'train_data_by_threshold_date.csv', index=False)\n",
    "# test_df.to_csv(interim_data_dir / 'test_data_by_threshold_date.csv', index=False)\n",
    "\n",
    "# df_sales_articul = pd.read_csv(interim_data_dir / 'df_sales_articul.csv')\n",
    "# train_df = pd.read_csv(interim_data_dir / 'train_data_by_threshold_date.csv')\n",
    "# test_df = pd.read_csv(interim_data_dir / 'test_data_by_threshold_date.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print()\n",
    "print(f\"Total number of users: {len(df_sales_articul['anon_id_encrypred'].unique())}\")\n",
    "print(f\"Number of users in the training set: {len(train_df['anon_id_encrypred'].unique())}\")\n",
    "print(f\"Number of users in the test set: {len(test_df['anon_id_encrypred'].unique())}\")\n",
    "print()\n",
    "print(f\"Total number of 'articul_encrypred_id': {len(df_sales_articul['articul_encrypred_id'].unique())}\")\n",
    "print(f\"Number of 'articul_encrypred_id' in the training set: {len(train_df['articul_encrypred_id'].unique())}\")\n",
    "print(f\"Number of 'articul_encrypred_id' in the test set: {len(test_df['articul_encrypred_id'].unique())}\")\n",
    "\n",
    "test_user_to_true_items = test_df.groupby('anon_id_encrypred')['articul_encrypred_id'].apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83dfa9",
   "metadata": {},
   "source": [
    "**Config for simple models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30cda962",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "k = 10\n",
    "round_level = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46edc970-1bd8-411a-a29f-10c11418a180",
   "metadata": {},
   "source": [
    "### 1. Top-K Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c19127f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing saved recommendations\n",
    "with open(models_outputs_dir / 'top_sales' / 'user_recommendations_top_k.pkl', \"rb\") as f:\n",
    "    user_recommendations_top_k = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12aa9a3e-3dd8-4ecd-ab58-a28a3e1d5031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54fa985e8114b42a6b816e11af413c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generation of new recommendations\n",
    "popular_items = train_df['articul_encrypred_id'].value_counts().index.tolist()\n",
    "\n",
    "def recommend_top_k(top_k_items=k):\n",
    "    return popular_items[:top_k_items]\n",
    "\n",
    "user_recommendations_top_k = {user: recommend_top_k(top_k_items=k) for user in tqdm(test_df['anon_id_encrypred'].unique())}\n",
    "\n",
    "with open(models_outputs_dir / 'top_sales' / 'user_recommendations_top_k.pkl', 'wb') as f:\n",
    "    pickle.dump(user_recommendations_top_k, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32918cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_top_k = RecommendationDataset(user_recommendations=user_recommendations_top_k, user_to_true_items=test_user_to_true_items, k=k)\n",
    "loader = DataLoader(dataset_top_k, batch_size=batch_size, num_workers=0, \n",
    "                    collate_fn=lambda batch: collate_fn(batch, device='mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3cf2548-9d9b-482d-936d-b017c7766712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27797fa9db464d01ac4f46b120566fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Precision@K:   0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10 for Top 10 Recommender model: 0.00577893\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9fd63d5710443b8d663c9d120c7665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recall@K:   0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 for Top 10 Recommender model: 0.005419\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb884ae9a624d29ae1ad365c4496c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MAP@K:   0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@10 for Top 10 Recommender model: 0.00263268\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc43896b2504120adb23254d66bebeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NDCG@K:   0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@10 for Top 10 Recommender model: 0.00483357\n"
     ]
    }
   ],
   "source": [
    "precision_k = precision_at_k_gpu(loader=loader, k=k)\n",
    "print(f'Precision@{k} for Top {k} Recommender model: {precision_k:.8f}')\n",
    "\n",
    "recall_k = recall_at_k_gpu(loader=loader, k=k)\n",
    "print(f'Recall@{k} for Top {k} Recommender model: {recall_k:8f}')\n",
    "\n",
    "map_k = map_at_k_gpu(loader=loader, k=k)\n",
    "print(f'MAP@{k} for Top {k} Recommender model: {map_k:.8f}')\n",
    "\n",
    "ndcg_k = ndcg_at_k_gpu(loader=loader, k=k)\n",
    "print(f'NDCG@{k} for Top {k} Recommender model: {ndcg_k:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c85fe4e-ef69-4bc8-aaeb-15879d72bfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>k</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>MAP@k</th>\n",
       "      <th>NDCG@k</th>\n",
       "      <th>Other_hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top-K</td>\n",
       "      <td>10</td>\n",
       "      <td>0.005779</td>\n",
       "      <td>0.005419</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model   k  Precision@k  Recall@k     MAP@k    NDCG@k Other_hyperparameters\n",
       "0  Top-K  10     0.005779  0.005419  0.002633  0.004834                  None"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model_results(model_name='Top-K', k=k, precision_k=precision_k, recall_k=recall_k, map_k=map_k, ndcg_k=ndcg_k, hyperparameters=None, round_level=round_level)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996d325-c77b-434d-ba15-eb17e2608809",
   "metadata": {},
   "source": [
    "### 2. Random Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9afb51c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing saved recommendations\n",
    "with open(models_outputs_dir / 'random' / 'user_recommendations_random.pkl', \"rb\") as f:\n",
    "    user_recommendations_random = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb35dc09-640d-46ed-ae31-0f4b3cc6e22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504777ae8a2b48c0a5e77cd04270f424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Assigning recommendations:   0%|          | 0/79468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generation of new recommendations\n",
    "def recommend_random(df, top_k_items=k):\n",
    "    return np.random.choice(df.unique(), size=min(top_k_items, len(df.unique())), replace=False)\n",
    "\n",
    "random_recommendations = recommend_random(df=train_df['articul_encrypred_id'], top_k_items=k)\n",
    "user_recommendations_random = {user: random_recommendations for user in tqdm(test_df['anon_id_encrypred'].unique(), desc='Assigning recommendations')}\n",
    "\n",
    "with open(models_outputs_dir / 'random' / 'user_recommendations_random.pkl', 'wb') as f:\n",
    "    pickle.dump(user_recommendations_random, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3389614",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_random = RecommendationDataset(user_recommendations=user_recommendations_random, user_to_true_items=test_user_to_true_items, k=k)\n",
    "loader = DataLoader(dataset_random, batch_size=batch_size, num_workers=0, \n",
    "                    collate_fn=lambda batch: collate_fn(batch, device='mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfdb09d3-1cff-4823-b614-a75af109b3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a65a5a9708343d3a81f7291c39b8870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Precision@K:   0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10 for Random Recommender model: 0.00003272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e5f0d5d2604a599b6d1d518cab7b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recall@K:   0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 for Random Recommender model: 0.000032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6befee51d2464453867f9c3a2253f766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MAP@K:   0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@10 for Random Recommender model: 0.00000534\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4de06b35eb4ac9aceb2f5202355589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NDCG@K:   0%|          | 0/621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@10 for Random Recommender model: 0.00001315\n"
     ]
    }
   ],
   "source": [
    "precision_k = precision_at_k_gpu(loader=loader, k=k)\n",
    "print(f'Precision@{k} for Random Recommender model: {precision_k:.8f}')\n",
    "\n",
    "recall_k = recall_at_k_gpu(loader=loader, k=k)\n",
    "print(f'Recall@{k} for Random Recommender model: {recall_k:8f}')\n",
    "\n",
    "map_k = map_at_k_gpu(loader=loader, k=k)\n",
    "print(f'MAP@{k} for Random Recommender model: {map_k:.8f}')\n",
    "\n",
    "ndcg_k = ndcg_at_k_gpu(loader=loader, k=k)\n",
    "print(f'NDCG@{k} for Random Recommender model: {ndcg_k:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d17403b2-e74b-4e05-bccd-50f65e603fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>k</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>MAP@k</th>\n",
       "      <th>NDCG@k</th>\n",
       "      <th>Other_hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top-K</td>\n",
       "      <td>10</td>\n",
       "      <td>0.005779</td>\n",
       "      <td>0.005419</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model   k  Precision@k  Recall@k     MAP@k    NDCG@k Other_hyperparameters\n",
       "0   Top-K  10     0.005779  0.005419  0.002633  0.004834                  None\n",
       "1  Random  10     0.000033  0.000032  0.000005  0.000013                  None"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model_results(model_name='Random', k=k, precision_k=precision_k, recall_k=recall_k, map_k=map_k, ndcg_k=ndcg_k, hyperparameters=None, round_level=round_level)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a861ad65-6435-47bf-ad54-085126a7ba8a",
   "metadata": {},
   "source": [
    "### 3. User-Based Collaborative Filtering (UBCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb4fae8f-9af4-4e6b-a35a-03ad6457aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20  # top_n_similar_users\n",
    "filter_already_purchased = True\n",
    "n_iter_x5_top_n_similar_users = 1\n",
    "# Important!!! If you're using a GPU desktop — comment out the line below\n",
    "device = torch.device(\"cpu\")  # Sparse ops on MPS (MacBook) are not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd9c7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Function to create a user-item matrix\n",
    "def create_user_item_matrix(df):\n",
    "    \"\"\"\n",
    "    Returns a sparse matrix (csr_matrix) and index mappings \n",
    "    for users and products (user_labels, product_labels) in the exact order of factorize.\n",
    "    \"\"\"\n",
    "    user_item_counts = df.groupby(['anon_id_encrypred', 'articul_encrypred_id']).size().reset_index(name='count')\n",
    "    \n",
    "    # Factorize users and products\n",
    "    u_codes, u_labels = pd.factorize(user_item_counts['anon_id_encrypred'])\n",
    "    p_codes, p_labels = pd.factorize(user_item_counts['articul_encrypred_id'])\n",
    "    \n",
    "    data = user_item_counts['count'].values\n",
    "    \n",
    "    # Create a sparse matrix (rows = users, columns = products)\n",
    "    user_item_matrix = csr_matrix((data, (u_codes, p_codes)))\n",
    "    \n",
    "    return user_item_matrix, u_labels, p_labels\n",
    "\n",
    "\n",
    "# 2. Function to compute user similarity matrix on GPU\n",
    "def compute_user_similarity(df):\n",
    "    \"\"\"\n",
    "    Creates a user-item matrix, applies log-smoothing to counts,\n",
    "    converts it to a PyTorch sparse tensor, and computes cosine similarity.\n",
    "    Returns user_similarity (result of sparse.mm) and user_labels.\n",
    "    \"\"\"\n",
    "    # Create user-item matrix\n",
    "    user_item_matrix, user_labels, product_labels = create_user_item_matrix(df)\n",
    "    \n",
    "    # Apply log(1 + count) to reduce the influence of frequent purchases\n",
    "    user_item_matrix.data = np.log1p(user_item_matrix.data)\n",
    "    \n",
    "    # Convert sparse matrix to PyTorch sparse_coo_tensor\n",
    "    # indices:  shape = (2, number of non-zero elements)\n",
    "    # values:   shape = (number of non-zero elements,)\n",
    "    # size:     (number of users, number of products)\n",
    "    coo_indices = np.vstack(user_item_matrix.nonzero())\n",
    "    coo_values = user_item_matrix.data\n",
    "    \n",
    "    user_item_tensor = torch.sparse_coo_tensor(\n",
    "        torch.tensor(coo_indices, dtype=torch.long),\n",
    "        torch.tensor(coo_values, dtype=torch.float32),\n",
    "        size=user_item_matrix.shape\n",
    "    ).coalesce().to(device)\n",
    "    \n",
    "    # Normalize users row-wise for cosine similarity\n",
    "    # row_norms.shape = (num_users,)\n",
    "    row_norms = torch.sqrt(torch.sparse.sum(user_item_tensor.pow(2), dim=1).to_dense())\n",
    "    row_norms[row_norms == 0] = 1.0\n",
    "    \n",
    "    # Divide each value by the norm of its corresponding row\n",
    "    # user_item_tensor.indices()[0] = row indices (users)\n",
    "    normalized_values = user_item_tensor.values() / row_norms[user_item_tensor.indices()[0]]\n",
    "    user_item_tensor_normalized = torch.sparse_coo_tensor(\n",
    "        user_item_tensor.indices(),\n",
    "        normalized_values,\n",
    "        size=user_item_tensor.size()\n",
    "    ).coalesce()\n",
    "    \n",
    "    # Compute cosine similarity matrix as M * M^T\n",
    "    user_similarity = torch.sparse.mm(user_item_tensor_normalized, user_item_tensor_normalized.t())\n",
    "    \n",
    "    return user_similarity, user_labels  # product_labels not needed for UBCF\n",
    "\n",
    "\n",
    "# 3. Recommendation function (User-Based CF) on GPU with row-wise processing\n",
    "def recommend_user_based_batch(\n",
    "    user_ids,\n",
    "    user_similarity,\n",
    "    user_labels,\n",
    "    df,\n",
    "    top_k_items=k,\n",
    "    top_n_similar_users=n,\n",
    "    batch_size=batch_size,\n",
    "    filter_already_purchased=False,\n",
    "    n_iter_x5_top_n_similar_users=None\n",
    "):\n",
    "    \"\"\"\n",
    "    For a list of user_ids, returns recommendations based on top-N similar users.\n",
    "    \"\"\"\n",
    "    # For fast user index lookup by user_id\n",
    "    user_labels_index = pd.Index(user_labels)\n",
    "    \n",
    "    # Dictionary of all user purchases\n",
    "    user_purchases = df.groupby('anon_id_encrypred')['articul_encrypred_id'].apply(set).to_dict()\n",
    "\n",
    "    # Precompute popular items (store more than top_k_items for filtering)\n",
    "    most_popular_items = df['articul_encrypred_id'].value_counts().index.tolist()\n",
    "    \n",
    "    # Dictionary: user_id -> list of recommended items\n",
    "    recommendations = {}\n",
    "    \n",
    "    # Split user_ids into batches (to avoid allocating the full dense matrix)\n",
    "    for start_idx in tqdm(range(0, len(user_ids), batch_size), desc='Generating recommendations'):\n",
    "        batch_user_ids = user_ids[start_idx : start_idx + batch_size]\n",
    "    \n",
    "        # For each user in the batch, extract their similarity row (sparse)\n",
    "        for user_id in batch_user_ids:\n",
    "            # Get user index in the user_similarity matrix\n",
    "            try:\n",
    "                u_idx = user_labels_index.get_loc(user_id)\n",
    "            except KeyError:\n",
    "                # If user_id is not found in user_labels\n",
    "                recommendations[user_id] = np.array(most_popular_items[:top_k_items])\n",
    "                continue\n",
    "    \n",
    "            # Extract row u_idx from user_similarity — shape (1, num_users) (sparse)\n",
    "            row_sparse = user_similarity[u_idx]  # submatrix (1, U)\n",
    "            # Convert it to a dense vector [U] for this user only\n",
    "            row_dense = row_sparse.to_dense().flatten()  # shape = [num_users]\n",
    "    \n",
    "            # Start with base `top_n_similar_users`\n",
    "            num_similar_users = top_n_similar_users\n",
    "            recommended_products = []\n",
    "            expansion_step = 0  # Number of expansion iterations\n",
    "    \n",
    "            # Check if dynamic expansion of top_n_similar_users is needed\n",
    "            while (len(recommended_products) < top_k_items \n",
    "                    and n_iter_x5_top_n_similar_users is not None \n",
    "                    and n_iter_x5_top_n_similar_users > 0 \n",
    "                    and expansion_step <= n_iter_x5_top_n_similar_users):\n",
    "    \n",
    "                # Find top-N similar users (dynamically increasing N)\n",
    "                similar_users_scores, similar_users_indices = torch.topk(row_dense, num_similar_users + 1, dim=0)\n",
    "    \n",
    "                # Exclude the user themselves (similarity with self = 1)\n",
    "                mask = (similar_users_indices != u_idx)\n",
    "                similar_users_indices = similar_users_indices[mask]\n",
    "    \n",
    "                # Convert similar user indices to user_ids\n",
    "                similar_users_ids = user_labels[similar_users_indices.cpu().numpy()]\n",
    "                user_bought = user_purchases.get(user_id, set())\n",
    "                # user_bought = user_purchases.get(user_id)\n",
    "                # if not user_bought:\n",
    "                #     print(f\"[DEBUG] User {user_id} has no purchases in train\")\n",
    "                #     user_bought = set()\n",
    "    \n",
    "                # Collect unique items from similar users\n",
    "                product_counter = Counter()  # Count item frequencies among similar users\n",
    "    \n",
    "                for sim_u in similar_users_ids:\n",
    "                    sim_bought = user_purchases.get(sim_u, set())\n",
    "                    # Filter only if filter_already_purchased=True\n",
    "                    new_items = sim_bought - user_bought if filter_already_purchased else sim_bought\n",
    "                    product_counter.update(new_items)  # Update item counts\n",
    "    \n",
    "                # Take top-K most frequent items\n",
    "                recommended_products = [p for p, _ in product_counter.most_common(top_k_items)]\n",
    "    \n",
    "                if len(recommended_products) >= top_k_items:\n",
    "                    break  # Stop if enough items collected\n",
    "    \n",
    "                # Increase number of similar users by 5×\n",
    "                num_similar_users *= 5\n",
    "                expansion_step += 1\n",
    "    \n",
    "            # If n_iter_x5_top_n_similar_users=None → do not expand\n",
    "            if n_iter_x5_top_n_similar_users is None or n_iter_x5_top_n_similar_users <= 0:\n",
    "                similar_users_scores, similar_users_indices = torch.topk(row_dense, top_n_similar_users + 1, dim=0)\n",
    "                mask = (similar_users_indices != u_idx)\n",
    "                similar_users_indices = similar_users_indices[mask][:top_n_similar_users]\n",
    "    \n",
    "                similar_users_ids = user_labels[similar_users_indices.cpu().numpy()]\n",
    "                user_bought = user_purchases.get(user_id, set())\n",
    "    \n",
    "                product_counter = Counter()\n",
    "                for sim_u in similar_users_ids:\n",
    "                    sim_bought = user_purchases.get(sim_u, set())\n",
    "                    new_items = sim_bought - user_bought if filter_already_purchased else sim_bought\n",
    "                    product_counter.update(new_items)  # Update item counts\n",
    "    \n",
    "                recommended_products = [p for p, _ in product_counter.most_common(top_k_items)]\n",
    "    \n",
    "            # If not enough items → fill with popular items\n",
    "            if len(recommended_products) < top_k_items:\n",
    "                needed_items = top_k_items - len(recommended_products)\n",
    "                # Use generator expression + islice for efficient cutoff\n",
    "                additional_items = list(itertools.islice(\n",
    "                    (p for p in most_popular_items if p not in recommended_products and \n",
    "                     (p not in user_bought if filter_already_purchased else True)),  \n",
    "                    needed_items\n",
    "                ))\n",
    "                if len(additional_items) == 0:\n",
    "                    print(f\"[WARN] No fallback items available for user {user_id} after filtering\")\n",
    "                recommended_products.extend(additional_items)\n",
    "            # Safety check — final fallback\n",
    "            if not recommended_products:\n",
    "                print(f\"[Warning] Fallback failed for user {user_id}. Filling with top-{top_k_items} popular items.\")\n",
    "                recommended_products = most_popular_items[:top_k_items]\n",
    "            # Store result as np.array\n",
    "            recommendations[user_id] = np.array(list(recommended_products)[:top_k_items])\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "612f2a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing saved recommendations\n",
    "with open(models_outputs_dir / 'user_based' / 'user_recommendations_ubcf.pkl', \"rb\") as f:\n",
    "    user_recommendations_ubcf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf2811c5-4abb-46c5-8469-359c3bc21d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0707a979ceee4e46aa494e586ab87f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating recommendations:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generation of new recommendations\n",
    "# 1) Compute user similarity matrix on GPU\n",
    "user_similarity, user_labels = compute_user_similarity(train_df)\n",
    "\n",
    "# 2) Generate recommendations for all users\n",
    "user_ids = test_df['anon_id_encrypred'].unique()\n",
    "user_recommendations_ubcf = recommend_user_based_batch(\n",
    "    user_ids=user_ids[:len(user_ids) // 50],\n",
    "    user_similarity=user_similarity,\n",
    "    user_labels=user_labels,\n",
    "    df=train_df,\n",
    "    top_k_items=k,\n",
    "    top_n_similar_users=n,\n",
    "    batch_size=batch_size,\n",
    "    filter_already_purchased=filter_already_purchased, \n",
    "    n_iter_x5_top_n_similar_users=n_iter_x5_top_n_similar_users\n",
    ")\n",
    "\n",
    "with open(models_outputs_dir / 'user_based' / 'user_recommendations_ubcf.pkl', 'wb') as f:\n",
    "    pickle.dump(user_recommendations_ubcf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f5fad0c-74ca-403f-8e7a-1a512c642b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ubcf = RecommendationDataset(user_recommendations=user_recommendations_ubcf, user_to_true_items=test_user_to_true_items, k=k)\n",
    "loader = DataLoader(dataset_ubcf, batch_size=batch_size, num_workers=0, \n",
    "                    collate_fn=lambda batch: collate_fn(batch, device='mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30715fd4-48dc-4dfb-b9dd-0adc7155fe4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2abf5efae964e9ca5ad5c927ccabb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Precision@K:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10 for UBCF model: 0.00323829\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50309dceebc346e2915e3dbc836e37da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recall@K:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 for UBCF model: 0.003224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc95344135344dbb1684366a57400dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MAP@K:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@10 for UBCF model: 0.00172968\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4a324ca75f484cac9f1efbf3af32da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NDCG@K:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@10 for UBCF model: 0.00273970\n"
     ]
    }
   ],
   "source": [
    "precision_k = precision_at_k_gpu(loader=loader, k=k)\n",
    "print(f'Precision@{k} for UBCF model: {precision_k:.8f}')\n",
    "\n",
    "recall_k = recall_at_k_gpu(loader=loader, k=k)\n",
    "print(f'Recall@{k} for UBCF model: {recall_k:8f}')\n",
    "\n",
    "map_k = map_at_k_gpu(loader=loader, k=k)\n",
    "print(f'MAP@{k} for UBCF model: {map_k:.8f}')\n",
    "\n",
    "ndcg_k = ndcg_at_k_gpu(loader=loader, k=k)\n",
    "print(f'NDCG@{k} for UBCF model: {ndcg_k:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c273a11-ce6b-499c-97a1-e2a7b8334d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>k</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>MAP@k</th>\n",
       "      <th>NDCG@k</th>\n",
       "      <th>Other_hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top-K</td>\n",
       "      <td>10</td>\n",
       "      <td>0.005779</td>\n",
       "      <td>0.005419</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UBCF</td>\n",
       "      <td>10</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.003224</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>{'top_k_items': 10, 'top_n_similar_users': 20,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model   k  Precision@k  Recall@k     MAP@k    NDCG@k  \\\n",
       "0   Top-K  10     0.005779  0.005419  0.002633  0.004834   \n",
       "1  Random  10     0.000033  0.000032  0.000005  0.000013   \n",
       "2    UBCF  10     0.003238  0.003224  0.001730  0.002740   \n",
       "\n",
       "                               Other_hyperparameters  \n",
       "0                                               None  \n",
       "1                                               None  \n",
       "2  {'top_k_items': 10, 'top_n_similar_users': 20,...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model_results(model_name='UBCF', k=k, precision_k=precision_k, recall_k=recall_k, map_k=map_k, ndcg_k=ndcg_k, round_level=round_level, \n",
    "                  hyperparameters={'top_k_items': k, 'top_n_similar_users': n, \n",
    "                                   'filter_already_purchased': filter_already_purchased, 'n_iter_x5_top_n_similar_users': n_iter_x5_top_n_similar_users})\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab1e79-dc3b-4da9-bf3c-deeeacd47e72",
   "metadata": {},
   "source": [
    "### 4. Item-Based Collaborative Filtering (IBCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0a7c584-a282-44da-88ce-b0da9e4455df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20  # top_n_similar_items\n",
    "filter_already_purchased = True\n",
    "n_iter_x5_top_n_similar_items = 1\n",
    "# Important!!! If you're using a GPU desktop — comment out the line below\n",
    "device = torch.device(\"cpu\")  # Sparse ops on MPS (MacBook) are not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f436e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_matrix(df):\n",
    "    \"\"\"\n",
    "    Returns a sparse matrix (csr_matrix) and index mappings \n",
    "    for users and products (user_labels, product_labels) in the exact order of factorize.\n",
    "    \"\"\"\n",
    "    user_item_counts = df.groupby(['anon_id_encrypred', 'articul_encrypred_id']).size().reset_index(name='count')\n",
    "    \n",
    "    # Factorize users and products\n",
    "    u_codes, u_labels = pd.factorize(user_item_counts['anon_id_encrypred'])\n",
    "    p_codes, p_labels = pd.factorize(user_item_counts['articul_encrypred_id'])\n",
    "    \n",
    "    data = user_item_counts['count'].values\n",
    "    \n",
    "    # Create a sparse matrix (rows = users, columns = products)\n",
    "    user_item_matrix = csr_matrix((data, (u_codes, p_codes)))\n",
    "    \n",
    "    return user_item_matrix, u_labels, p_labels\n",
    "\n",
    "\n",
    "def compute_item_similarity(df):\n",
    "    \"\"\"\n",
    "    Creates a user-item matrix, applies log-smoothing to counts,\n",
    "    transposes it to (items x users), and computes cosine similarity between items.\n",
    "    Returns item_similarity (result of sparse.mm) and product_labels.\n",
    "    \"\"\"\n",
    "    # 1) Create the user-item matrix\n",
    "    user_item_matrix, user_labels, product_labels = create_user_item_matrix(df)\n",
    "    \n",
    "    # Apply log(1 + count) to reduce the influence of frequent purchases\n",
    "    user_item_matrix.data = np.log1p(user_item_matrix.data)\n",
    "    \n",
    "    # 2) Convert to PyTorch sparse_coo_tensor\n",
    "    coo_indices = np.vstack(user_item_matrix.nonzero())\n",
    "    coo_values = user_item_matrix.data\n",
    "    \n",
    "    # user_item_matrix.shape = (num_users, num_items)\n",
    "    user_item_tensor = torch.sparse_coo_tensor(\n",
    "        torch.tensor(coo_indices, dtype=torch.long),\n",
    "        torch.tensor(coo_values, dtype=torch.float32),\n",
    "        size=user_item_matrix.shape\n",
    "    ).coalesce().to(device)\n",
    "    \n",
    "    # 3) Transpose to get the item-user matrix (shape = num_items × num_users)\n",
    "    # In PyTorch, use sparse.transpose(dim0=0, dim1=1)\n",
    "    item_user_tensor = user_item_tensor.transpose(0, 1).coalesce()\n",
    "    \n",
    "    # 4) Normalize each row (i.e., item) for cosine similarity\n",
    "    # Compute L2 norm of each row\n",
    "    row_norms = torch.sqrt(torch.sparse.sum(item_user_tensor.pow(2), dim=1).to_dense())\n",
    "    row_norms[row_norms == 0] = 1.0\n",
    "    \n",
    "    # Divide values by the norm of the corresponding row (item)\n",
    "    normalized_values = item_user_tensor.values() / row_norms[item_user_tensor.indices()[0]]\n",
    "    item_user_tensor_normalized = torch.sparse_coo_tensor(\n",
    "        item_user_tensor.indices(),\n",
    "        normalized_values,\n",
    "        size=item_user_tensor.size()\n",
    "    ).coalesce()\n",
    "    \n",
    "    # 5) Compute item-item similarity matrix: M * M^T\n",
    "    item_similarity = torch.sparse.mm(item_user_tensor_normalized, item_user_tensor_normalized.t())\n",
    "    \n",
    "    # Return item similarity matrix and product_labels\n",
    "    # (user_labels are not needed for item-based CF)\n",
    "    return item_similarity, product_labels\n",
    "\n",
    "\n",
    "def recommend_item_based_batch(\n",
    "    user_ids,\n",
    "    item_similarity,\n",
    "    item_labels,\n",
    "    df,\n",
    "    top_k_items=k,                  # Number of items to recommend\n",
    "    top_n_similar_items=n,          # Number of similar items to consider for each purchased item\n",
    "    batch_size=batch_size,\n",
    "    filter_already_purchased=False,\n",
    "    n_iter_x5_top_n_similar_items=None  # Same idea as n_iter_x5_top_n_similar_users\n",
    "):\n",
    "    \"\"\"\n",
    "    For a list of user_ids, returns item-based recommendations.\n",
    "    Similar to user-based, but instead of finding top-N similar users,\n",
    "    we find top-N similar items for each item purchased by the user.\n",
    "    \"\"\"\n",
    "    # For fast lookup of item index by articul_encrypred_id\n",
    "    item_labels_index = pd.Index(item_labels)\n",
    "    \n",
    "    # Dictionary of user purchases: anon_id_encrypred -> set(articul_encrypred_id)\n",
    "    user_purchases = df.groupby('anon_id_encrypred')['articul_encrypred_id'].apply(set).to_dict()\n",
    "    \n",
    "    # Dictionary: user_id -> list of recommended items\n",
    "    recommendations = {}\n",
    "    \n",
    "    # Precompute most popular items (used as fallback)\n",
    "    most_popular_items_all = df['articul_encrypred_id'].value_counts().index.tolist()\n",
    "    \n",
    "    # Process user_ids in batches\n",
    "    for start_idx in tqdm(range(0, len(user_ids), batch_size), desc='Generating Item-Based recommendations'):\n",
    "        batch_user_ids = user_ids[start_idx : start_idx + batch_size]\n",
    "        \n",
    "        for user_id in batch_user_ids:\n",
    "            user_bought = user_purchases.get(user_id, set())\n",
    "            if len(user_bought) == 0:\n",
    "                # If the user has no purchases, recommend popular items\n",
    "                needed_items = top_k_items\n",
    "                additional_items = list(itertools.islice(\n",
    "                    (p for p in most_popular_items_all \n",
    "                     if (p not in user_bought if filter_already_purchased else True)),\n",
    "                    needed_items\n",
    "                ))\n",
    "                recommendations[user_id] = np.array(additional_items)\n",
    "                continue\n",
    "            \n",
    "            product_counter = Counter()\n",
    "\n",
    "            valid_purchased_items = [item for item in user_bought if item in item_labels_index]\n",
    "            if not valid_purchased_items:\n",
    "                recommendations[user_id] = np.array(most_popular_items_all[:top_k_items])\n",
    "                continue\n",
    "            \n",
    "            # For each item the user has purchased, find similar items\n",
    "            for purchased_item in valid_purchased_items:\n",
    "                # Look up index of purchased_item in item_labels_index\n",
    "                try:\n",
    "                    i_idx = item_labels_index.get_loc(purchased_item)\n",
    "                except KeyError:\n",
    "                    # Skip if the item is not found in item_labels\n",
    "                    continue\n",
    "                \n",
    "                row_sparse = item_similarity[i_idx]  # submatrix (1, num_items) (sparse)\n",
    "                row_dense = row_sparse.to_dense().flatten()\n",
    "                \n",
    "                # Start with base top_n_similar_items\n",
    "                num_similar_items = top_n_similar_items\n",
    "                similar_items_list = []\n",
    "                expansion_step = 0\n",
    "                \n",
    "                # Same logic as user-based: expand until we gather top_k_items\n",
    "                while (len(similar_items_list) < top_k_items\n",
    "                       and n_iter_x5_top_n_similar_items is not None\n",
    "                       and n_iter_x5_top_n_similar_items > 0\n",
    "                       and expansion_step <= n_iter_x5_top_n_similar_items):\n",
    "                    \n",
    "                    # Get top-(num_similar_items+1)\n",
    "                    # (+1 to exclude the item itself, if included)\n",
    "                    sim_scores, sim_indices = torch.topk(row_dense, num_similar_items + 1, dim=0)\n",
    "                    \n",
    "                    # Remove the item itself if it's in the list\n",
    "                    mask = (sim_indices != i_idx)\n",
    "                    sim_indices = sim_indices[mask]\n",
    "                    \n",
    "                    # Convert indices to articul_encrypred_id\n",
    "                    similar_item_ids = item_labels[sim_indices.cpu().numpy()]\n",
    "                    \n",
    "                    # If filter_already_purchased=True, exclude already purchased items\n",
    "                    for sim_item_id in similar_item_ids:\n",
    "                        if filter_already_purchased and sim_item_id in valid_purchased_items:\n",
    "                            continue\n",
    "                        similar_items_list.append(sim_item_id)\n",
    "                    \n",
    "                    if len(similar_items_list) >= top_k_items:\n",
    "                        break\n",
    "                    \n",
    "                    # Increase number of similar items 5×\n",
    "                    num_similar_items *= 5\n",
    "                    expansion_step += 1\n",
    "                \n",
    "                # If dynamic expansion is disabled\n",
    "                if n_iter_x5_top_n_similar_items is None or n_iter_x5_top_n_similar_items <= 0:\n",
    "                    sim_scores, sim_indices = torch.topk(row_dense, top_n_similar_items + 1, dim=0)\n",
    "                    mask = (sim_indices != i_idx)\n",
    "                    sim_indices = sim_indices[mask][:top_n_similar_items]\n",
    "                    \n",
    "                    similar_item_ids = item_labels[sim_indices.cpu().numpy()]\n",
    "                    for sim_item_id in similar_item_ids:\n",
    "                        if filter_already_purchased and sim_item_id in valid_purchased_items:\n",
    "                            continue\n",
    "                        similar_items_list.append(sim_item_id)\n",
    "                \n",
    "                # Count how often each similar item appears\n",
    "                product_counter.update(similar_items_list)\n",
    "            \n",
    "            # Select top-K items by frequency\n",
    "            recommended_products = [p for p, _ in product_counter.most_common(top_k_items)]\n",
    "            \n",
    "            # If not enough recommendations → fill with popular items\n",
    "            if len(recommended_products) < top_k_items:\n",
    "                needed_items = top_k_items - len(recommended_products)\n",
    "                additional_items = list(itertools.islice(\n",
    "                    (p for p in most_popular_items_all \n",
    "                     if p not in recommended_products and (p not in valid_purchased_items if filter_already_purchased else True)),\n",
    "                    needed_items\n",
    "                ))\n",
    "                recommended_products.extend(additional_items)\n",
    "\n",
    "            if not recommended_products:\n",
    "                print(f\"[Warning] Empty recommendation for user {user_id}. Fallback to top-{top_k_items} popular items.\")\n",
    "                recommended_products = most_popular_items_all[:top_k_items]\n",
    "            \n",
    "            recommendations[user_id] = np.array(recommended_products[:top_k_items])\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5fbbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing saved recommendations\n",
    "with open(models_outputs_dir / 'item_based' / 'user_recommendations_ibcf.pkl', \"rb\") as f:\n",
    "    user_recommendations_ibcf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db8a5888-09d1-4269-ba4f-7329f10aeec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f143554add44c4bc17b3d4537c6408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Item-Based recommendations:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 2) Generate recommendations for all users\u001b[39;00m\n\u001b[32m      8\u001b[39m user_ids = test_df[\u001b[33m'\u001b[39m\u001b[33manon_id_encrypred\u001b[39m\u001b[33m'\u001b[39m].unique()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m user_recommendations_ibcf = \u001b[43mrecommend_item_based_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muser_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# [:len(user_ids) // 50]\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mitem_similarity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mitem_similarity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mitem_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mitem_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k_items\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_n_similar_items\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_already_purchased\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_already_purchased\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iter_x5_top_n_similar_items\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_iter_x5_top_n_similar_users\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(models_outputs_dir / \u001b[33m'\u001b[39m\u001b[33mitem_based\u001b[39m\u001b[33m'\u001b[39m / \u001b[33m'\u001b[39m\u001b[33muser_recommendations_ibcf.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     22\u001b[39m     pickle.dump(user_recommendations_ibcf, f)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36mrecommend_item_based_batch\u001b[39m\u001b[34m(user_ids, item_similarity, item_labels, df, top_k_items, top_n_similar_items, batch_size, filter_already_purchased, n_iter_x5_top_n_similar_items)\u001b[39m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    129\u001b[39m row_sparse = item_similarity[i_idx]  \u001b[38;5;66;03m# submatrix (1, num_items) (sparse)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m row_dense = \u001b[43mrow_sparse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.flatten()\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Start with base top_n_similar_items\u001b[39;00m\n\u001b[32m    133\u001b[39m num_similar_items = top_n_similar_items\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Generation of new recommendations\n",
    "batch_size = 8\n",
    "\n",
    "# 1) Compute item similarity matrix on GPU\n",
    "item_similarity, item_labels = compute_item_similarity(train_df)\n",
    "\n",
    "# 2) Generate recommendations for all users\n",
    "user_ids = test_df['anon_id_encrypred'].unique()\n",
    "user_recommendations_ibcf = recommend_item_based_batch(\n",
    "    user_ids=user_ids[:len(user_ids) // 50], # [:len(user_ids) // 50]\n",
    "    item_similarity=item_similarity,\n",
    "    item_labels=item_labels,\n",
    "    df=train_df,\n",
    "    top_k_items=k,\n",
    "    top_n_similar_items=n,\n",
    "    batch_size=batch_size,\n",
    "    filter_already_purchased=filter_already_purchased, \n",
    "    n_iter_x5_top_n_similar_items=n_iter_x5_top_n_similar_users\n",
    ")\n",
    "\n",
    "with open(models_outputs_dir / 'item_based' / 'user_recommendations_ibcf.pkl', 'wb') as f:\n",
    "    pickle.dump(user_recommendations_ibcf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2c098fa-14ac-4395-b28c-68add56459cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ibcf = RecommendationDataset(user_recommendations=user_recommendations_ibcf, user_to_true_items=test_user_to_true_items, k=k)\n",
    "loader = DataLoader(dataset_ibcf, batch_size=batch_size, num_workers=0, \n",
    "                    collate_fn=lambda batch: collate_fn(batch, device='mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d3b920e-e22e-4424-a525-ba83016b7a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b3d4374fcc4e6f98bef42e059135f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Precision@K:   0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10 for IBCF model: 0.00224513\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62db88e1019453b919f775cb080d476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recall@K:   0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 for IBCF model: 0.002245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94632003fb46423fa93cb2cfb42567af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MAP@K:   0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@10 for IBCF model: 0.00101459\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69367860859046e6ac23b4457be0e568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NDCG@K:   0%|          | 0/308 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@10 for IBCF model: 0.00157713\n"
     ]
    }
   ],
   "source": [
    "precision_k = precision_at_k_gpu(loader=loader, k=k)\n",
    "print(f'Precision@{k} for IBCF model: {precision_k:.8f}')\n",
    "\n",
    "recall_k = recall_at_k_gpu(loader=loader, k=k)\n",
    "print(f'Recall@{k} for IBCF model: {recall_k:8f}')\n",
    "\n",
    "map_k = map_at_k_gpu(loader=loader, k=k)\n",
    "print(f'MAP@{k} for IBCF model: {map_k:.8f}')\n",
    "\n",
    "ndcg_k = ndcg_at_k_gpu(loader=loader, k=k)\n",
    "print(f'NDCG@{k} for IBCF model: {ndcg_k:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a15377d8-dde8-4a44-a256-84b18c95d2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>k</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>MAP@k</th>\n",
       "      <th>NDCG@k</th>\n",
       "      <th>Other_hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top-K</td>\n",
       "      <td>10</td>\n",
       "      <td>0.005630</td>\n",
       "      <td>0.005536</td>\n",
       "      <td>0.002329</td>\n",
       "      <td>0.003767</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UBCF</td>\n",
       "      <td>10</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.006275</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>{'top_k_items': 10, 'top_n_similar_users': 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IBCF</td>\n",
       "      <td>10</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>{'top_k_items': 10, 'top_n_similar_items': 20,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model   k  Precision@k  Recall@k     MAP@k    NDCG@k  \\\n",
       "0   Top-K  10     0.005630  0.005536  0.002329  0.003767   \n",
       "1  Random  10     0.000033  0.000032  0.000010  0.000022   \n",
       "2    UBCF  10     0.006300  0.006275  0.002561  0.003900   \n",
       "3    IBCF  10     0.002245  0.002245  0.001015  0.001577   \n",
       "\n",
       "                               Other_hyperparameters  \n",
       "0                                               None  \n",
       "1                                               None  \n",
       "2  {'top_k_items': 10, 'top_n_similar_users': 20,...  \n",
       "3  {'top_k_items': 10, 'top_n_similar_items': 20,...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model_results(model_name='IBCF', k=k, precision_k=precision_k, recall_k=recall_k, map_k=map_k, ndcg_k=ndcg_k, round_level=round_level, \n",
    "                  hyperparameters={'top_k_items': k, 'top_n_similar_items': n, \n",
    "                                   'filter_already_purchased': filter_already_purchased, 'n_iter_x5_top_n_similar_items': n_iter_x5_top_n_similar_items})\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c02406f-cf6c-4a83-b0a7-ecbea66cb300",
   "metadata": {},
   "source": [
    "### 5. Matrix Factorization (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93bdf1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    torch.mps.set_per_process_memory_fraction(0.9) # Memory usage limit for MacOS\n",
    "    torch.mps.empty_cache()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee142b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if isinstance(obj, torch.Tensor) and obj.is_mps: # or obj.is_cuda\n",
    "            ref = weakref.ref(obj)\n",
    "            del obj\n",
    "            del ref\n",
    "    except ReferenceError:\n",
    "        pass\n",
    "\n",
    "gc.collect()\n",
    "torch.mps.empty_cache() # or torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fe85a64-294d-4e05-b095-c8222a9ab797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_matrix(df):\n",
    "    \"\"\"\n",
    "    Returns a sparse matrix (csr_matrix) and index mappings \n",
    "    for users and products (user_labels, product_labels) in the exact order of factorize.\n",
    "    \"\"\"\n",
    "    user_item_counts = df.groupby(['anon_id_encrypred', 'articul_encrypred_id']).size().reset_index(name='count')\n",
    "    \n",
    "    # Factorize users and products\n",
    "    u_codes, u_labels = pd.factorize(user_item_counts['anon_id_encrypred'])\n",
    "    p_codes, p_labels = pd.factorize(user_item_counts['articul_encrypred_id'])\n",
    "    \n",
    "    data = user_item_counts['count'].values\n",
    "    \n",
    "    # Create a sparse matrix (rows = users, columns = products)\n",
    "    user_item_matrix = csr_matrix((data, (u_codes, p_codes)))\n",
    "    \n",
    "    return user_item_matrix, u_labels, p_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb724ebc-b55a-4daa-8fe6-1aad44b89231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, num_users, num_items, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(num_users, latent_dim)\n",
    "        self.item_factors = nn.Embedding(num_items, latent_dim)\n",
    "        \n",
    "        # Initialize embeddings (e.g., Xavier uniform)\n",
    "        nn.init.xavier_uniform_(self.user_factors.weight)\n",
    "        nn.init.xavier_uniform_(self.item_factors.weight)\n",
    "\n",
    "    def forward(self, user_indices, item_indices):\n",
    "        \"\"\"\n",
    "        user_indices: LongTensor (batch_size,)\n",
    "        item_indices: LongTensor (batch_size,)\n",
    "        Returns the predicted rating (dot product of embeddings).\n",
    "        \"\"\"\n",
    "        user_embedding = self.user_factors(user_indices)   # (batch_size, latent_dim)\n",
    "        item_embedding = self.item_factors(item_indices)   # (batch_size, latent_dim)\n",
    "        rating_pred = (user_embedding * item_embedding).sum(dim=1)  # (batch_size,)\n",
    "        return rating_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d21e0d00-734d-4118-a1a7-f2337c88fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_matrix_factorization(\n",
    "    df,\n",
    "    latent_dim=256,\n",
    "    epochs=3,\n",
    "    lr=0.01,\n",
    "    batch_size=1024\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains matrix factorization (SVD with gradient descent) on the user-item matrix.\n",
    "    Returns the model along with user_labels and product_labels for inference.\n",
    "    \"\"\"\n",
    "    # Create the user-item matrix\n",
    "    user_item_matrix, user_labels, product_labels = create_user_item_matrix(df)\n",
    "    \n",
    "    # Apply logarithmic scaling to smooth the impact of large counts\n",
    "    user_item_matrix.data = np.log1p(user_item_matrix.data)\n",
    "    num_users, num_items = user_item_matrix.shape\n",
    "\n",
    "    model = MatrixFactorization(num_users, num_items, latent_dim).to(device)\n",
    "\n",
    "    coo = user_item_matrix.tocoo()\n",
    "    user_indices = torch.tensor(coo.row, dtype=torch.long, device=device)\n",
    "    item_indices = torch.tensor(coo.col, dtype=torch.long, device=device)\n",
    "    ratings = torch.tensor(coo.data, dtype=torch.float32, device=device)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(user_indices, item_indices, ratings)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Track loss\n",
    "    batch_losses = []\n",
    "    batch_avg_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc='Training Matrix Factorization (epochs)'):\n",
    "        total_loss = 0.0\n",
    "        epoch_losses = []\n",
    "        verbose = len(dataloader) // 10\n",
    "        for batch_idx, (batch_user, batch_item, batch_rating) in enumerate(tqdm(dataloader, total=len(dataloader), desc=f\"Epoch {epoch+1}\")):\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(batch_user, batch_item)\n",
    "            loss = criterion(preds, batch_rating)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            # Log average loss every 50 batches\n",
    "            if (batch_idx + 1) % verbose == 0:\n",
    "                avg_loss = sum(epoch_losses[-50:]) / 50\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Avg Loss: {avg_loss:.4f}\")\n",
    "                batch_avg_losses.append(avg_loss)\n",
    "                print(f\"Step {batch_idx + 1}, last AVG loss: {avg_loss:.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Total Loss: {total_loss:.4f}\")\n",
    "        batch_losses.extend(epoch_losses)\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(len(batch_avg_losses)), batch_avg_losses, label='Avg Loss per 50 batches', linewidth=2, color='royalblue', marker='o', markersize=4)\n",
    "    plt.xlabel('Logging step (every 50 batches)', fontsize=12)\n",
    "    plt.ylabel('MSE Loss', fontsize=12)\n",
    "    plt.title('Matrix Factorization Training Loss', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc='best', fontsize=11)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model, user_labels, product_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "001adee4-6610-4e0d-9076-b92e0334da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_mf_batch(\n",
    "    user_ids,\n",
    "    model,\n",
    "    user_labels,\n",
    "    product_labels,\n",
    "    df,\n",
    "    top_k_items=10,\n",
    "    batch_size=1024,\n",
    "    filter_already_purchased=True\n",
    "):\n",
    "    \"\"\"\n",
    "    For a list of user_ids, returns recommendations based on matrix factorization.\n",
    "    Logic is similar to User-Based/Item-Based:\n",
    "      1) Predict scores for all items\n",
    "      2) If filter_already_purchased=True, exclude items already purchased by the user\n",
    "      3) Select top-K\n",
    "      4) If not enough, fill in with popular items\n",
    "    \"\"\"\n",
    "    user_labels_index = pd.Index(user_labels)\n",
    "    product_labels_index = pd.Index(product_labels)\n",
    "\n",
    "    # Collect user purchases\n",
    "    user_purchases = df.groupby('anon_id_encrypred')['articul_encrypred_id'].apply(set).to_dict()\n",
    "\n",
    "    # Most popular items (used as fallback)\n",
    "    most_popular_items = df['articul_encrypred_id'].value_counts().index.tolist()\n",
    "\n",
    "    recommendations = {}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for start_idx in tqdm(range(0, len(user_ids), batch_size), desc='Generating MF recommendations'):\n",
    "            batch_user_ids = user_ids[start_idx : start_idx + batch_size]\n",
    "\n",
    "            # Convert user_ids to user_indices\n",
    "            valid_indices = []\n",
    "            valid_user_ids = []\n",
    "            for uid in batch_user_ids:\n",
    "                try:\n",
    "                    uidx = user_labels_index.get_loc(uid)\n",
    "                    valid_indices.append(uidx)\n",
    "                    valid_user_ids.append(uid)\n",
    "                except KeyError: \n",
    "                    # If the user is not in train_df — cold start, fallback to top-k popular items\n",
    "                    recommendations[uid] = np.array(most_popular_items[:top_k_items])\n",
    "            \n",
    "            if len(valid_indices) == 0:\n",
    "                continue  # All users in this batch are invalid\n",
    "\n",
    "            user_tensor = torch.tensor(valid_indices, dtype=torch.long, device=device)\n",
    "            item_tensor = torch.arange(len(product_labels), dtype=torch.long, device=device)\n",
    "\n",
    "            # Predict ratings for (batch_users x all items)\n",
    "            # Expand user_tensor to generate all combinations\n",
    "            # user_tensor.shape -> (batch_size_valid,)\n",
    "            # item_tensor.shape -> (num_items,)\n",
    "\n",
    "            # predictions.shape -> (batch_size_valid, num_items)\n",
    "            predictions = model(\n",
    "                user_tensor.unsqueeze(1).expand(-1, len(item_tensor)).flatten(),\n",
    "                item_tensor.repeat(len(user_tensor))\n",
    "            )\n",
    "            predictions = predictions.view(len(user_tensor), len(item_tensor))\n",
    "\n",
    "            # For each user, select top-K\n",
    "            for i, uid in enumerate(valid_user_ids):\n",
    "                user_bought = user_purchases.get(uid, set())\n",
    "\n",
    "                # If filter_already_purchased=True, exclude already purchased items\n",
    "                if filter_already_purchased:\n",
    "                    mask = torch.tensor(\n",
    "                        [product_labels[j] not in user_bought for j in range(len(product_labels))],\n",
    "                        dtype=torch.bool,\n",
    "                        device=device\n",
    "                    )\n",
    "                    # Assign -inf to filtered items\n",
    "                    predictions[i][~mask] = float('-inf')\n",
    "\n",
    "                # Select top-K\n",
    "                top_k_indices = torch.topk(predictions[i], top_k_items).indices\n",
    "                top_k_products = [product_labels[idx.item()] for idx in top_k_indices]\n",
    "\n",
    "                # If fewer than K recommendations (theoretically possible if filtering is too strong),\n",
    "                # fill with popular items (unlikely with MF due to many items)\n",
    "                if len(top_k_products) < top_k_items:\n",
    "                    needed = top_k_items - len(top_k_products)\n",
    "                    fallback = list(itertools.islice(\n",
    "                        (p for p in most_popular_items if p not in top_k_products and\n",
    "                         (p not in user_bought if filter_already_purchased else True)),\n",
    "                        needed\n",
    "                    ))\n",
    "\n",
    "                    # If fallback is still short, fill with just top popular items\n",
    "                    if top_k_items - len(top_k_products) - len(fallback) != 0:\n",
    "                        needed = top_k_items - len(top_k_products) - len(fallback)\n",
    "                        fallback = most_popular_items[:needed]\n",
    "                    top_k_products.extend(fallback)\n",
    "\n",
    "                recommendations[uid] = np.array(top_k_products[:top_k_items])\n",
    "\n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0138a94e-7069-4832-8ccc-f3e61e421825",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_already_purchased = True\n",
    "latent_dim=256\n",
    "batch_size=1024\n",
    "epochs=3\n",
    "lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a25dbef-1823-41f5-9773-80ba4e627d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561afda5503d40bfb6f911690c6c8262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Matrix Factorization (epochs):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a8b766ffd048adbdc0288c1a05841d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/2205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 220, Avg Loss: 0.5145\n",
      "Step 220, last AVG loss: 0.5145\n",
      "Epoch 1, Batch 440, Avg Loss: 0.5136\n",
      "Step 440, last AVG loss: 0.5136\n",
      "Epoch 1, Batch 660, Avg Loss: 0.4974\n",
      "Step 660, last AVG loss: 0.4974\n",
      "Epoch 1, Batch 880, Avg Loss: 0.4273\n",
      "Step 880, last AVG loss: 0.4273\n",
      "Epoch 1, Batch 1100, Avg Loss: 0.3096\n",
      "Step 1100, last AVG loss: 0.3096\n",
      "Epoch 1, Batch 1320, Avg Loss: 0.2109\n",
      "Step 1320, last AVG loss: 0.2109\n",
      "Epoch 1, Batch 1540, Avg Loss: 0.1472\n",
      "Step 1540, last AVG loss: 0.1472\n",
      "Epoch 1, Batch 1760, Avg Loss: 0.1060\n",
      "Step 1760, last AVG loss: 0.1060\n",
      "Epoch 1, Batch 1980, Avg Loss: 0.0797\n",
      "Step 1980, last AVG loss: 0.0797\n",
      "Epoch 1, Batch 2200, Avg Loss: 0.0632\n",
      "Step 2200, last AVG loss: 0.0632\n",
      "Epoch 1/3, Total Loss: 670.0374\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108f382f87864ed3ac288b000d63e9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/2205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 220, Avg Loss: 0.0278\n",
      "Step 220, last AVG loss: 0.0278\n",
      "Epoch 2, Batch 440, Avg Loss: 0.0254\n",
      "Step 440, last AVG loss: 0.0254\n",
      "Epoch 2, Batch 660, Avg Loss: 0.0238\n",
      "Step 660, last AVG loss: 0.0238\n",
      "Epoch 2, Batch 880, Avg Loss: 0.0231\n",
      "Step 880, last AVG loss: 0.0231\n",
      "Epoch 2, Batch 1100, Avg Loss: 0.0222\n",
      "Step 1100, last AVG loss: 0.0222\n",
      "Epoch 2, Batch 1320, Avg Loss: 0.0217\n",
      "Step 1320, last AVG loss: 0.0217\n",
      "Epoch 2, Batch 1540, Avg Loss: 0.0215\n",
      "Step 1540, last AVG loss: 0.0215\n",
      "Epoch 2, Batch 1760, Avg Loss: 0.0211\n",
      "Step 1760, last AVG loss: 0.0211\n",
      "Epoch 2, Batch 1980, Avg Loss: 0.0215\n",
      "Step 1980, last AVG loss: 0.0215\n",
      "Epoch 2, Batch 2200, Avg Loss: 0.0212\n",
      "Step 2200, last AVG loss: 0.0212\n",
      "Epoch 2/3, Total Loss: 51.2838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8d8491d6074bda8fc90bb8e440a215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/2205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 220, Avg Loss: 0.0139\n",
      "Step 220, last AVG loss: 0.0139\n",
      "Epoch 3, Batch 440, Avg Loss: 0.0137\n",
      "Step 440, last AVG loss: 0.0137\n",
      "Epoch 3, Batch 660, Avg Loss: 0.0138\n",
      "Step 660, last AVG loss: 0.0138\n",
      "Epoch 3, Batch 880, Avg Loss: 0.0145\n",
      "Step 880, last AVG loss: 0.0145\n",
      "Epoch 3, Batch 1100, Avg Loss: 0.0144\n",
      "Step 1100, last AVG loss: 0.0144\n",
      "Epoch 3, Batch 1320, Avg Loss: 0.0148\n",
      "Step 1320, last AVG loss: 0.0148\n",
      "Epoch 3, Batch 1540, Avg Loss: 0.0151\n",
      "Step 1540, last AVG loss: 0.0151\n",
      "Epoch 3, Batch 1760, Avg Loss: 0.0153\n",
      "Step 1760, last AVG loss: 0.0153\n",
      "Epoch 3, Batch 1980, Avg Loss: 0.0157\n",
      "Step 1980, last AVG loss: 0.0157\n",
      "Epoch 3, Batch 2200, Avg Loss: 0.0160\n",
      "Step 2200, last AVG loss: 0.0160\n",
      "Epoch 3/3, Total Loss: 32.3600\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mf_model, user_labels, product_labels = train_matrix_factorization(\n",
    "    train_df,\n",
    "    latent_dim=latent_dim,\n",
    "    epochs=epochs,\n",
    "    lr=lr,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e48cf0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatrixFactorization(\n",
       "  (user_factors): Embedding(130282, 256)\n",
       "  (item_factors): Embedding(202924, 256)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(mf_model.state_dict(), models_dir / f'mf_model_latent_dim_{latent_dim}_epochs_{epochs}_lr_{lr}.pth')\n",
    "\n",
    "with open(models_outputs_dir / 'mf' / 'user_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(user_labels, f)\n",
    "\n",
    "with open(models_outputs_dir / 'mf' / 'product_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(product_labels, f)\n",
    "\n",
    "mf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c84e0c5-4feb-4629-9828-eb0722ec9209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatrixFactorization(\n",
       "  (user_factors): Embedding(370091, 512)\n",
       "  (item_factors): Embedding(267600, 512)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(models_outputs_dir / 'mf' / 'user_labels.pkl', \"rb\") as f:\n",
    "    user_labels = pickle.load(f)\n",
    "\n",
    "with open(models_outputs_dir / 'mf' / 'product_labels.pkl', \"rb\") as f:\n",
    "    product_labels = pickle.load(f)\n",
    "\n",
    "# Reconstruct the model architecture\n",
    "num_users = len(user_labels)\n",
    "num_items = len(product_labels)\n",
    "\n",
    "mf_model = MatrixFactorization(num_users, num_items, latent_dim).to(device)\n",
    "\n",
    "# Load model weights\n",
    "mf_model.load_state_dict(torch.load(models_dir / f'mf_model_latent_dim_{latent_dim}_epochs_{epochs}_lr_{lr}.pth'))\n",
    "\n",
    "# Switch model to evaluation (inference) mode\n",
    "mf_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d94b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing saved recommendations\n",
    "with open(models_outputs_dir / 'mf' / 'user_recommendations_mf.pkl', \"rb\") as f:\n",
    "    user_recommendations_mf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c44d349a-ecb1-4827-b7dd-5545f7ee8e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a6441b893848dc9273884b1aefa8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating MF recommendations:   0%|          | 0/3973 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generation of new recommendations\n",
    "user_ids = test_df['anon_id_encrypred'].unique()\n",
    "\n",
    "user_recommendations_mf = recommend_mf_batch(\n",
    "    user_ids=user_ids[:len(user_ids) // 10],\n",
    "    model=mf_model,\n",
    "    user_labels=user_labels,\n",
    "    product_labels=product_labels,\n",
    "    df=train_df,\n",
    "    top_k_items=k,\n",
    "    batch_size=2,\n",
    "    filter_already_purchased=filter_already_purchased\n",
    ")\n",
    "\n",
    "with open(models_outputs_dir / 'mf' / 'user_recommendations_mf.pkl', 'wb') as f:\n",
    "    pickle.dump(user_recommendations_mf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1022f56b-2eaa-4714-aff5-82579d3b718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mf = RecommendationDataset(user_recommendations=user_recommendations_mf, user_to_true_items=test_user_to_true_items, k=k)\n",
    "loader = DataLoader(dataset_mf, batch_size=batch_size, num_workers=0, \n",
    "                    collate_fn=lambda batch: collate_fn(batch, device='mps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cc94cdf-56f6-4ad1-8dfb-5e46cc0940ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3618c6c871486d9c5fdc75a3b32afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Precision@K:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10 for MF model: 0.00028945\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3266ed39a4c244aa81f944e3a1c33e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recall@K:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10 for MF model: 0.000264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5390811a2a4b1d96c6c774f496df23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MAP@K:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@10 for MF model: 0.00006727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21875c369f4d4a7e97d0417b4925cc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NDCG@K:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@10 for MF model: 0.00018895\n"
     ]
    }
   ],
   "source": [
    "precision_k = precision_at_k_gpu(loader=loader, k=k)\n",
    "print(f'Precision@{k} for MF model: {precision_k:.8f}')\n",
    "\n",
    "recall_k = recall_at_k_gpu(loader=loader, k=k)\n",
    "print(f'Recall@{k} for MF model: {recall_k:8f}')\n",
    "\n",
    "map_k = map_at_k_gpu(loader=loader, k=k)\n",
    "print(f'MAP@{k} for MF model: {map_k:.8f}')\n",
    "\n",
    "ndcg_k = ndcg_at_k_gpu(loader=loader, k=k)\n",
    "print(f'NDCG@{k} for MF model: {ndcg_k:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30839158-0bd9-4da4-9c35-97c3debca75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>k</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>MAP@k</th>\n",
       "      <th>NDCG@k</th>\n",
       "      <th>Other_hyperparameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top-K</td>\n",
       "      <td>10</td>\n",
       "      <td>0.005779</td>\n",
       "      <td>0.005419</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UBCF</td>\n",
       "      <td>10</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.003224</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>{'top_k_items': 10, 'top_n_similar_users': 20,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matrix Factorization</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>{'top_k_items': 10, 'latent_dim': 256, 'filter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model   k  Precision@k  Recall@k     MAP@k    NDCG@k  \\\n",
       "0                 Top-K  10     0.005779  0.005419  0.002633  0.004834   \n",
       "1                Random  10     0.000033  0.000032  0.000005  0.000013   \n",
       "2                  UBCF  10     0.003238  0.003224  0.001730  0.002740   \n",
       "3  Matrix Factorization  10     0.000289  0.000264  0.000067  0.000189   \n",
       "\n",
       "                               Other_hyperparameters  \n",
       "0                                               None  \n",
       "1                                               None  \n",
       "2  {'top_k_items': 10, 'top_n_similar_users': 20,...  \n",
       "3  {'top_k_items': 10, 'latent_dim': 256, 'filter...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model_results(model_name='Matrix Factorization', k=k, precision_k=precision_k, recall_k=recall_k, map_k=map_k, ndcg_k=ndcg_k, round_level=round_level,\n",
    "                  hyperparameters={'top_k_items': k, 'latent_dim': latent_dim, \n",
    "                                   'filter_already_purchased': filter_already_purchased})\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69aadc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.to_excel(interim_data_dir / f'df_metrics_k_{k}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324128d7-db5a-439e-b84a-1a7c4e60f824",
   "metadata": {},
   "source": [
    "## Strong Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046dfcf5-3ad3-42cf-a453-6852d5bb70e8",
   "metadata": {},
   "source": [
    "### BERT4Rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab2e29",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1904.06690"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b518c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if isinstance(obj, torch.Tensor) and obj.is_mps: # or obj.is_cuda\n",
    "            ref = weakref.ref(obj)\n",
    "            del obj\n",
    "            del ref\n",
    "    except ReferenceError:\n",
    "        pass\n",
    "\n",
    "gc.collect()\n",
    "torch.mps.empty_cache() # or torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac253c",
   "metadata": {},
   "source": [
    "#### BERT4Rec Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e8124b6-5cc8-42cd-9834-65490e3e04e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT4RecDataset(Dataset):\n",
    "    def __init__(self, sequences, max_len, mask_prob, num_items, pad_token=0, mask_token=None, is_train=True, external_targets=None):\n",
    "        self.max_len = max_len\n",
    "        self.mask_prob = mask_prob\n",
    "        self.num_items = num_items\n",
    "        self.pad_token = pad_token\n",
    "        self.mask_token = mask_token if mask_token is not None else num_items + 1\n",
    "        self.is_train = is_train \n",
    "        self.external_targets = external_targets  # user_id -> true items (list or set)\n",
    "\n",
    "        self.user_ids = []\n",
    "        self.processed_sequences = []\n",
    "        \n",
    "        for user_id, seq in sequences:\n",
    "            self.user_ids.append(user_id)\n",
    "            truncated = seq[-self.max_len:] if len(seq) > self.max_len else seq\n",
    "            padded = truncated + [self.pad_token] * (self.max_len - len(truncated))  # RIGHT SIDE PADDING\n",
    "            self.processed_sequences.append(padded)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id = self.user_ids[idx]\n",
    "        seq = self.processed_sequences[idx].copy()\n",
    "\n",
    "        # === Validation Mode ===\n",
    "        if not self.is_train and self.external_targets is not None:\n",
    "            input_seq = seq\n",
    "            true_items = list(self.external_targets.get(user_id, []))[:self.max_len]\n",
    "\n",
    "            # RIGHT SIDE PADDING for labels\n",
    "            padded_labels = true_items + [-100] * (self.max_len - len(true_items))  # -100 — default ignore_index (also default in CrossEntropyLoss)\n",
    "            labels = torch.tensor(padded_labels, dtype=torch.long)\n",
    "\n",
    "            attention_mask = [1 if x != self.pad_token else 0 for x in input_seq]\n",
    "            position_ids = torch.arange(self.max_len, dtype=torch.long)\n",
    "\n",
    "            return {\n",
    "                \"user_id\": user_id,\n",
    "                \"input_ids\": torch.tensor(input_seq, dtype=torch.long),\n",
    "                \"labels\": labels,\n",
    "                \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "                \"position_ids\": position_ids\n",
    "            }\n",
    "\n",
    "\n",
    "        # === Train Mode (with guarantee min_masked = 1) ===\n",
    "        input_seq = seq.copy()\n",
    "        target_seq = [-100] * self.max_len\n",
    "\n",
    "        candidate_idxs = [i for i, token in enumerate(seq) if token != self.pad_token]\n",
    "        masked_idxs = [i for i in candidate_idxs if random.random() < self.mask_prob]\n",
    "\n",
    "        # === min_masked = 1 ===\n",
    "        if len(masked_idxs) == 0 and len(candidate_idxs) > 0:\n",
    "            masked_idxs = [random.choice(candidate_idxs)]\n",
    "\n",
    "        for i in masked_idxs: # 90, 9, 1\n",
    "            original_token = seq[i]\n",
    "            prob = random.random()\n",
    "            if prob < 0.9:\n",
    "                input_seq[i] = self.mask_token\n",
    "            elif prob < 0.99:\n",
    "                input_seq[i] = random.randint(1, self.num_items)\n",
    "            else:\n",
    "                input_seq[i] = original_token\n",
    "            target_seq[i] = original_token\n",
    "\n",
    "        attention_mask = [1 if x != self.pad_token else 0 for x in input_seq]\n",
    "        position_ids = torch.arange(self.max_len, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"user_id\": user_id,\n",
    "            \"input_ids\": torch.tensor(input_seq, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(target_seq, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"position_ids\": position_ids\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10821f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min date: 2019-01-01 00:00:00\n",
      "Max date: 2021-02-18 00:00:00\n",
      "Threshold date (85.0%): 2020-10-24 00:00:00\n",
      "Train shape: (2361946, 7)\n",
      "Test shape: (445954, 7)\n",
      "\n",
      "Total number of users: 134362\n",
      "Number of users in the training set: 130282\n",
      "Number of users in the test set: 79468\n",
      "\n",
      "Total number of 'articul_encrypred_idx': 222135\n",
      "Number of 'articul_encrypred_idx' in the training set: 202924\n",
      "Number of 'articul_encrypred_idx' in the test set: 99765\n"
     ]
    }
   ],
   "source": [
    "df_sales_articul['order_date'] = pd.to_datetime(df_sales_articul['order_date'])\n",
    "df_sales_articul = df_sales_articul.sort_values(by=['anon_id_encrypred', 'order_date'])\n",
    "\n",
    "# Leave only users with 5+ purchases (for decreasing the noise)\n",
    "user_counts = df_sales_articul['anon_id_encrypred'].value_counts()\n",
    "active_users = user_counts[user_counts >= 5].index\n",
    "df_sales_articul = df_sales_articul[df_sales_articul['anon_id_encrypred'].isin(active_users)]\n",
    "\n",
    "# Index mapping\n",
    "unique_articul_encrypred_id = df_sales_articul['articul_encrypred_id'].unique()\n",
    "articul_encrypred_id_to_idx = {pid: idx + 1 for idx, pid in enumerate(unique_articul_encrypred_id)} # + 1 for PAD zero token (0)\n",
    "df_sales_articul['articul_encrypred_idx'] = df_sales_articul['articul_encrypred_id'].map(articul_encrypred_id_to_idx)\n",
    "\n",
    "# Train/Test time split\n",
    "threshold_level = 0.85\n",
    "min_date = df_sales_articul['order_date'].min()\n",
    "max_date = df_sales_articul['order_date'].max()\n",
    "\n",
    "print(f\"Min date: {min_date}\")\n",
    "print(f\"Max date: {max_date}\")\n",
    "\n",
    "total_days = (max_date - min_date).days\n",
    "threshold_days = int(total_days * threshold_level)\n",
    "threshold_date = min_date + pd.Timedelta(days=threshold_days)\n",
    "\n",
    "print(f\"Threshold date ({round(threshold_level * 100, 0)}%): {threshold_date}\")\n",
    "\n",
    "train_df = df_sales_articul[df_sales_articul['order_date'] < threshold_date]\n",
    "test_df = df_sales_articul[df_sales_articul['order_date'] >= threshold_date]\n",
    "\n",
    "df_sales_articul.to_csv(interim_data_dir / 'df_sales_articul.csv', index=False)\n",
    "train_df.to_csv(interim_data_dir / 'train_data_by_threshold_date.csv', index=False)\n",
    "test_df.to_csv(interim_data_dir / 'test_data_by_threshold_date.csv', index=False)\n",
    "\n",
    "# df_sales_articul = pd.read_csv(interim_data_dir / 'df_sales_articul.csv')\n",
    "# train_df = pd.read_csv(interim_data_dir / 'train_data_by_threshold_date.csv')\n",
    "# test_df = pd.read_csv(interim_data_dir / 'test_data_by_threshold_date.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print()\n",
    "print(f\"Total number of users: {len(df_sales_articul['anon_id_encrypred'].unique())}\")\n",
    "print(f\"Number of users in the training set: {len(train_df['anon_id_encrypred'].unique())}\")\n",
    "print(f\"Number of users in the test set: {len(test_df['anon_id_encrypred'].unique())}\")\n",
    "print()\n",
    "print(f\"Total number of 'articul_encrypred_idx': {len(df_sales_articul['articul_encrypred_idx'].unique())}\")\n",
    "print(f\"Number of 'articul_encrypred_idx' in the training set: {len(train_df['articul_encrypred_idx'].unique())}\")\n",
    "print(f\"Number of 'articul_encrypred_idx' in the test set: {len(test_df['articul_encrypred_idx'].unique())}\")\n",
    "\n",
    "test_user_to_true_items = test_df.groupby('anon_id_encrypred')['articul_encrypred_idx'].apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c70c0cb7-b5be-49a9-ab82-e373f9229888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length percentiles:\n",
      "0.25      7.0\n",
      "0.50     11.0\n",
      "0.75     21.0\n",
      "0.90     44.0\n",
      "0.95     68.0\n",
      "0.99    159.0\n",
      "dtype: float64\n",
      "Maximum sequence length: 2192\n"
     ]
    }
   ],
   "source": [
    "# Group by users and compute the sequence length for each\n",
    "user_sequence_lengths = df_sales_articul.groupby('anon_id_encrypred').size()\n",
    "percentiles = user_sequence_lengths.quantile([0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\n",
    "\n",
    "print(\"Sequence length percentiles:\")\n",
    "print(percentiles)\n",
    "print(f\"Maximum sequence length: {user_sequence_lengths.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "819bd52e-45a6-4cd9-b01f-78a5e1a7476d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sequence for user \u0004\u000eqqwrtrxq: [1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Create purchase sequences for each user\n",
    "train_sequences = train_df.groupby('anon_id_encrypred')['articul_encrypred_idx'].apply(list).to_dict()\n",
    "test_sequences = test_df.groupby('anon_id_encrypred')['articul_encrypred_idx'].apply(list).to_dict()\n",
    "\n",
    "print(f\"Example sequence for user {list(train_sequences.keys())[0]}: {train_sequences[list(train_sequences.keys())[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2714adc-e30b-4b91-924e-3087dd732895",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 15 # Maximum sequence length\n",
    "mask_prob = 0.5 # Masking probability\n",
    "num_items = len(articul_encrypred_id_to_idx) \n",
    "\n",
    "# Convert sequence dictionaries to list of tuples (user_id, sequence)\n",
    "train_sequences_list = list(train_sequences.items())\n",
    "test_sequences_list = list(test_sequences.items())\n",
    "\n",
    "train_dataset = BERT4RecDataset(\n",
    "    sequences=train_sequences_list,\n",
    "    max_len=max_len,\n",
    "    mask_prob=mask_prob,\n",
    "    num_items=num_items,\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "test_dataset = BERT4RecDataset(\n",
    "    sequences=train_sequences_list,     # <-- train for inputs\n",
    "    max_len=max_len,\n",
    "    mask_prob=mask_prob,\n",
    "    num_items=num_items,\n",
    "    is_train=False,\n",
    "    external_targets=test_sequences  # <-- targets from test_df\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8975b988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One object from `train_loader`\n",
      "User ID: wyyypqqtqyqvxvwr\n",
      "Input IDs: tensor([222136,  25379,  48230, 222136, 222136,  95061, 222136,  69611,  58895,\n",
      "         30343,   3187,  23052, 222136, 222136,   7247])\n",
      "Labels: tensor([ 5744,  -100,  -100, 17434, 26453,  -100, 47944,  -100,  -100,  -100,\n",
      "         -100,  -100, 57116, 23802,  -100])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Position IDs: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n",
      "\n",
      "One object from `test_dataset`\n",
      "User ID: \u0004\u000eqqwrtrxq\n",
      "Input IDs: tensor([1, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Labels: tensor([   5, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Position IDs: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print('One object from `train_loader`')\n",
    "    user_ids = batch['user_id'][0]\n",
    "    input_ids = batch['input_ids'][0]\n",
    "    labels = batch['labels'][0]\n",
    "    attention_mask = batch['attention_mask'][0]\n",
    "    position_ids = batch['position_ids'][0]\n",
    "    \n",
    "    print(\"User ID:\", user_ids)\n",
    "    print(\"Input IDs:\", input_ids)\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Attention Mask:\", attention_mask)\n",
    "    print(\"Position IDs:\", position_ids)\n",
    "    break \n",
    "\n",
    "print()\n",
    "\n",
    "for batch in test_loader:\n",
    "    print('One object from `test_dataset`')\n",
    "    user_ids = batch['user_id'][0]\n",
    "    input_ids = batch['input_ids'][0]\n",
    "    labels = batch['labels'][0]\n",
    "    attention_mask = batch['attention_mask'][0]\n",
    "    position_ids = batch['position_ids'][0]\n",
    "    \n",
    "    print(\"User ID:\", user_ids)\n",
    "    print(\"Input IDs:\", input_ids)\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Attention Mask:\", attention_mask)\n",
    "    print(\"Position IDs:\", position_ids)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "735c4fde-5420-49ab-b517-b5ac14a445de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT4RecModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_items,\n",
    "                 max_len,\n",
    "                 embedding_dim=256,\n",
    "                 num_layers=6,\n",
    "                 num_heads=4,\n",
    "                 dropout=0.1,\n",
    "                 ffn_dim=1024):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_items = num_items\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # PAD = 0, items = 1..num_items, MASK = num_items+1\n",
    "        self.item_embeddings = nn.Embedding(num_items + 2, embedding_dim, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(max_len, embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ffn_dim,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer = TransformerEncoder(encoder_layer,\n",
    "                                              num_layers=num_layers)\n",
    "\n",
    "        # ——— Новая классифицирующая голова ———\n",
    "        # Проецируем обратно в embedding_dim, чтобы можно было dot-product'ом\n",
    "        self.fc_head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(embedding_dim * 2),\n",
    "            nn.Linear(embedding_dim * 2, embedding_dim, bias=False),\n",
    "            # nn.LayerNorm(embedding_dim),  \n",
    "        )\n",
    "        # bias для logits по словарю\n",
    "        self.vocab_bias = nn.Parameter(torch.zeros(num_items + 2))\n",
    "\n",
    "        # weight-tying: последний Linear выдаёт embedding_dim,\n",
    "        # а дальше dot(item_emb.weight.T) дает logits по vocab\n",
    "        # (embedding_dim == self.item_embeddings.embedding_dim)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # эмбеддинги\n",
    "        nn.init.xavier_uniform_(self.item_embeddings.weight)\n",
    "        # nn.init.normal_(self.item_embeddings.weight, 0.0, 0.02)\n",
    "        nn.init.normal_(self.position_embeddings.weight, 0.0, 0.02)\n",
    "\n",
    "        # трансформер\n",
    "        for layer in self.transformer.layers:\n",
    "            nn.init.xavier_uniform_(layer.self_attn.in_proj_weight)\n",
    "            nn.init.xavier_uniform_(layer.self_attn.out_proj.weight)\n",
    "            nn.init.xavier_uniform_(layer.linear1.weight)\n",
    "            nn.init.xavier_uniform_(layer.linear2.weight)\n",
    "\n",
    "        # голова\n",
    "        nn.init.xavier_uniform_(self.fc_head[0].weight)\n",
    "        nn.init.xavier_uniform_(self.fc_head[3].weight)\n",
    "        nn.init.zeros_(self.vocab_bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, position_ids):\n",
    "        B, L = input_ids.size()\n",
    "        # 1) Embeddings + scale\n",
    "        item_embeds = self.item_embeddings(input_ids)             # (B,L,dim)\n",
    "        pos_embeds  = self.position_embeddings(position_ids)      # (L,dim) -> broadcast\n",
    "        x = (item_embeds + pos_embeds) * math.sqrt(self.embedding_dim)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 2) Causal + padding masks\n",
    "        src_key_padding_mask = (attention_mask == 0)              # (B,L)\n",
    "\n",
    "        # 3) Transformer\n",
    "        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)  # (B,L,dim)\n",
    "\n",
    "        # 4) Head → back to embedding space\n",
    "        h = self.fc_head(x)                                      # (B,L,dim)\n",
    "\n",
    "        # 5) Dot-product with item_embeddings + bias → logits\n",
    "        logits = torch.matmul(h, self.item_embeddings.weight.T)  # (B,L,num_items+2)\n",
    "        logits = logits + self.vocab_bias                        # broadcast bias\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "197e78f4-b437-4234-966f-e10ea4a472b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    save_path: str,\n",
    "    num_epochs: int = 5,\n",
    "    log_interval: int = 50,\n",
    "    scheduler = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Тренирует модель с выводом прогресса через tqdm и периодическим логированием лосса.\n",
    "    Возвращает модель и историю лоссов.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100, label_smoothing=0.1) # label_smoothing=0.1 размывание таргета\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        # running_loss = 0.0\n",
    "        running, acc_steps = 0.0, 0\n",
    "\n",
    "        for n,p in model.named_parameters():\n",
    "            assert torch.all(torch.isfinite(p)), f\"NaN в параметре {n}\"\n",
    "\n",
    "\n",
    "        # оборачиваем сам loader, а затем делаем enumerate над tqdm-ом\n",
    "        pbar_train = tqdm(train_loader, total=len(train_loader),\n",
    "                          desc=f\"[Epoch {epoch}/{num_epochs}] Train\")\n",
    "        for batch_idx, batch in enumerate(pbar_train, 1):\n",
    "            try:\n",
    "                input_ids      = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                position_ids   = batch['position_ids'].to(device)\n",
    "                labels         = batch['labels'].to(device)\n",
    "\n",
    "                # === ✅ ДОПОЛНИТЕЛЬНАЯ ПРОВЕРКА ПЕРЕД LOSS ===\n",
    "                # print(f\"[Batch {batch_idx}] input_ids: min={input_ids.min().item()}, max={input_ids.max().item()}\")\n",
    "                # print(f\"[Batch {batch_idx}] labels:    min={labels[labels != -100].min().item() if (labels != -100).any() else 'n/a'}, \"\n",
    "                #     f\"max={labels[labels != -100].max().item() if (labels != -100).any() else 'n/a'}\")\n",
    "\n",
    "                # Проверка на допустимые значения\n",
    "                assert torch.all((labels == -100) | ((labels > 0) & (labels < num_items + 1))), \\\n",
    "                    f\"Invalid label value found! batch_idx={batch_idx}\"\n",
    "\n",
    "                assert input_ids.max() < num_items + 2, f\"input_ids contain invalid token! batch_idx={batch_idx}\"\n",
    "\n",
    "                if not (labels != -100).any():\n",
    "                    print(f\"[Batch {batch_idx}] All masked, skipping\")\n",
    "                    pbar_train.update(1)\n",
    "                    continue\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(input_ids, attention_mask, position_ids)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "\n",
    "                if not torch.isfinite(loss):\n",
    "                    print(f\"!!! NaN loss at epoch {epoch} batch {batch_idx}\")\n",
    "\n",
    "                    for name, p in model.named_parameters():\n",
    "                        if not torch.all(torch.isfinite(p)):\n",
    "                            bad = (~torch.isfinite(p)).sum().item()\n",
    "                            print(f\"  → {name}: {bad} bad entries\")\n",
    "                    raise RuntimeError(\"Stop: loss became NaN\")\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "                running += loss.item()\n",
    "                acc_steps += 1\n",
    "\n",
    "                if batch_idx % 500 == 0:\n",
    "                    print(f\"[Batch {batch_idx}] shape: input_ids={input_ids.shape}, labels={labels.shape}, outputs={outputs.shape}\")\n",
    "                    current_lr = scheduler.get_last_lr()[0]\n",
    "                    print(f\"[Batch {batch_idx}] lr = {current_lr:.8f}\")\n",
    "\n",
    "\n",
    "                # log n первых/каждый log_interval\n",
    "                if batch_idx == 1 or (acc_steps % log_interval == 0):\n",
    "                    avg = running / acc_steps\n",
    "                    pbar_train.set_postfix(batch_loss=f\"{avg:.4f}\")\n",
    "                    train_loss_history.append(avg)\n",
    "                    running, acc_steps = 0.0, 0\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at batch {batch_idx}: {e}\")\n",
    "\n",
    "\n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        # val_running = 0.0\n",
    "        val_running, val_batches = 0.0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pbar_val = tqdm(val_loader, total=len(val_loader),\n",
    "                            desc=f\"[Epoch {epoch}/{num_epochs}] Val  \")\n",
    "            for batch in pbar_val:\n",
    "                input_ids      = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                position_ids   = batch['position_ids'].to(device)\n",
    "                labels         = batch['labels'].to(device)\n",
    "\n",
    "                if (labels != -100).any():           # есть валид-таргеты\n",
    "                    outputs = model(input_ids, attention_mask, position_ids)\n",
    "                    loss    = criterion(\n",
    "                        outputs.view(-1, outputs.size(-1)),\n",
    "                        labels.view(-1)\n",
    "                    )\n",
    "                    val_running += loss.item()\n",
    "                    val_batches += 1\n",
    "                    pbar_val.set_postfix(val_loss=f\"{loss.item():.4f}\")\n",
    "                else:\n",
    "                    pbar_val.set_postfix(val_loss=\"skip\")\n",
    "\n",
    "        avg_val = val_running / val_batches if val_batches else float(\"nan\")\n",
    "        val_loss_history.append(avg_val)\n",
    "        print(f\"Epoch {epoch}: val_loss = {avg_val:.4f}\")\n",
    "\n",
    "        # сохраняем лучшую модель\n",
    "        if avg_val < best_val_loss:\n",
    "            best_val_loss = avg_val\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"→ saved new best model  (val_loss {best_val_loss:.4f})\\n\")\n",
    "\n",
    "    return model, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5347caca-e80e-413c-8b1a-0963c5debeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=1024\n",
    "num_layers=12\n",
    "num_heads=8\n",
    "ffn_dim=4096\n",
    "dropout=0.1\n",
    "lr=1e-4\n",
    "log_interval=100\n",
    "\n",
    "model = BERT4RecModel(\n",
    "    num_items=num_items,\n",
    "    max_len=max_len,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    ffn_dim=ffn_dim,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "\n",
    "num_epochs = 3\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "scheduler = LambdaLR(optimizer,\n",
    "    lr_lambda=lambda step: (step / warmup_steps if step < warmup_steps\n",
    "        else max(0.0, (total_steps - step) / float(max(1, total_steps - warmup_steps)))\n",
    "    )\n",
    ")\n",
    "\n",
    "save_path = models_dir / f'bert4rec_model_articul_encrypred_id_embedding_dim_{embedding_dim}_epochs_{num_epochs}_lr_{lr}_max_len_{max_len}_mask_prob_{mask_prob}_num_layers_{num_layers}_num_heads_{num_heads}_ffn_dim_{ffn_dim}_dropout_{dropout}_num_items_{num_items}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07196eb4-b595-40fd-b09d-e59cb716234f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT4RecModel(\n",
       "  (item_embeddings): Embedding(222137, 1024, padding_idx=0)\n",
       "  (position_embeddings): Embedding(15, 1024)\n",
       "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_head): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20fc7a8-4bef-4b0a-aae7-6d1cad1448fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c821c94f33c141b4b9cf9ecd82f11398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Epoch 1/3] Train:   0%|          | 0/4072 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 500] shape: input_ids=torch.Size([32, 15]), labels=torch.Size([32, 15]), outputs=torch.Size([32, 15, 222137])\n",
      "[Batch 500] lr = 0.00004095\n",
      "[Batch 1000] shape: input_ids=torch.Size([32, 15]), labels=torch.Size([32, 15]), outputs=torch.Size([32, 15, 222137])\n",
      "[Batch 1000] lr = 0.00008190\n"
     ]
    }
   ],
   "source": [
    "trained_model, train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    log_interval=log_interval,\n",
    "    save_path=save_path,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "torch.save(trained_model.state_dict(), models_dir / f'LAST_EPOCH_bert4rec_model_embedding_dim_{embedding_dim}_epochs_{num_epochs}_lr_{lr}_max_len_{max_len}_mask_prob_{mask_prob}_num_layers_{num_layers}_num_heads_{num_heads}_ffn_dim_{ffn_dim}_dropout_{dropout}_num_items_{num_items}.pth')\n",
    "\n",
    "print(f\"Last train loss = {train_losses[-1]:.4f}\")\n",
    "print(f\"Last val loss = {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981420d9-a16e-420d-9e90-977ee915eef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_train_loss(train_losses, log_interval, name='Loss Over Batches'):\n",
    "    \"\"\"\n",
    "    Визуализация train_losses.\n",
    "    \n",
    "    :param train_losses: Список значений потерь на тренировке.\n",
    "    :param log_interval: Интервал логирования (для корректной оси X).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Ось X: количество батчей\n",
    "    x = [i * log_interval for i in range(len(train_losses))]\n",
    "    \n",
    "    # Отрисовка графика\n",
    "    plt.plot(x, train_losses, label='Train Loss', marker='o', color='blue', linestyle='-', linewidth=2, markersize=5)\n",
    "    \n",
    "    # Настройки графика\n",
    "    plt.title(name, fontsize=16)\n",
    "    plt.xlabel('Batches', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # Отображение графика\n",
    "    plt.show()\n",
    "\n",
    "# Пример использования после обучения\n",
    "plot_train_loss(train_losses, log_interval=log_interval, name='Training Loss Over Batches')\n",
    "plot_train_loss(val_losses, log_interval=100, name='Validation Loss Over Batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c47e6d",
   "metadata": {},
   "source": [
    "#### BERT4Rec + Content embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b58aea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT4RecDataset(Dataset):\n",
    "    def __init__(self, sequences, max_len, mask_prob, num_items, pad_token=0, mask_token=None, is_train=True, external_targets=None):\n",
    "        self.max_len = max_len\n",
    "        self.mask_prob = mask_prob\n",
    "        self.num_items = num_items\n",
    "        self.pad_token = pad_token\n",
    "        self.mask_token = mask_token if mask_token is not None else num_items + 1\n",
    "        self.is_train = is_train \n",
    "        self.external_targets = external_targets  # user_id -> true items (list or set)\n",
    "\n",
    "        self.user_ids = []\n",
    "        self.processed_sequences = []\n",
    "        \n",
    "        for user_id, seq in sequences:\n",
    "            self.user_ids.append(user_id)\n",
    "            truncated = seq[-self.max_len:] if len(seq) > self.max_len else seq\n",
    "            padded = truncated + [self.pad_token] * (self.max_len - len(truncated))  # RIGHT SIDE PADDING\n",
    "            self.processed_sequences.append(padded)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id = self.user_ids[idx]\n",
    "        seq = self.processed_sequences[idx].copy()\n",
    "\n",
    "        # === Validation Mode ===\n",
    "        if not self.is_train and self.external_targets is not None:\n",
    "            input_seq = seq\n",
    "            true_items = list(self.external_targets.get(user_id, []))[:self.max_len]\n",
    "\n",
    "            # RIGHT SIDE PADDING for labels\n",
    "            padded_labels = true_items + [-100] * (self.max_len - len(true_items))  # -100 — default ignore_index (also default in CrossEntropyLoss)\n",
    "            labels = torch.tensor(padded_labels, dtype=torch.long)\n",
    "\n",
    "            attention_mask = [1 if x != self.pad_token else 0 for x in input_seq]\n",
    "            position_ids = torch.arange(self.max_len, dtype=torch.long)\n",
    "\n",
    "            return {\n",
    "                \"user_id\": user_id,\n",
    "                \"input_ids\": torch.tensor(input_seq, dtype=torch.long),\n",
    "                \"labels\": labels,\n",
    "                \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "                \"position_ids\": position_ids\n",
    "            }\n",
    "\n",
    "\n",
    "        # === Train Mode (with guarantee min_masked = 1) ===\n",
    "        input_seq = seq.copy()\n",
    "        target_seq = [-100] * self.max_len\n",
    "\n",
    "        candidate_idxs = [i for i, token in enumerate(seq) if token != self.pad_token]\n",
    "        masked_idxs = [i for i in candidate_idxs if random.random() < self.mask_prob]\n",
    "\n",
    "        # === min_masked = 1 ===\n",
    "        if len(masked_idxs) == 0 and len(candidate_idxs) > 0:\n",
    "            masked_idxs = [random.choice(candidate_idxs)]\n",
    "\n",
    "        for i in masked_idxs: # 90, 9, 1\n",
    "            original_token = seq[i]\n",
    "            prob = random.random()\n",
    "            if prob < 0.9:\n",
    "                input_seq[i] = self.mask_token\n",
    "            elif prob < 0.99:\n",
    "                input_seq[i] = random.randint(1, self.num_items)\n",
    "            else:\n",
    "                input_seq[i] = original_token\n",
    "            target_seq[i] = original_token\n",
    "\n",
    "        attention_mask = [1 if x != self.pad_token else 0 for x in input_seq]\n",
    "        position_ids = torch.arange(self.max_len, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"user_id\": user_id,\n",
    "            \"input_ids\": torch.tensor(input_seq, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(target_seq, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"position_ids\": position_ids\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a296886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2361946, 7)\n",
      "Test shape: (445954, 7)\n",
      "\n",
      "Total number of users: 134362\n",
      "Number of users in the training set: 130282\n",
      "Number of users in the test set: 79468\n",
      "\n",
      "Total number of 'articul_encrypred_idx': 222135\n",
      "Number of 'articul_encrypred_idx' in the training set: 202924\n",
      "Number of 'articul_encrypred_idx' in the test set: 99765\n"
     ]
    }
   ],
   "source": [
    "# df_sales_articul['order_date'] = pd.to_datetime(df_sales_articul['order_date'])\n",
    "# df_sales_articul = df_sales_articul.sort_values(by=['anon_id_encrypred', 'order_date'])\n",
    "\n",
    "# # Leave only users with 5+ purchases (for decreasing the noise)\n",
    "# user_counts = df_sales_articul['anon_id_encrypred'].value_counts()\n",
    "# active_users = user_counts[user_counts >= 5].index\n",
    "# df_sales_articul = df_sales_articul[df_sales_articul['anon_id_encrypred'].isin(active_users)]\n",
    "\n",
    "# # Index mapping\n",
    "# unique_articul_encrypred_id = df_sales_articul['articul_encrypred_id'].unique()\n",
    "# articul_encrypred_id_to_idx = {pid: idx + 1 for idx, pid in enumerate(unique_articul_encrypred_id)} # + 1 for PAD zero token (0)\n",
    "# df_sales_articul['articul_encrypred_idx'] = df_sales_articul['articul_encrypred_id'].map(articul_encrypred_id_to_idx)\n",
    "# num_items = len(articul_encrypred_id_to_idx) \n",
    "\n",
    "# # Train/Test time split\n",
    "# threshold_level = 0.85\n",
    "# min_date = df_sales_articul['order_date'].min()\n",
    "# max_date = df_sales_articul['order_date'].max()\n",
    "\n",
    "# print(f\"Min date: {min_date}\")\n",
    "# print(f\"Max date: {max_date}\")\n",
    "\n",
    "# total_days = (max_date - min_date).days\n",
    "# threshold_days = int(total_days * threshold_level)\n",
    "# threshold_date = min_date + pd.Timedelta(days=threshold_days)\n",
    "\n",
    "# print(f\"Threshold date ({round(threshold_level * 100, 0)}%): {threshold_date}\")\n",
    "\n",
    "# train_df = df_sales_articul[df_sales_articul['order_date'] < threshold_date]\n",
    "# test_df = df_sales_articul[df_sales_articul['order_date'] >= threshold_date]\n",
    "\n",
    "# df_sales_articul.to_csv(interim_data_dir / 'df_sales_articul.csv', index=False)\n",
    "# train_df.to_csv(interim_data_dir / 'train_data_by_threshold_date.csv', index=False)\n",
    "# test_df.to_csv(interim_data_dir / 'test_data_by_threshold_date.csv', index=False)\n",
    "\n",
    "df_sales_articul = pd.read_csv(interim_data_dir / 'df_sales_articul.csv')\n",
    "train_df = pd.read_csv(interim_data_dir / 'train_data_by_threshold_date.csv')\n",
    "test_df = pd.read_csv(interim_data_dir / 'test_data_by_threshold_date.csv')\n",
    "# Reconstructing the articul_encrypred_id_to_idx dictionary from the dataframe\n",
    "articul_encrypred_id_to_idx = dict(df_sales_articul[['articul_encrypred_id', 'articul_encrypred_idx']].drop_duplicates().values)\n",
    "num_items = len(articul_encrypred_id_to_idx)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print()\n",
    "print(f\"Total number of users: {len(df_sales_articul['anon_id_encrypred'].unique())}\")\n",
    "print(f\"Number of users in the training set: {len(train_df['anon_id_encrypred'].unique())}\")\n",
    "print(f\"Number of users in the test set: {len(test_df['anon_id_encrypred'].unique())}\")\n",
    "print()\n",
    "print(f\"Total number of 'articul_encrypred_idx': {len(df_sales_articul['articul_encrypred_idx'].unique())}\")\n",
    "print(f\"Number of 'articul_encrypred_idx' in the training set: {len(train_df['articul_encrypred_idx'].unique())}\")\n",
    "print(f\"Number of 'articul_encrypred_idx' in the test set: {len(test_df['articul_encrypred_idx'].unique())}\")\n",
    "\n",
    "test_user_to_true_items = test_df.groupby('anon_id_encrypred')['articul_encrypred_idx'].apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca2efff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_cols = [\n",
    "#     'CLS_google_vit_huge_patch14_224_in21k',\n",
    "#     'mean_patch_google_vit_huge_patch14_224_in21k',\n",
    "#     'pooled_google_vit_huge_patch14_224_in21k',\n",
    "#     'pooled_microsoft_resnet50',\n",
    "#     'CLS_openai_clip_vit_large_patch14',\n",
    "#     'mean_patch_openai_clip_vit_large_patch14',\n",
    "#     'pooled_openai_clip_vit_large_patch14',\n",
    "#     'embedding_e5_large_v2',\n",
    "#     'embedding_bge_large_en_v15',\n",
    "#     'embedding_nomic_embed_text_v15',\n",
    "# ]\n",
    "\n",
    "# extra_numeric_cols = ['product_created_at_day', 'sales_total']\n",
    "\n",
    "# embedding_dim = sum(len(df_products_articul[col].iloc[0]) for col in embedding_cols) + len(extra_numeric_cols)\n",
    "\n",
    "# # [num_items + 2, embedding_dim]\n",
    "# item_features_matrix = np.zeros((num_items + 2, embedding_dim), dtype=np.float32)\n",
    "\n",
    "# for _, row in df_products_articul.iterrows():\n",
    "#     pid = row['articul_encrypred_id']\n",
    "#     idx = articul_encrypred_id_to_idx.get(pid)\n",
    "#     if idx is None:\n",
    "#         continue\n",
    "\n",
    "#     vec = []\n",
    "#     for col in embedding_cols:\n",
    "#         vec.extend(row[col])  # col — list with float\n",
    "#     for col in extra_numeric_cols:\n",
    "#         vec.append(float(row[col]))\n",
    "    \n",
    "#     item_features_matrix[idx] = np.array(vec, dtype=np.float32)\n",
    "\n",
    "# item_features_tensor = torch.tensor(item_features_matrix)  # [num_items + 2, embedding_dim]\n",
    "\n",
    "# assert sum(item_features_tensor[0]) == sum(item_features_tensor[-1]) == 0, f\"PAD or MASK tokens have non-zero embeddings!\"\n",
    "\n",
    "# # Save\n",
    "# np.save(interim_data_dir / 'item_features_tensor.npy', item_features_tensor.cpu().numpy())\n",
    "\n",
    "# Load\n",
    "item_features_tensor = torch.from_numpy(np.load(interim_data_dir / 'item_features_tensor.npy')).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82b4d4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length percentiles:\n",
      "0.25      7.0\n",
      "0.50     11.0\n",
      "0.75     21.0\n",
      "0.90     44.0\n",
      "0.95     68.0\n",
      "0.99    159.0\n",
      "dtype: float64\n",
      "Maximum sequence length: 2192\n"
     ]
    }
   ],
   "source": [
    "# Group by users and compute the sequence length for each\n",
    "user_sequence_lengths = df_sales_articul.groupby('anon_id_encrypred').size()\n",
    "percentiles = user_sequence_lengths.quantile([0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\n",
    "\n",
    "print(\"Sequence length percentiles:\")\n",
    "print(percentiles)\n",
    "print(f\"Maximum sequence length: {user_sequence_lengths.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c112182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sequence for user \u0004\u000eqqwrtrxq: [1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Create purchase sequences for each user\n",
    "train_sequences = train_df.groupby('anon_id_encrypred')['articul_encrypred_idx'].apply(list).to_dict()\n",
    "test_sequences = test_df.groupby('anon_id_encrypred')['articul_encrypred_idx'].apply(list).to_dict()\n",
    "\n",
    "print(f\"Example sequence for user {list(train_sequences.keys())[0]}: {train_sequences[list(train_sequences.keys())[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "251a43ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20 # Maximum sequence length\n",
    "mask_prob = 0.25 # Masking probability\n",
    "\n",
    "# Convert sequence dictionaries to list of tuples (user_id, sequence)\n",
    "train_sequences_list = list(train_sequences.items())\n",
    "test_sequences_list = list(test_sequences.items())\n",
    "\n",
    "train_dataset = BERT4RecDataset(\n",
    "    sequences=train_sequences_list,\n",
    "    max_len=max_len,\n",
    "    mask_prob=mask_prob,\n",
    "    num_items=num_items,\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "test_dataset = BERT4RecDataset(\n",
    "    sequences=train_sequences_list,     # <-- train for inputs\n",
    "    max_len=max_len,\n",
    "    mask_prob=mask_prob,\n",
    "    num_items=num_items,\n",
    "    is_train=False,\n",
    "    external_targets=test_sequences  # <-- targets from test_df\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77f809ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One object from `train_loader`\n",
      "User ID: wyyypvqpqqtxpxvy\n",
      "Input IDs: tensor([156245, 171638, 222136, 222136,  37442, 222136,  28303, 222136, 222136,\n",
      "        144029,  33583, 222136,  98483,  47652,  81236,  19124, 171488, 222136,\n",
      "        159717, 191441])\n",
      "Labels: tensor([  -100,   -100, 164587,  91666,   -100,   2602,   -100, 171870,  37442,\n",
      "          -100,   -100,  98983,   -100,   -100,   -100,   -100,   -100, 113009,\n",
      "          -100,   -100])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Position IDs: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19])\n",
      "\n",
      "One object from `test_dataset`\n",
      "User ID: \u0004\u000eqqwrtrxq\n",
      "Input IDs: tensor([1, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Labels: tensor([   5, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Position IDs: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print('One object from `train_loader`')\n",
    "    user_ids = batch['user_id'][0]\n",
    "    input_ids = batch['input_ids'][0]\n",
    "    labels = batch['labels'][0]\n",
    "    attention_mask = batch['attention_mask'][0]\n",
    "    position_ids = batch['position_ids'][0]\n",
    "    \n",
    "    print(\"User ID:\", user_ids)\n",
    "    print(\"Input IDs:\", input_ids)\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Attention Mask:\", attention_mask)\n",
    "    print(\"Position IDs:\", position_ids)\n",
    "    break \n",
    "\n",
    "print()\n",
    "\n",
    "for batch in test_loader:\n",
    "    print('One object from `test_dataset`')\n",
    "    user_ids = batch['user_id'][0]\n",
    "    input_ids = batch['input_ids'][0]\n",
    "    labels = batch['labels'][0]\n",
    "    attention_mask = batch['attention_mask'][0]\n",
    "    position_ids = batch['position_ids'][0]\n",
    "    \n",
    "    print(\"User ID:\", user_ids)\n",
    "    print(\"Input IDs:\", input_ids)\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Attention Mask:\", attention_mask)\n",
    "    print(\"Position IDs:\", position_ids)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e1392fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [1, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Index: 1 → Articul: 650\n",
      "Index: 2 → Articul: 112123\n",
      "Index: 3 → Articul: 60067\n",
      "Index: 4 → Articul: 2464\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n",
      "PAD\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(test_loader))\n",
    "sample_input_ids = batch['input_ids'][0]\n",
    "print(\"Input IDs:\", sample_input_ids.tolist())\n",
    "\n",
    "idx_to_articul = {v: k for k, v in articul_encrypred_id_to_idx.items()}\n",
    "for idx in sample_input_ids.tolist():\n",
    "    if idx == 0:\n",
    "        print(\"PAD\")\n",
    "    elif idx == num_items + 1:\n",
    "        print(\"MASK\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Index: {idx} → Articul: {idx_to_articul[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69f48a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT4RecModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_items,\n",
    "                 max_len,\n",
    "                 embedding_dim=256,\n",
    "                 num_layers=6,\n",
    "                 num_heads=4,\n",
    "                 dropout=0.1,\n",
    "                 ffn_dim=1024,\n",
    "                 external_features=None):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_items = num_items\n",
    "        self.max_len = max_len\n",
    "        ext_dim = item_features_tensor.shape[1]\n",
    "        proj_dim = embedding_dim // 2\n",
    "\n",
    "        # PAD = 0, items = 1..num_items, MASK = num_items+1\n",
    "        self.item_embeddings = nn.Embedding(num_items + 2, embedding_dim, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(max_len, embedding_dim)\n",
    "        self.external_embeddings = nn.Embedding.from_pretrained(external_features, freeze=True, padding_idx=0)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.external_proj = nn.Sequential(\n",
    "            nn.Linear(ext_dim, proj_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(proj_dim)\n",
    "        )\n",
    "\n",
    "        self.combine_proj = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + proj_dim, embedding_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ffn_dim,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer = TransformerEncoder(encoder_layer,\n",
    "                                              num_layers=num_layers)\n",
    "\n",
    "        # ——— Новая классифицирующая голова ———\n",
    "        # Проецируем обратно в embedding_dim, чтобы можно было dot-product'ом\n",
    "        self.fc_head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(embedding_dim * 2),\n",
    "            nn.Linear(embedding_dim * 2, embedding_dim, bias=False),\n",
    "            # nn.LayerNorm(embedding_dim),  \n",
    "        )\n",
    "        # bias для logits по словарю\n",
    "        self.vocab_bias = nn.Parameter(torch.zeros(num_items + 2))\n",
    "\n",
    "        # weight-tying: последний Linear выдаёт embedding_dim,\n",
    "        # а дальше dot(item_emb.weight.T) дает logits по vocab\n",
    "        # (embedding_dim == self.item_embeddings.embedding_dim)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # embs\n",
    "        nn.init.xavier_uniform_(self.item_embeddings.weight)\n",
    "        # nn.init.normal_(self.item_embeddings.weight, 0.0, 0.02)\n",
    "        nn.init.normal_(self.position_embeddings.weight, 0.0, 0.02)\n",
    "\n",
    "        # transformer\n",
    "        for layer in self.transformer.layers:\n",
    "            nn.init.xavier_uniform_(layer.self_attn.in_proj_weight)\n",
    "            nn.init.xavier_uniform_(layer.self_attn.out_proj.weight)\n",
    "            nn.init.xavier_uniform_(layer.linear1.weight)\n",
    "            nn.init.xavier_uniform_(layer.linear2.weight)\n",
    "\n",
    "        # heads\n",
    "        nn.init.xavier_uniform_(self.fc_head[0].weight)\n",
    "        nn.init.xavier_uniform_(self.fc_head[3].weight)\n",
    "        nn.init.zeros_(self.vocab_bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, position_ids):\n",
    "        B, L = input_ids.size()\n",
    "        # 1) Embeddings + scale\n",
    "        external_embeds = self.external_embeddings(input_ids)\n",
    "        external_embeds = self.external_proj(external_embeds)\n",
    "        item_embeds = self.item_embeddings(input_ids)             # (B,L,dim)\n",
    "        item_embeds = torch.cat([item_embeds, external_embeds], dim=-1)\n",
    "        item_embeds = self.combine_proj(item_embeds)\n",
    "        pos_embeds  = self.position_embeddings(position_ids)      # (L,dim) -> broadcast\n",
    "        x = (item_embeds + pos_embeds) * math.sqrt(self.embedding_dim)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 2) Causal + padding masks\n",
    "        src_key_padding_mask = (attention_mask == 0)              # (B,L)\n",
    "\n",
    "        # 3) Transformer\n",
    "        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)  # (B,L,dim)\n",
    "\n",
    "        # 4) Head → back to embedding space\n",
    "        h = self.fc_head(x)                                      # (B,L,dim)\n",
    "\n",
    "        # 5) Dot-product with item_embeddings + bias → logits\n",
    "        logits = torch.matmul(h, self.item_embeddings.weight.T)  # (B,L,num_items+2)\n",
    "        logits = logits + self.vocab_bias                        # broadcast bias\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c690471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    save_path: str,\n",
    "    num_epochs: int = 5,\n",
    "    log_interval: int = 50,\n",
    "    scheduler = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Тренирует модель с выводом прогресса через tqdm и периодическим логированием лосса.\n",
    "    Возвращает модель и историю лоссов.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100, label_smoothing=0.1) # label_smoothing=0.1 размывание таргета\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        # running_loss = 0.0\n",
    "        running, acc_steps = 0.0, 0\n",
    "\n",
    "        for n,p in model.named_parameters():\n",
    "            assert torch.all(torch.isfinite(p)), f\"NaN в параметре {n}\"\n",
    "\n",
    "\n",
    "        # оборачиваем сам loader, а затем делаем enumerate над tqdm-ом\n",
    "        pbar_train = tqdm(train_loader, total=len(train_loader),\n",
    "                          desc=f\"[Epoch {epoch}/{num_epochs}] Train\")\n",
    "        for batch_idx, batch in enumerate(pbar_train, 1):\n",
    "            try:\n",
    "                input_ids      = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                position_ids   = batch['position_ids'].to(device)\n",
    "                labels         = batch['labels'].to(device)\n",
    "\n",
    "                # === ✅ ДОПОЛНИТЕЛЬНАЯ ПРОВЕРКА ПЕРЕД LOSS ===\n",
    "                # print(f\"[Batch {batch_idx}] input_ids: min={input_ids.min().item()}, max={input_ids.max().item()}\")\n",
    "                # print(f\"[Batch {batch_idx}] labels:    min={labels[labels != -100].min().item() if (labels != -100).any() else 'n/a'}, \"\n",
    "                #     f\"max={labels[labels != -100].max().item() if (labels != -100).any() else 'n/a'}\")\n",
    "\n",
    "                # Проверка на допустимые значения\n",
    "                assert torch.all((labels == -100) | ((labels > 0) & (labels < num_items + 1))), \\\n",
    "                    f\"Invalid label value found! batch_idx={batch_idx}\"\n",
    "\n",
    "                assert input_ids.max() < num_items + 2, f\"input_ids contain invalid token! batch_idx={batch_idx}\"\n",
    "\n",
    "                if not (labels != -100).any():\n",
    "                    print(f\"[Batch {batch_idx}] All masked, skipping\")\n",
    "                    pbar_train.update(1)\n",
    "                    continue\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(input_ids, attention_mask, position_ids)\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "\n",
    "                if not torch.isfinite(loss):\n",
    "                    print(f\"!!! NaN loss at epoch {epoch} batch {batch_idx}\")\n",
    "\n",
    "                    for name, p in model.named_parameters():\n",
    "                        if not torch.all(torch.isfinite(p)):\n",
    "                            bad = (~torch.isfinite(p)).sum().item()\n",
    "                            print(f\"  → {name}: {bad} bad entries\")\n",
    "                    raise RuntimeError(\"Stop: loss became NaN\")\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "                running += loss.item()\n",
    "                acc_steps += 1\n",
    "\n",
    "                # log n первых/каждый log_interval\n",
    "                if batch_idx == 1 or (acc_steps % log_interval == 0):\n",
    "                    avg = running / acc_steps\n",
    "                    pbar_train.set_postfix(batch_loss=f\"{avg:.4f}\")\n",
    "                    train_loss_history.append(avg)\n",
    "                    wandb.log({\n",
    "                        \"train/loss\": avg,\n",
    "                        \"train/lr\": scheduler.get_last_lr()[0] if scheduler else optimizer.param_groups[0][\"lr\"],\n",
    "                        \"epoch\": epoch,\n",
    "                        \"step\": (epoch - 1) * len(train_loader) + batch_idx\n",
    "                    })\n",
    "                    current_lr = scheduler.get_last_lr()[0] if scheduler is not None else float('nan')\n",
    "                    valid_tokens = (labels != -100).sum().item()\n",
    "                    masked_ratio = valid_tokens / labels.numel()\n",
    "                    preds = outputs.argmax(dim=-1)\n",
    "                    correct = ((preds == labels) & (labels != -100)).sum().item()\n",
    "                    accuracy = correct / valid_tokens if valid_tokens else 0.0\n",
    "                    max_grad = max(p.grad.abs().max().item() for p in model.parameters() if p.grad is not None)\n",
    "                \n",
    "                    print(\n",
    "                        f\"[Batch {batch_idx}] \"\n",
    "                        f\"train/loss={avg};\\n\"\n",
    "                        f\"input_ids={input_ids.shape}, labels={labels.shape}, outputs={outputs.shape};\\n\"\n",
    "                        f\"lr={current_lr:.8f}, \"\n",
    "                        f\"valid_tokens={valid_tokens}, masked_ratio={masked_ratio:.2%}, \"\n",
    "                        f\"accuracy={accuracy:.2%}, max_grad={max_grad:.4f}\\n\"\n",
    "                    )\n",
    "                    running, acc_steps = 0.0, 0\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at batch {batch_idx}: {e}\")\n",
    "\n",
    "\n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        # val_running = 0.0\n",
    "        val_running, val_batches = 0.0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pbar_val = tqdm(val_loader, total=len(val_loader), desc=f\"[Epoch {epoch}/{num_epochs}] Val  \")\n",
    "            for batch in pbar_val:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                position_ids = batch['position_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                if (labels != -100).any():           # есть валид-таргеты\n",
    "                    outputs = model(input_ids, attention_mask, position_ids)\n",
    "                    loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "                    val_running += loss.item()\n",
    "                    val_batches += 1\n",
    "                    pbar_val.set_postfix(val_loss=f\"{loss.item():.4f}\")\n",
    "                else:\n",
    "                    pbar_val.set_postfix(val_loss=\"skip\")\n",
    "\n",
    "        avg_val = val_running / val_batches if val_batches else float(\"nan\")\n",
    "        val_loss_history.append(avg_val)\n",
    "        print(f\"Epoch {epoch}: val_loss = {avg_val:.4f}\")\n",
    "\n",
    "        # сохраняем лучшую модель\n",
    "        if avg_val < best_val_loss:\n",
    "            best_val_loss = avg_val\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            wandb.save(str(save_path))\n",
    "            print(f\"→ saved new best model  (val_loss {best_val_loss:.4f})\\n\")\n",
    "\n",
    "        wandb.log({\n",
    "            \"val/loss\": avg_val,\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    return model, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a87c652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/likvh7dk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x354676f60>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "embedding_dim = 768\n",
    "num_layers = 8\n",
    "num_heads = 6\n",
    "ffn_dim = 2048\n",
    "dropout = 0.05\n",
    "lr = 3e-4\n",
    "log_interval = max(1, len(train_loader) // 20)\n",
    "num_epochs = 10\n",
    "\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "model = BERT4RecModel(\n",
    "    num_items=num_items,\n",
    "    max_len=max_len,\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    ffn_dim=ffn_dim,\n",
    "    dropout=dropout,\n",
    "    external_features=item_features_tensor\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "\n",
    "\n",
    "scheduler = LambdaLR(optimizer,\n",
    "    lr_lambda=lambda step: (step / warmup_steps if step < warmup_steps\n",
    "        else max(0.0, (total_steps - step) / float(max(1, total_steps - warmup_steps)))\n",
    "    )\n",
    ")\n",
    "\n",
    "save_path = models_dir / f'bert4rec_model_articul_encrypred_id_embedding_dim_{embedding_dim}_epochs_{num_epochs}_lr_{lr}_max_len_{max_len}_mask_prob_{mask_prob}_num_layers_{num_layers}_num_heads_{num_heads}_ffn_dim_{ffn_dim}_dropout_{dropout}_num_items_{num_items}_with_external_embs.pth'\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Recommender_System_with_LLM\",\n",
    "    name=f'bert4rec_model_articul_encrypred_id_embedding_dim_{embedding_dim}_epochs_{num_epochs}_lr_{lr}_max_len_{max_len}_mask_prob_{mask_prob}_num_layers_{num_layers}_num_heads_{num_heads}_ffn_dim_{ffn_dim}_dropout_{dropout}_num_items_{num_items}_with_external_embs',\n",
    "    config={\n",
    "        \"epochs\": num_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": lr,\n",
    "        \"embedding_dim\": embedding_dim,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"ffn_dim\": ffn_dim,\n",
    "        \"dropout\": dropout,\n",
    "        \"max_len\": max_len,\n",
    "        \"mask_prob\": mask_prob,\n",
    "        \"scheduler\": \"linear with warmup\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50f422ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT4RecModel(\n",
       "  (item_embeddings): Embedding(222137, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(20, 768)\n",
       "  (external_embeddings): Embedding(222137, 11266, padding_idx=0)\n",
       "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.05, inplace=False)\n",
       "  (external_proj): Sequential(\n",
       "    (0): Linear(in_features=11266, out_features=384, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (combine_proj): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=768, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.05, inplace=False)\n",
       "        (dropout2): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Linear(in_features=1536, out_features=768, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d6ae0b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66223016f3874e3f82ed668525f0323c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Epoch 1/10] Train:   0%|          | 0/2036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1] train/loss=12.320131301879883;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00000015, valid_tokens=186, masked_ratio=14.53%, accuracy=0.00%, max_grad=0.0867\n",
      "\n",
      "[Batch 102] train/loss=12.284293524109492;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00001503, valid_tokens=209, masked_ratio=16.33%, accuracy=0.00%, max_grad=0.0517\n",
      "\n",
      "[Batch 203] train/loss=12.08257781869114;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00002991, valid_tokens=176, masked_ratio=13.75%, accuracy=0.57%, max_grad=0.0441\n",
      "\n",
      "[Batch 304] train/loss=11.986538565985047;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00004479, valid_tokens=187, masked_ratio=14.61%, accuracy=0.53%, max_grad=0.0777\n",
      "\n",
      "[Batch 405] train/loss=11.962498202182278;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00005968, valid_tokens=197, masked_ratio=15.39%, accuracy=0.00%, max_grad=0.0517\n",
      "\n",
      "[Batch 506] train/loss=11.940158135820143;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00007456, valid_tokens=197, masked_ratio=15.39%, accuracy=0.00%, max_grad=0.2791\n",
      "\n",
      "[Batch 607] train/loss=11.896908372935682;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00008944, valid_tokens=194, masked_ratio=15.16%, accuracy=0.00%, max_grad=0.2980\n",
      "\n",
      "[Batch 708] train/loss=11.859923277751054;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00010432, valid_tokens=196, masked_ratio=15.31%, accuracy=1.02%, max_grad=0.3125\n",
      "\n",
      "[Batch 809] train/loss=11.83875075425252;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00011920, valid_tokens=191, masked_ratio=14.92%, accuracy=0.00%, max_grad=0.0840\n",
      "\n",
      "[Batch 910] train/loss=11.812809887498911;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00013409, valid_tokens=194, masked_ratio=15.16%, accuracy=0.00%, max_grad=0.2850\n",
      "\n",
      "[Batch 1011] train/loss=11.796964550962542;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00014897, valid_tokens=191, masked_ratio=14.92%, accuracy=0.00%, max_grad=0.2252\n",
      "\n",
      "[Batch 1112] train/loss=11.786145786247632;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00016385, valid_tokens=173, masked_ratio=13.52%, accuracy=0.00%, max_grad=0.1135\n",
      "\n",
      "[Batch 1213] train/loss=11.783961314966183;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00017873, valid_tokens=207, masked_ratio=16.17%, accuracy=0.00%, max_grad=0.3296\n",
      "\n",
      "[Batch 1314] train/loss=11.758408272620475;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00019361, valid_tokens=212, masked_ratio=16.56%, accuracy=0.00%, max_grad=0.2975\n",
      "\n",
      "[Batch 1415] train/loss=11.748927597952362;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00020850, valid_tokens=179, masked_ratio=13.98%, accuracy=0.00%, max_grad=0.0891\n",
      "\n",
      "[Batch 1516] train/loss=11.732293950449122;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00022338, valid_tokens=183, masked_ratio=14.30%, accuracy=0.55%, max_grad=0.3400\n",
      "\n",
      "[Batch 1617] train/loss=11.702476567561083;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00023826, valid_tokens=162, masked_ratio=12.66%, accuracy=0.00%, max_grad=0.0497\n",
      "\n",
      "[Batch 1718] train/loss=11.697490427753714;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00025314, valid_tokens=161, masked_ratio=12.58%, accuracy=0.00%, max_grad=0.2618\n",
      "\n",
      "[Batch 1819] train/loss=11.68726386646233;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00026803, valid_tokens=186, masked_ratio=14.53%, accuracy=0.54%, max_grad=0.2509\n",
      "\n",
      "[Batch 1920] train/loss=11.711449141549592;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00028291, valid_tokens=169, masked_ratio=13.20%, accuracy=0.00%, max_grad=0.0973\n",
      "\n",
      "[Batch 2021] train/loss=11.667569538154224;\n",
      "input_ids=torch.Size([64, 20]), labels=torch.Size([64, 20]), outputs=torch.Size([64, 20, 222137]);\n",
      "lr=0.00029779, valid_tokens=185, masked_ratio=14.45%, accuracy=0.00%, max_grad=0.0780\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01299ec95b34b85a919a43557291f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Epoch 1/10] Val  :   0%|          | 0/2036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss = 12.0957\n",
      "→ saved new best model  (val_loss 12.0957)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 40.01 GB, other allocations: 1.91 GB, max allowed: 43.20 GB). Tried to allocate 2.33 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trained_model, train_losses, val_losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m torch.save(trained_model.state_dict(), models_dir / \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLAST_EPOCH_bert4rec_model_embedding_dim_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_epochs_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_max_len_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_mask_prob_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_prob\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_num_layers_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_num_heads_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_ffn_dim_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mffn_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_dropout_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdropout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_num_items_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_items\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLast train loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_losses[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, device, save_path, num_epochs, log_interval, scheduler)\u001b[39m\n\u001b[32m     25\u001b[39m running, acc_steps = \u001b[32m0.0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n,p \u001b[38;5;129;01min\u001b[39;00m model.named_parameters():\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m torch.all(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNaN в параметре \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# оборачиваем сам loader, а затем делаем enumerate над tqdm-ом\u001b[39;00m\n\u001b[32m     32\u001b[39m pbar_train = tqdm(train_loader, total=\u001b[38;5;28mlen\u001b[39m(train_loader),\n\u001b[32m     33\u001b[39m                   desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Train\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 40.01 GB, other allocations: 1.91 GB, max allowed: 43.20 GB). Tried to allocate 2.33 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "trained_model, train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    log_interval=log_interval,\n",
    "    save_path=save_path,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "torch.save(trained_model.state_dict(), models_dir / f'LAST_EPOCH_bert4rec_model_embedding_dim_{embedding_dim}_epochs_{num_epochs}_lr_{lr}_max_len_{max_len}_mask_prob_{mask_prob}_num_layers_{num_layers}_num_heads_{num_heads}_ffn_dim_{ffn_dim}_dropout_{dropout}_num_items_{num_items}.pth')\n",
    "\n",
    "print(f\"Last train loss = {train_losses[-1]:.4f}\")\n",
    "print(f\"Last val loss = {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d207d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8514c1f-e421-4ccd-af89-a3a4c6aeeba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdfa6aa-f438-4a9d-bd6f-ff2b52a1207e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e524b397-2aae-4e86-b2d0-7dcacc8a3ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd07e1bc-ef20-4889-9275-e36a7535b48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3cbee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f18649fe",
   "metadata": {},
   "source": [
    "#### BERT4Rec Generation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70412d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для параллельной генерации рекомендаций (на входе батч из тестового DataLoader)\n",
    "def generate_parallel_recommendations(model, input_ids, attention_mask, position_ids, k=10, device=device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        position_ids = position_ids.to(device)\n",
    "        \n",
    "        # Получаем выходы модели: [B, seq_len, num_items+1]\n",
    "        outputs = model(input_ids, attention_mask, position_ids)\n",
    "        # Берем логиты последнего токена (последний временной шаг)\n",
    "        logits = outputs[:, -1, :]  # размер [B, num_items+1]\n",
    "        # Выбираем топ-k рекомендаций для каждого примера\n",
    "        recs = torch.topk(logits, k=k, dim=-1).indices\n",
    "    return recs\n",
    "\n",
    "# Функция для последовательной генерации рекомендаций для одного пользователя\n",
    "def generate_sequential_recommendations(model, initial_sequence, max_len, k=10, device=device):\n",
    "    \"\"\"\n",
    "    Генерирует последовательные рекомендации (авторегрессивно) для одного пользователя.\n",
    "    \n",
    "    :param initial_sequence: Исходная последовательность (список int) без паддинга.\n",
    "    :param max_len: Максимальная длина последовательности, с которой обучалась модель.\n",
    "    :param k: Количество генерируемых рекомендаций.\n",
    "    :param device: Устройство.\n",
    "    :return: Список сгенерированных рекомендаций.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated = []\n",
    "    current_seq = initial_sequence.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(k):\n",
    "            # Если длина последовательности меньше max_len – дополняем слева паддингом (значение 0)\n",
    "            if len(current_seq) < max_len:\n",
    "                padded_seq = [0] * (max_len - len(current_seq)) + current_seq\n",
    "            else:\n",
    "                padded_seq = current_seq[-max_len:]\n",
    "            \n",
    "            input_ids = torch.tensor(padded_seq, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            attention_mask = (input_ids != 0).long()\n",
    "            position_ids = torch.arange(max_len, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, position_ids)  # [1, max_len, num_items+1]\n",
    "            logits = outputs[:, -1, :]  # [1, num_items+1]\n",
    "            next_token = torch.topk(logits, k=1, dim=-1).indices.squeeze().item()\n",
    "            generated.append(next_token)\n",
    "            current_seq.append(next_token)\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389ba619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_inference(model, inference_loader, k=10, device=device):\n",
    "    \"\"\"\n",
    "    Запускает инференс на inference_loader и получает топ-K рекомендаций для каждого пользователя.\n",
    "    \n",
    "    :param model: Обученная модель BERT4Rec\n",
    "    :param inference_loader: DataLoader без маскированных токенов (данные из train)\n",
    "    :param k: Количество рекомендаций\n",
    "    :param device: Устройство (CPU/GPU)\n",
    "    :return: Список предсказанных рекомендаций для всех пользователей\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_recommendations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(inference_loader, desc=\"Parallel Inference\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            position_ids = batch[\"position_ids\"].to(device)\n",
    "\n",
    "            # Генерируем k рекомендаций\n",
    "            recs = generate_parallel_recommendations(model, input_ids, attention_mask, position_ids, k=k, device=device)\n",
    "            \n",
    "            all_recommendations.extend(recs.cpu().tolist())\n",
    "\n",
    "    return all_recommendations\n",
    "\n",
    "\n",
    "def run_sequential_inference(model, inference_loader, max_len, k=10, device=device):\n",
    "    \"\"\"\n",
    "    Запускает последовательный инференс: для каждого пользователя из inference_loader извлекается исходная (непадённая) последовательность,\n",
    "    и генерируется последовательность рекомендаций методом авторегрессии.\n",
    "    \n",
    "    :param model: Обученная модель BERT4Rec.\n",
    "    :param inference_loader: DataLoader с данными (is_train=False).\n",
    "    :param max_len: Максимальная длина последовательности.\n",
    "    :param k: Количество генерируемых рекомендаций для каждого пользователя.\n",
    "    :param device: Устройство.\n",
    "    :return: Словарь вида {user_id: [последовательность рекомендаций]}.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    user_recs = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(inference_loader, desc=\"Sequential Inference\"):\n",
    "            input_ids = batch['input_ids']           # [B, max_len]\n",
    "            attention_mask = batch['attention_mask']   # [B, max_len]\n",
    "            user_ids = batch['user_id']                # список user_id\n",
    "            \n",
    "            # Для каждого пользователя в батче извлекаем исходную последовательность без паддинга\n",
    "            for i in range(input_ids.shape[0]):\n",
    "                # Переходим на CPU, чтобы легко работать со списками\n",
    "                seq = input_ids[i].cpu().tolist()\n",
    "                mask = attention_mask[i].cpu().tolist()\n",
    "                # Извлекаем только те токены, где mask==1 (непаддинговые элементы)\n",
    "                initial_seq = [token for token, m in zip(seq, mask) if m == 1]\n",
    "                recs = generate_sequential_recommendations(model, initial_seq, max_len, k=k, device=device)\n",
    "                user_recs[user_ids[i]] = recs\n",
    "\n",
    "    return user_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d5cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем train-данные для генерации предсказаний\n",
    "inference_dataset = BERT4RecDataset(sequences=train_sequences_list, max_len=max_len, mask_prob=0.0, num_items=num_items, is_train=False)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb18144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_recommendations_to_users(user_ids, recommendations):\n",
    "    \"\"\"\n",
    "    Преобразует список рекомендаций в словарь {user_id: recommendations}.\n",
    "\n",
    "    :param user_ids: Список ID пользователей из inference_loader\n",
    "    :param recommendations: Список рекомендаций из parallel_recs\n",
    "    :return: Словарь {user_id: [recommendations]}\n",
    "    \"\"\"\n",
    "    user_to_recs = {user: recs for user, recs in zip(user_ids, recommendations)}\n",
    "    return user_to_recs\n",
    "\n",
    "# Получаем список пользователей из inference_loader\n",
    "user_ids = inference_loader.dataset.user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6725a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параллельные предсказания (Batch Inference)\n",
    "parallel_recs = run_parallel_inference(model, inference_loader, k=k, device=device)\n",
    "\n",
    "# Создаём словарь {user_id: recommendations}\n",
    "test_user_to_parallel_recs = map_recommendations_to_users(user_ids, parallel_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Последовательные предсказания (Sequential Inference)\n",
    "sequential_recs = run_sequential_inference(model, inference_loader, max_len=max_len, k=k, device=device)\n",
    "\n",
    "# Создаём словарь {user_id: recommendations}\n",
    "test_user_to_sequential_recs = map_recommendations_to_users(user_ids, sequential_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9745bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8dc9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec10d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343f58d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba432f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2310b650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94a36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8198e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_lr_finder import TrainDataLoaderIter\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss(ignore_index=0) # игнорим паддинги\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), \n",
    "#                              # lr=1e-5, # Было 1e-4\n",
    "#                              weight_decay=0.01)\n",
    "\n",
    "# class CustomTrainDataLoaderIter(TrainDataLoaderIter):\n",
    "#     def inputs_labels_from_batch(self, batch_data):\n",
    "#         # Извлекаем данные из батча\n",
    "#         inputs = {\n",
    "#             'input_ids': batch_data['input_ids'],          # Input IDs\n",
    "#             'attention_mask': batch_data['attention_mask'], # Attention Mask\n",
    "#             'position_ids': batch_data['position_ids']     # Position IDs\n",
    "#         }\n",
    "#         labels = batch_data['labels']  # Labels\n",
    "#         return inputs, labels\n",
    "\n",
    "\n",
    "# # Создаем кастомный итератор\n",
    "# train_iter = CustomTrainDataLoaderIter(train_loader)\n",
    "\n",
    "# # Инициализация LRFinder\n",
    "# lr_finder = LRFinder(model, optimizer, criterion)\n",
    "\n",
    "# # Запуск поиска learning rate\n",
    "# lr_finder.range_test(train_iter, end_lr=0.1, num_iter=100)\n",
    "\n",
    "# # Визуализация результатов\n",
    "# lr_finder.plot()\n",
    "\n",
    "# # Сброс состояния модели\n",
    "# lr_finder.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Создаем экземпляр модели\n",
    "# model = BERT4RecModel(\n",
    "#     num_items=num_items,\n",
    "#     max_len=max_len,\n",
    "#     embedding_dim=256,\n",
    "#     num_layers=4,       # Кол-во атеншн слоёв (Аттеншн + лин + нелин + дропаут)\n",
    "#     num_heads=8,        # Головы аттеншн слоёв\n",
    "#     dropout=0.1\n",
    "# ).to(device)\n",
    "\n",
    "# # Загружаем веса\n",
    "# model.load_state_dict(torch.load(models_path / 'bert4rec_embedding_dim_256_num_layers_3_num_heads_4_dropout_001_lr_1e5_base.pth'))\n",
    "\n",
    "# # Переводим модель в режим оценки\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45562441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667dc57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71b6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300236e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce8762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data preparing:   0%|          | 19/309343 [01:21<367:54:27,  4.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# df_filtered = pd.concat([train_df, test_df], ignore_index=True)  # если необходимо объединить\u001b[39;00m\n\u001b[32m     79\u001b[39m \n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Предположим, что у вас уже есть df_filtered с нужными столбцами\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m product_meta_dict = \u001b[43mcompute_product_meta_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_filtered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Количество уникальных товаров\u001b[39;00m\n\u001b[32m     84\u001b[39m num_items = df_filtered[\u001b[33m'\u001b[39m\u001b[33mproduct_idx\u001b[39m\u001b[33m'\u001b[39m].nunique()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mcompute_product_meta_features\u001b[39m\u001b[34m(df, meta_dim, device)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Извлекаем признаки изображения\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mphoto_analytics\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     image = Image.open(BytesIO(response.content)).convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     59\u001b[39m     image = image_transform(image).unsqueeze(\u001b[32m0\u001b[39m).to(device)  \u001b[38;5;66;03m# переносим на нужное устройство\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Downloads/my_python_projects/project_for_TSUM_V2/TSUM_recommender_system/venv/lib/python3.13/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Downloads/my_python_projects/project_for_TSUM_V2/TSUM_recommender_system/venv/lib/python3.13/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Downloads/my_python_projects/project_for_TSUM_V2/TSUM_recommender_system/venv/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Downloads/my_python_projects/project_for_TSUM_V2/TSUM_recommender_system/venv/lib/python3.13/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Downloads/my_python_projects/project_for_TSUM_V2/TSUM_recommender_system/venv/lib/python3.13/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Downloads/my_python_projects/project_for_TSUM_V2/TSUM_recommender_system/venv/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Downloads/my_python_projects/project_for_TSUM_V2/TSUM_recommender_system/venv/lib/python3.13/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/Downloads/my_python_projects/project_for_TSUM_V2/TSUM_recommender_system/venv/lib/python3.13/site-packages/urllib3/connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    513\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    519\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# Дополнительные импорты для работы с изображениями и текстом\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "torch.mps.set_per_process_memory_fraction(0.95)  # Ограничение памяти до 80%\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "#########################\n",
    "# 1. Предобработка метаданных\n",
    "#########################\n",
    "\n",
    "def compute_product_meta_features(df, meta_dim=128, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Вычисляет мета-вектор для каждого уникального продукта.\n",
    "    Используются признаки: brand, title, color_base, ktt1 (категория) и изображение из photo_analytics.\n",
    "    \"\"\"\n",
    "    # Инициализируем модель для текстовых эмбеддингов\n",
    "    text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Инициализируем предобученную модель для изображений (удаляем последний классификационный слой)\n",
    "    image_model = models.resnet18(pretrained=True)\n",
    "    image_model.fc = nn.Identity()\n",
    "    image_model = image_model.to(device)\n",
    "    image_model.eval()  # замораживаем веса для извлечения признаков\n",
    "    \n",
    "    image_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    product_meta = {}\n",
    "    unique_products = df.drop_duplicates(subset=['articul_encrypred_idx'])\n",
    "    \n",
    "    for _, row in tqdm(unique_products.iterrows(), desc='Data preparing', total=unique_products.shape[0]):\n",
    "        articul_encrypred_idx = row['articul_encrypred_idx']\n",
    "        # Объединяем текстовые признаки\n",
    "        text_input = f\"{row['brand']} {row['title']} {row['color_base']} {row['ktt1']}\"\n",
    "        text_feat = text_model.encode(text_input, convert_to_tensor=True).to(device)\n",
    "        \n",
    "        # Извлекаем признаки изображения\n",
    "        try:\n",
    "            response = requests.get(row['photo_analytics'], timeout=5)\n",
    "            image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            image = image_transform(image).unsqueeze(0).to(device)  # переносим на нужное устройство\n",
    "            with torch.no_grad():\n",
    "                image_feat = image_model(image).squeeze(0)\n",
    "        except Exception as e:\n",
    "            # Если не удалось получить изображение, используем вектор нулей (размер 512)\n",
    "            image_feat = torch.zeros(512, device=device)\n",
    "        \n",
    "        # Конкатенируем текстовые и визуальные признаки\n",
    "        meta_feat = torch.cat([text_feat, image_feat], dim=0)\n",
    "        # Если размер полученного вектора не совпадает с meta_dim, применяем линейную проекцию\n",
    "        if meta_feat.shape[0] != meta_dim:\n",
    "            proj = nn.Linear(meta_feat.shape[0], meta_dim).to(device)\n",
    "            meta_feat = proj(meta_feat.unsqueeze(0)).squeeze(0)\n",
    "        product_meta[articul_encrypred_idx] = meta_feat\n",
    "    \n",
    "    return product_meta\n",
    "\n",
    "# Пример: объединяем train_df и test_df (или используем df_sales, если он есть)\n",
    "import pandas as pd\n",
    "# df_sales = pd.concat([train_df, test_df], ignore_index=True)  # если необходимо объединить\n",
    "\n",
    "# Предположим, что у вас уже есть df_sales с нужными столбцами\n",
    "product_meta_dict = compute_product_meta_features(df_sales, meta_dim=128)\n",
    "\n",
    "# Количество уникальных товаров\n",
    "num_items = df_sales['articul_encrypred_idx'].nunique()\n",
    "\n",
    "# Формируем матрицу весов для мета-эмбеддингов размерности (num_items+2, meta_dim)\n",
    "# +2 для pad (индекс 0) и mask токена (индекс num_items+1)\n",
    "meta_dim = 128\n",
    "meta_weights = torch.zeros(num_items + 2, meta_dim)\n",
    "# Заполняем веса для индексов от 1 до num_items (предполагается, что articul_encrypred_idx начинается с 1)\n",
    "for idx in range(1, num_items + 1):\n",
    "    if idx in product_meta_dict:\n",
    "        meta_weights[idx] = product_meta_dict[idx]\n",
    "    else:\n",
    "        meta_weights[idx] = torch.zeros(meta_dim)\n",
    "# Для токена 0 (pad) и токена mask (num_items+1) оставляем нули\n",
    "\n",
    "#########################\n",
    "# 2. Модификация модели BERT4Rec\n",
    "#########################\n",
    "\n",
    "class BERT4RecModel(nn.Module):\n",
    "    def __init__(self, num_items, max_len, meta_embedding_weights, embedding_dim=512, num_layers=6, num_heads=8, \n",
    "                 dropout=0.1, ffn_dim=2048):\n",
    "        \"\"\"\n",
    "        Модель BERT4Rec с дополнительными метаданными продукта.\n",
    "        \n",
    "        :param num_items: количество уникальных товаров\n",
    "        :param max_len: максимальная длина последовательности\n",
    "        :param meta_embedding_weights: предвычисленная матрица мета-векторов (torch.Tensor) размером (num_items+2, meta_dim)\n",
    "        :param embedding_dim: размерность эмбеддингов товара\n",
    "        :param num_layers: количество слоёв трансформера\n",
    "        :param num_heads: количество голов внимания\n",
    "        :param dropout: dropout\n",
    "        :param ffn_dim: размерность FFN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_items = num_items\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Эмбеддинги товара (токены: индексы от 0 до num_items+1)\n",
    "        self.item_embeddings = nn.Embedding(num_items + 2, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Слой мета-эмбеддингов (инициализируется предвычисленными признаками)\n",
    "        meta_dim = meta_embedding_weights.shape[1]\n",
    "        self.meta_embedding = nn.Embedding(num_items + 2, meta_dim, padding_idx=0)\n",
    "        self.meta_embedding.weight = nn.Parameter(meta_embedding_weights, requires_grad=False)  # замораживаем или можно fine-tune\n",
    "        # Проекция объединённого представления (конкатенация item и meta признаков)\n",
    "        self.meta_proj = nn.Linear(embedding_dim + meta_dim, embedding_dim)\n",
    "        \n",
    "        # Позиционные эмбеддинги\n",
    "        self.position_embeddings = nn.Embedding(max_len, embedding_dim)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Трансформерный энкодер\n",
    "        encoder_layers = TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ffn_dim,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        \n",
    "        # Классификационная голова\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(embedding_dim * 2),\n",
    "            nn.Linear(embedding_dim * 2, num_items + 1)  # +1 так как target начинается с 1\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Инициализация эмбеддингов и весов трансформера\n",
    "        nn.init.normal_(self.item_embeddings.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.position_embeddings.weight, mean=0.0, std=0.02)\n",
    "        for layer in self.transformer.layers:\n",
    "            nn.init.xavier_uniform_(layer.self_attn.in_proj_weight)\n",
    "            nn.init.xavier_uniform_(layer.self_attn.out_proj.weight)\n",
    "            nn.init.xavier_uniform_(layer.linear1.weight)\n",
    "            nn.init.xavier_uniform_(layer.linear2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc[0].weight)\n",
    "        nn.init.xavier_uniform_(self.fc[3].weight)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, position_ids):\n",
    "        # Эмбеддинги товара\n",
    "        item_embeds = self.item_embeddings(input_ids)\n",
    "        # Мета-эмбеддинги (дополнительная информация)\n",
    "        meta_embeds = self.meta_embedding(input_ids)\n",
    "        # Объединяем: конкатенируем и проецируем в embedding_dim\n",
    "        combined_embeds = torch.cat([item_embeds, meta_embeds], dim=-1)\n",
    "        combined_embeds = self.meta_proj(combined_embeds)\n",
    "        \n",
    "        # Позиционные эмбеддинги\n",
    "        pos_embeds = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # Суммируем эмбеддинги и позиционные признаки, применяем LayerNorm и Dropout\n",
    "        embeddings = self.layer_norm(combined_embeds + pos_embeds)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        # Маска для паддингов\n",
    "        src_key_padding_mask = (attention_mask == 0)\n",
    "        \n",
    "        # Пропускаем через трансформер\n",
    "        transformer_output = self.transformer(embeddings, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # Классификационная голова\n",
    "        logits = self.fc(transformer_output)\n",
    "        return logits\n",
    "\n",
    "#########################\n",
    "# 3. Остальной код обучения остаётся аналогичным\n",
    "#########################\n",
    "\n",
    "# Пример создания датасетов, DataLoader и тренировки (код у вас уже есть)\n",
    "# При этом BERT4RecDataset не меняется, поскольку последовательности остаются на уровне articul_encrypred_idx\n",
    "\n",
    "# Параметры\n",
    "max_len = 12          # Максимальная длина последовательности\n",
    "mask_prob = 0.2       # Вероятность маскирования\n",
    "batch_size = 64\n",
    "\n",
    "# Преобразуем словари последовательностей в список кортежей (user_id, sequence)\n",
    "train_sequences_list = list(train_sequences.items())\n",
    "test_sequences_list = list(test_sequences.items())\n",
    "\n",
    "# Создаём датасеты (ваш класс BERT4RecDataset не изменился)\n",
    "train_dataset = BERT4RecDataset(\n",
    "    sequences=train_sequences_list,\n",
    "    max_len=max_len,\n",
    "    mask_prob=mask_prob,\n",
    "    num_items=num_items,\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "test_dataset = BERT4RecDataset(\n",
    "    sequences=test_sequences_list,\n",
    "    max_len=max_len,\n",
    "    mask_prob=mask_prob,\n",
    "    num_items=num_items,\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "model = BERT4RecModel(\n",
    "    num_items=num_items,\n",
    "    max_len=max_len,\n",
    "    meta_embedding_weights=meta_weights,\n",
    "    embedding_dim=256,    # например, 256\n",
    "    num_layers=3,         # количество слоёв\n",
    "    num_heads=4,          # количество голов\n",
    "    dropout=0.01\n",
    ").to(device)\n",
    "\n",
    "# Настраиваем оптимизатор\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "\n",
    "# Функция тренировки (ваша, как ранее)\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=5,\n",
    "    log_interval=50,\n",
    "    save_path=\"best_model.pth\",\n",
    "):\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar_train = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Training, Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, batch in progress_bar_train:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            position_ids = batch['position_ids'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, position_ids)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            if (batch_idx + 1) % log_interval == 0:\n",
    "                avg_loss = running_loss / log_interval\n",
    "                train_loss_history.append(avg_loss)\n",
    "                running_loss = 0.0\n",
    "                progress_bar_train.set_postfix(loss=avg_loss)\n",
    "        \n",
    "        # Валидация (если нужно, можно раскомментировать)\n",
    "        # model.eval()\n",
    "        # val_loss = 0.0\n",
    "        # with torch.no_grad():\n",
    "        #     for batch in val_loader:\n",
    "        #         input_ids = batch['input_ids'].to(device)\n",
    "        #         labels = batch['labels'].to(device)\n",
    "        #         attention_mask = batch['attention_mask'].to(device)\n",
    "        #         position_ids = batch['position_ids'].to(device)\n",
    "        #         outputs = model(input_ids, attention_mask, position_ids)\n",
    "        #         loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "        #         val_loss += loss.item()\n",
    "        # avg_val_loss = val_loss / len(val_loader)\n",
    "        # val_loss_history.append(avg_val_loss)\n",
    "        # if avg_val_loss < best_loss:\n",
    "        #     best_loss = avg_val_loss\n",
    "        #     torch.save(model.state_dict(), save_path)\n",
    "        #     print(f\"New best model saved with val loss: {best_loss:.4f}\")\n",
    "    \n",
    "    return train_loss_history, val_loss_history\n",
    "\n",
    "# Запускаем обучение\n",
    "train_losses, val_losses = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=5,\n",
    "    log_interval=100,\n",
    "    save_path=\"bert4rec_best.pth\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee022bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920db365-8e6e-4865-a447-4ffb58880c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6033b24-76f5-4771-bbcf-b65828b1b4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c42a19b4-ec48-412d-a21b-748b9d7531d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для параллельной генерации рекомендаций (на входе батч из тестового DataLoader)\n",
    "def generate_parallel_recommendations(model, input_ids, attention_mask, position_ids, k=6, device=device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        position_ids = position_ids.to(device)\n",
    "        \n",
    "        # Получаем выходы модели: [B, seq_len, num_items+1]\n",
    "        outputs = model(input_ids, attention_mask, position_ids)\n",
    "        # Берем логиты последнего токена (последний временной шаг)\n",
    "        logits = outputs[:, -1, :]  # размер [B, num_items+1]\n",
    "        # Выбираем топ-k рекомендаций для каждого примера\n",
    "        recs = torch.topk(logits, k=k, dim=-1).indices\n",
    "    return recs\n",
    "\n",
    "# Функция для последовательной генерации рекомендаций для одного пользователя\n",
    "def generate_sequential_recommendations(model, initial_sequence, max_len, k=6, device=device):\n",
    "    \"\"\"\n",
    "    Генерирует последовательные рекомендации (авторегрессивно) для одного пользователя.\n",
    "    \n",
    "    :param initial_sequence: Исходная последовательность (список int) без паддинга.\n",
    "    :param max_len: Максимальная длина последовательности, с которой обучалась модель.\n",
    "    :param k: Количество генерируемых рекомендаций.\n",
    "    :param device: Устройство.\n",
    "    :return: Список сгенерированных рекомендаций.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated = []\n",
    "    current_seq = initial_sequence.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(k):\n",
    "            # Если длина последовательности меньше max_len – дополняем слева паддингом (значение 0)\n",
    "            if len(current_seq) < max_len:\n",
    "                padded_seq = [0] * (max_len - len(current_seq)) + current_seq\n",
    "            else:\n",
    "                padded_seq = current_seq[-max_len:]\n",
    "            \n",
    "            input_ids = torch.tensor(padded_seq, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            attention_mask = (input_ids != 0).long()\n",
    "            position_ids = torch.arange(max_len, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, position_ids)  # [1, max_len, num_items+1]\n",
    "            logits = outputs[:, -1, :]  # [1, num_items+1]\n",
    "            next_token = torch.topk(logits, k=1, dim=-1).indices.squeeze().item()\n",
    "            generated.append(next_token)\n",
    "            current_seq.append(next_token)\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a46b59b3-6b92-48ce-a873-715d43d47d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_inference(model, inference_loader, k=6, device=device):\n",
    "    \"\"\"\n",
    "    Запускает инференс на inference_loader и получает топ-K рекомендаций для каждого пользователя.\n",
    "    \n",
    "    :param model: Обученная модель BERT4Rec\n",
    "    :param inference_loader: DataLoader без маскированных токенов (данные из train)\n",
    "    :param k: Количество рекомендаций\n",
    "    :param device: Устройство (CPU/GPU)\n",
    "    :return: Список предсказанных рекомендаций для всех пользователей\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_recommendations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(inference_loader, desc=\"Parallel Inference\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            position_ids = batch[\"position_ids\"].to(device)\n",
    "\n",
    "            # Генерируем k рекомендаций\n",
    "            recs = generate_parallel_recommendations(model, input_ids, attention_mask, position_ids, k=k, device=device)\n",
    "            \n",
    "            all_recommendations.extend(recs.cpu().tolist())\n",
    "\n",
    "    return all_recommendations\n",
    "\n",
    "\n",
    "def run_sequential_inference(model, inference_loader, max_len, k=6, device=device):\n",
    "    \"\"\"\n",
    "    Запускает последовательный инференс: для каждого пользователя из inference_loader извлекается исходная (непадённая) последовательность,\n",
    "    и генерируется последовательность рекомендаций методом авторегрессии.\n",
    "    \n",
    "    :param model: Обученная модель BERT4Rec.\n",
    "    :param inference_loader: DataLoader с данными (is_train=False).\n",
    "    :param max_len: Максимальная длина последовательности.\n",
    "    :param k: Количество генерируемых рекомендаций для каждого пользователя.\n",
    "    :param device: Устройство.\n",
    "    :return: Словарь вида {user_id: [последовательность рекомендаций]}.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    user_recs = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(inference_loader, desc=\"Sequential Inference\"):\n",
    "            input_ids = batch['input_ids']           # [B, max_len]\n",
    "            attention_mask = batch['attention_mask']   # [B, max_len]\n",
    "            user_ids = batch['user_id']                # список user_id\n",
    "            \n",
    "            # Для каждого пользователя в батче извлекаем исходную последовательность без паддинга\n",
    "            for i in range(input_ids.shape[0]):\n",
    "                # Переходим на CPU, чтобы легко работать со списками\n",
    "                seq = input_ids[i].cpu().tolist()\n",
    "                mask = attention_mask[i].cpu().tolist()\n",
    "                # Извлекаем только те токены, где mask==1 (непаддинговые элементы)\n",
    "                initial_seq = [token for token, m in zip(seq, mask) if m == 1]\n",
    "                recs = generate_sequential_recommendations(model, initial_seq, max_len, k=k, device=device)\n",
    "                user_recs[user_ids[i]] = recs\n",
    "\n",
    "    return user_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "87317aef-b821-4a5d-a03e-e43eb437bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем train-данные для генерации предсказаний\n",
    "inference_dataset = BERT4RecDataset(sequences=train_sequences_list, max_len=max_len, mask_prob=0.0, num_items=num_items, is_train=False)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c64e54-d85c-4360-8e34-795eddad8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_recommendations_to_users(user_ids, recommendations):\n",
    "    \"\"\"\n",
    "    Преобразует список рекомендаций в словарь {user_id: recommendations}.\n",
    "\n",
    "    :param user_ids: Список ID пользователей из inference_loader\n",
    "    :param recommendations: Список рекомендаций из parallel_recs\n",
    "    :return: Словарь {user_id: [recommendations]}\n",
    "    \"\"\"\n",
    "    user_to_recs = {user: recs for user, recs in zip(user_ids, recommendations)}\n",
    "    return user_to_recs\n",
    "\n",
    "# Получаем список пользователей из inference_loader\n",
    "user_ids = inference_loader.dataset.user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e673eef-f9b2-40e3-847e-d6856c4bd678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параллельные предсказания (Batch Inference)\n",
    "parallel_recs = run_parallel_inference(model, inference_loader, k=k, device=device)\n",
    "\n",
    "# Создаём словарь {user_id: recommendations}\n",
    "test_user_to_parallel_recs = map_recommendations_to_users(user_ids, parallel_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815bdfe-b39f-4947-b7f4-6ab29f00735d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cdb0d477a142b384762265dc7e67a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sequential Inference:   0%|          | 0/8541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Параллельные предсказания (Batch Inference)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# parallel_recs = run_parallel_inference(model, inference_loader, k=k, device=device)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Последовательные предсказания (Sequential Inference)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m sequential_recs \u001b[38;5;241m=\u001b[39m run_sequential_inference(model, inference_loader, max_len\u001b[38;5;241m=\u001b[39mmax_len, k\u001b[38;5;241m=\u001b[39mk, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[125], line 56\u001b[0m, in \u001b[0;36mrun_sequential_inference\u001b[1;34m(model, inference_loader, max_len, k, device)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;66;03m# Извлекаем только те токены, где mask==1 (непаддинговые элементы)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m             initial_seq \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(seq, mask) \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 56\u001b[0m             recs \u001b[38;5;241m=\u001b[39m generate_sequential_recommendations(model, initial_seq, max_len, k\u001b[38;5;241m=\u001b[39mk, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     57\u001b[0m             user_recs[user_ids[i]] \u001b[38;5;241m=\u001b[39m recs\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m user_recs\n",
      "Cell \u001b[1;32mIn[124], line 44\u001b[0m, in \u001b[0;36mgenerate_sequential_recommendations\u001b[1;34m(model, initial_sequence, max_len, k, device)\u001b[0m\n\u001b[0;32m     41\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m (input_ids \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     42\u001b[0m position_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(max_len, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask, position_ids)  \u001b[38;5;66;03m# [1, max_len, num_items+1]\u001b[39;00m\n\u001b[0;32m     45\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# [1, num_items+1]\u001b[39;00m\n\u001b[0;32m     46\u001b[0m next_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(logits, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 72\u001b[0m, in \u001b[0;36mBERT4RecModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids)\u001b[0m\n\u001b[0;32m     69\u001b[0m src_key_padding_mask \u001b[38;5;241m=\u001b[39m (attention_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Пропускаем через трансформер\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m transformer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m     73\u001b[0m     embeddings,\n\u001b[0;32m     74\u001b[0m     src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask\n\u001b[0;32m     75\u001b[0m )\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Классификация\u001b[39;00m\n\u001b[0;32m     78\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(transformer_output)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:511\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    508\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 511\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(\n\u001b[0;32m    512\u001b[0m         output,\n\u001b[0;32m    513\u001b[0m         src_mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    514\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m    515\u001b[0m         src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask_for_layers,\n\u001b[0;32m    516\u001b[0m     )\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    519\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:871\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m why_not_sparsity_fast_path:\n\u001b[0;32m    868\u001b[0m         merged_mask, mask_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mmerge_masks(\n\u001b[0;32m    869\u001b[0m             src_mask, src_key_padding_mask, src\n\u001b[0;32m    870\u001b[0m         )\n\u001b[1;32m--> 871\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_transformer_encoder_layer_fwd(\n\u001b[0;32m    872\u001b[0m             src,\n\u001b[0;32m    873\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[0;32m    874\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m    875\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39min_proj_weight,\n\u001b[0;32m    876\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m    877\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    878\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mout_proj\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    879\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_relu_or_gelu \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    880\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first,\n\u001b[0;32m    881\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1\u001b[38;5;241m.\u001b[39meps,\n\u001b[0;32m    882\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    883\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    884\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    885\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    886\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    887\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    888\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    889\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    890\u001b[0m             merged_mask,\n\u001b[0;32m    891\u001b[0m             mask_type,\n\u001b[0;32m    892\u001b[0m         )\n\u001b[0;32m    894\u001b[0m \u001b[38;5;66;03m# see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\u001b[39;00m\n\u001b[0;32m    895\u001b[0m x \u001b[38;5;241m=\u001b[39m src\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Последовательные предсказания (Sequential Inference)\n",
    "sequential_recs = run_sequential_inference(model, inference_loader, max_len=max_len, k=k, device=device)\n",
    "\n",
    "# Создаём словарь {user_id: recommendations}\n",
    "test_user_to_sequential_recs = map_recommendations_to_users(user_ids, sequential_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "87f0810d-990a-4a77-b949-a4b4eec3cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bert4rec_classic_parallel = RecommendationDataset(user_recommendations=test_user_to_parallel_recs, user_to_true_items=test_user_to_true_items, k=k)\n",
    "loader = DataLoader(dataset_bert4rec_classic_parallel, batch_size=batch_size, collate_fn=collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5395d665-4a56-4e4f-81dd-06def694b5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef7b699571b48ddbf1489336e28721f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Precision@K:   0%|          | 0/8541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@k: 0.00379\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b1a05b31af41afb59943ba10380b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Recall@K:   0%|          | 0/8541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@k: 0.003697\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1e89860b7b407391803f87c3fcd243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MAP@K:   0%|          | 0/8541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@k: 0.00156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a079a86ca844dc853ab43b69ef59f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NDCG@K:   0%|          | 0/8541 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m map_k \u001b[38;5;241m=\u001b[39m map_at_k_gpu(loader\u001b[38;5;241m=\u001b[39mloader, k\u001b[38;5;241m=\u001b[39mk, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAP@k: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmap_k\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m ndcg_k \u001b[38;5;241m=\u001b[39m ndcg_at_k_gpu(loader\u001b[38;5;241m=\u001b[39mloader, k\u001b[38;5;241m=\u001b[39mk, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNDCG@k: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndcg_k\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[62], line 144\u001b[0m, in \u001b[0;36mndcg_at_k_gpu\u001b[1;34m(loader, k, batch_size, device)\u001b[0m\n\u001b[0;32m    141\u001b[0m         dcg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog2(torch\u001b[38;5;241m.\u001b[39mtensor(j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2.0\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# IDCG@K\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m ideal_dcg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog2(torch\u001b[38;5;241m.\u001b[39mtensor(j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2.0\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(true_items[i]), k)))\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# NDCG@K\u001b[39;00m\n\u001b[0;32m    147\u001b[0m ndcg \u001b[38;5;241m=\u001b[39m dcg \u001b[38;5;241m/\u001b[39m ideal_dcg \u001b[38;5;28;01mif\u001b[39;00m ideal_dcg \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[62], line 144\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    141\u001b[0m         dcg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog2(torch\u001b[38;5;241m.\u001b[39mtensor(j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2.0\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# IDCG@K\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m ideal_dcg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog2(torch\u001b[38;5;241m.\u001b[39mtensor(j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2.0\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(true_items[i]), k)))\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# NDCG@K\u001b[39;00m\n\u001b[0;32m    147\u001b[0m ndcg \u001b[38;5;241m=\u001b[39m dcg \u001b[38;5;241m/\u001b[39m ideal_dcg \u001b[38;5;28;01mif\u001b[39;00m ideal_dcg \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "precision_k = precision_at_k_gpu(loader=loader, k=k, batch_size=batch_size)\n",
    "print(f'Precision@k: {precision_k:.5f}')\n",
    "\n",
    "recall_k = recall_at_k_gpu(loader=loader, k=k, batch_size=batch_size)\n",
    "print(f'Recall@k: {recall_k:5f}')\n",
    "\n",
    "map_k = map_at_k_gpu(loader=loader, k=k, batch_size=batch_size)\n",
    "print(f'MAP@k: {map_k:.5f}')\n",
    "\n",
    "ndcg_k = ndcg_at_k_gpu(loader=loader, k=k, batch_size=batch_size)\n",
    "print(f'NDCG@k: {ndcg_k:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9861c6ad-fefb-4c45-95d6-d06ec7672a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_results(model_name='Top-K', k=k, precision_k=precision_k, recall_k=recall_k, map_k=map_k, ndcg_k=ndcg_k, hyperparameters=None)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874983a0-b07a-4d94-abf5-03b291ef2dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cdc972e-b2cd-48d8-b819-af4028a7d081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import weakref\n",
    "\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if isinstance(obj, torch.Tensor) and obj.is_mps:\n",
    "            ref = weakref.ref(obj)\n",
    "            del obj\n",
    "            del ref\n",
    "    except ReferenceError:\n",
    "        pass\n",
    "\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df4741a-4e39-4ca4-8b4d-d29256befedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea101faa-5d2b-44f3-8046-3fbe91144118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca21a7-500f-4abd-9b36-14ce26723bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3c64661-6729-4f2e-98bd-acfb01327fdc",
   "metadata": {},
   "source": [
    "### BERT4Rec Git version\n",
    "https://github.com/asash/bert4rec_repro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74766d03-ad72-4914-b4a4-6c9e1569aa18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv: Recommender System with LLM)",
   "language": "python",
   "name": "recommender_system_with_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
